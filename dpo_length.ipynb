{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/b0990106x/miniconda3/envs/trl/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append(\"/work/b0990106x/trl/vc\")\n",
    "import importlib\n",
    "import vc\n",
    "importlib.reload(vc)\n",
    "import torch\n",
    "from vc.trainer_encodec_vc_inference import get_ar_prediction_v3, pack_inputs_v2, get_ar_prediction_for_data\n",
    "from types import SimpleNamespace\n",
    "from transformers import BartForConditionalGeneration, AutoModelForCausalLM, AutoTokenizer\n",
    "from NISQA.nisqa.NISQA_model import nisqaModel\n",
    "from datasets import load_from_disk, Dataset\n",
    "from trl import DPOTrainer, DPOConfig, AutoModelForSeq2SeqLMWithValueHead, create_reference_model\n",
    "from vc.encodec_model.nar_bart_model import NARBartForConditionalGeneration\n",
    "from datetime import datetime\n",
    "import os\n",
    "import numpy as np\n",
    "from dpo_eval import get_reward, eval_dpo, eval_dpo_mos, get_length_reward, eval_dpo_token_length\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "import concurrent.futures\n",
    "import time\n",
    "from typing import List, Tuple\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "def get_length_reward(token_list):\n",
    "    # count the length of the token list\n",
    "    # print(token_list)\n",
    "    token_list = [int(token.split(\"_\")[2]) for token in token_list]\n",
    "    length = len(token_list)\n",
    "\n",
    "    score_level = [10, 50, 100, 200, 300]\n",
    "    if length < score_level[0]:\n",
    "        reward = 5\n",
    "    elif length < score_level[1]:\n",
    "        reward = 4 + ((score_level[1] - length) / (score_level[1] - score_level[0]))\n",
    "    elif length < score_level[2]:\n",
    "        reward = 3 + ((score_level[2] - length) / (score_level[2] - score_level[1]))\n",
    "    elif length < score_level[3]:\n",
    "        reward = 2 + ((score_level[3] - length) / (score_level[3] - score_level[2]))\n",
    "    elif length < score_level[4]:\n",
    "        reward = 1 + ((score_level[4] - length) / (score_level[4] - score_level[3]))\n",
    "    else:\n",
    "        reward = 0\n",
    "\n",
    "    return reward, length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "def generate_output(\n",
    "        ar_model, \n",
    "        ar_tokenizer, \n",
    "        src_encodec: list, \n",
    "        instruction: list, \n",
    "        args_predict: SimpleNamespace, \n",
    "        temperature: float = 1.0\n",
    ") -> tuple[float, str]:\n",
    "    '''\n",
    "    Generates output from AR model, synthesize the audio, and evaluate the audio using NISQA.\n",
    "\n",
    "    New Args: \n",
    "        ar_model(BartForConditionalGeneration): AR model\n",
    "        ar_tokenizer(AutoTokenizer): AR tokenizer\n",
    "        src_encodec(list): A list of inputs, where each input is a list of layers, and each layer is a list of v_token integers.\n",
    "        instruction(list): A list of string of instructions.\n",
    "        args_predict(SimpleNamespace): A SimpleNamespace object containing the arguments for the device.\n",
    "        temperature(float): The temperature for the AR model.\n",
    "\n",
    "    Returns:\n",
    "        tuple:\n",
    "            reward(float): The reward of the audio.\n",
    "            tokenized_decode_ar(str): The tokenized output of the AR model - first layer.\n",
    "    '''\n",
    "    # Generate predictions using the AR model\n",
    "\n",
    "    decode_ar = get_ar_prediction_for_data(args_predict, ar_model, ar_tokenizer, src_encodec, instruction)\n",
    "\n",
    "    # Flatten the decoded AR output tensor and convert it to a list\n",
    "    list_decode_ar = decode_ar.flatten().tolist()   \n",
    "    \n",
    "    # Filter the decoded AR output to remove special tokens\n",
    "    filtered_decode_ar_list = list_decode_ar[2:-1]\n",
    "\n",
    "    # Convert the filtered token IDs back to tokens and then to a string\n",
    "    decode_ar_tokens = ar_tokenizer.convert_ids_to_tokens(filtered_decode_ar_list)\n",
    "    tokenized_decode_ar = ar_tokenizer.convert_tokens_to_string(decode_ar_tokens)\n",
    "    # Evaluate the audio to get the reward\n",
    "    reward, output_length = get_length_reward(decode_ar_tokens)\n",
    "\n",
    "    return output_length, tokenized_decode_ar\n",
    "\n",
    "def extract_data_from_json(file_path: str) -> Tuple[List[list], List[str], List[list]]:\n",
    "    \"\"\"\n",
    "    Loads data from a JSON file and extracts 'src_encodec', 'instruction', and 'tgt_encodec'.\n",
    "\n",
    "    Args:\n",
    "        file_path (str): The path to the JSON file.\n",
    "\n",
    "    Returns:\n",
    "        tuple:\n",
    "            all_src_encodec (List[list]): A list containing the 'src_encodec' data from each item in the JSON file.\n",
    "            all_instruction (List[str]): A list containing the 'instruction' data from each item in the JSON file.\n",
    "            all_tgt_encodec (List[list]): A list containing the 'tgt_encodec' data from each item in the JSON file.\n",
    "    \"\"\"\n",
    "    with open(file_path, 'r') as f:\n",
    "        data = json.load(f)\n",
    "\n",
    "    all_src_encodec = [item[\"src_encodec\"] for item in data]\n",
    "    all_instruction = [item[\"instruction\"] for item in data]\n",
    "    all_tgt_encodec = [item[\"tgt_encodec\"] for item in data]\n",
    "\n",
    "    return all_src_encodec, all_instruction, all_tgt_encodec\n",
    "\n",
    "def train_model(\n",
    "        model,\n",
    "        model_ref,\n",
    "        ar_tokenizer,\n",
    "        train_dataset: Dataset,\n",
    "        val_dataset: Dataset,\n",
    "        model_output_dir: str,\n",
    "        beta: float,\n",
    "        resume_from_checkpoint: bool,\n",
    "        model_checkpoint: str\n",
    ") -> None:\n",
    "    '''\n",
    "    Train the DPO model and save the model.\n",
    "\n",
    "    Args:\n",
    "        model(AutoModelForSeq2SeqLMWithValueHead): The DPO model.\n",
    "        model_ref(AutoModelForCausalLM): The reference model.\n",
    "        ar_tokenizer(AutoTokenizer): The tokenizer.\n",
    "        train_dataset(Dataset): The training dataset.\n",
    "        val_dataset(Dataset): The validation dataset.\n",
    "        model_output_dir(str): The output directory for the model.\n",
    "        beta(float): The beta value.\n",
    "        resume_from_checkpoint(bool): Whether to resume from a checkpoint.\n",
    "        model_checkpoint(str): The path to the model\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    '''\n",
    "\n",
    "    training_args = DPOConfig(\n",
    "        beta = beta,\n",
    "        output_dir = model_output_dir,\n",
    "        generate_during_eval = True,\n",
    "        resume_from_checkpoint = model_checkpoint if resume_from_checkpoint else None,\n",
    "        seed = 42, \n",
    "        per_device_train_batch_size = 1, # Default = 8\n",
    "        num_train_epochs = 100, # Default\n",
    "        gradient_accumulation_steps = 1, # Default\n",
    "        learning_rate = 5e-05, # Default\n",
    "        remove_unused_columns=False,\n",
    "        max_length = 1024*9,\n",
    "        max_prompt_length = 1024*9, \n",
    "        max_target_length = 1024*9, \n",
    "    )\n",
    "\n",
    "    trainer = DPOTrainer(\n",
    "        model=model,\n",
    "        ref_model=model_ref,\n",
    "        args=training_args,\n",
    "        tokenizer=ar_tokenizer,\n",
    "        train_dataset=train_dataset,\n",
    "        eval_dataset=val_dataset,\n",
    "    )\n",
    "    # Train the model\n",
    "    trainer.train()\n",
    "\n",
    "    # Save the model\n",
    "    trainer.save_model(f\"{model_output_dir}/dpo_model\")\n",
    "    model.config.to_json_file(f\"{model_output_dir}/dpo_model/config.json\")\n",
    "    ar_tokenizer.save_pretrained(f\"{model_output_dir}/dpo_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "def process_data(sample_size, \n",
    "                 ar_model, \n",
    "                 ar_tokenizer, \n",
    "                 all_src_encodec, \n",
    "                 all_instruction,\n",
    "                 args_predict, \n",
    "                 temperature=1.0\n",
    "):\n",
    "    \"\"\"\n",
    "    Process data to generate outputs, calculate rewards, and organize chosen and rejected data.\n",
    "    \"\"\"\n",
    "    if sample_size < 2:\n",
    "        raise ValueError(\"Parameter 'sample_size' must be greater than 1.\")\n",
    "\n",
    "    chosen, rejected, prompts, chosen_rewards, rejected_rewards, average_rewards = [], [], [], [], [], []\n",
    "\n",
    "    for i in tqdm(range(len(all_src_encodec)), desc=\"Processing Data\"):\n",
    "        rewards, tokenized_outputs = [], []\n",
    "\n",
    "        for _ in tqdm(range(sample_size), desc=\"Processing Samples\"):\n",
    "            size_of_packed_input = (\n",
    "                    len(all_src_encodec[i][0]) +\n",
    "                    len(ar_tokenizer(all_instruction[i])[\"input_ids\"][1:-1]) +\n",
    "                    3\n",
    "            )\n",
    "            if 4 < size_of_packed_input <= 1024:\n",
    "                \n",
    "                reward, tokenized_decode_ar = generate_output(\n",
    "                    ar_model = ar_model,\n",
    "                    ar_tokenizer = ar_tokenizer,\n",
    "                    src_encodec = all_src_encodec[i],\n",
    "                    instruction = all_instruction[i],\n",
    "                    args_predict = args_predict,\n",
    "                    temperature = temperature\n",
    "                )\n",
    "                rewards.append(reward)\n",
    "                tokenized_outputs.append(tokenized_decode_ar)\n",
    "\n",
    "        valid_rewards = [r for r in rewards if r is not None]\n",
    "        valid_outputs = [tokenized_outputs[j] for j in range(len(rewards)) if rewards[j] is not None]\n",
    "\n",
    "        if len(valid_rewards) >= 2:\n",
    "            # NOTE: lesser length is higher reward\n",
    "            max_reward_index = np.argmax(valid_rewards)\n",
    "            min_reward_index = np.argmin(valid_rewards)\n",
    "            average_reward = np.mean(valid_rewards)\n",
    "            chosen_output = valid_outputs[min_reward_index]\n",
    "            rejected_output = valid_outputs[max_reward_index]\n",
    "\n",
    "            obs_input = pack_inputs_v2(ar_tokenizer, all_src_encodec[i], all_instruction[i])\n",
    "            \n",
    "            tokenize_input = ar_tokenizer.convert_ids_to_tokens(obs_input)\n",
    "            tokenize_input_str = ar_tokenizer.convert_tokens_to_string(tokenize_input)\n",
    "            prompts.append(tokenize_input_str)\n",
    "\n",
    "            chosen.append(chosen_output)\n",
    "            chosen_rewards.append(valid_rewards[min_reward_index])\n",
    "            rejected.append(rejected_output)\n",
    "            rejected_rewards.append(valid_rewards[max_reward_index])\n",
    "            average_rewards.append(average_reward)\n",
    "        else:\n",
    "            print(f\"Not enough valid rewards for data index {i}\")\n",
    "\n",
    "    # print(f\"{len(chosen)} datas are processed.\")\n",
    "    return chosen, rejected, prompts, chosen_rewards, rejected_rewards, average_rewards\n",
    "\n",
    "\n",
    "def generate_data(ar_model, \n",
    "                  ar_tokenizer, \n",
    "                  nar_model, \n",
    "                  nar_tokenizer, \n",
    "                  selected_src_encodec, \n",
    "                  selected_instruction,\n",
    "                  args_predict, \n",
    "                  sample_size, \n",
    "                  iteration, \n",
    "                  agent_output_dir, \n",
    "                  base_path=\"/work/b0990106x/trl\", \n",
    "                  temperature=1.0\n",
    "):\n",
    "    \"\"\"\n",
    "    Generates data for the dataset and saves info to a JSON file.\n",
    "    \"\"\"\n",
    "    # if iteration == -100:\n",
    "    chosen, rejected, prompts, chosen_rewards, rejected_rewards, average_rewards = process_data(\n",
    "        sample_size=sample_size,\n",
    "        ar_model=ar_model,\n",
    "        ar_tokenizer=ar_tokenizer,\n",
    "        all_src_encodec=selected_src_encodec,\n",
    "        all_instruction=selected_instruction,\n",
    "        args_predict=args_predict,\n",
    "        temperature=temperature\n",
    "    )\n",
    "\n",
    "    data = {\n",
    "        \"prompt\": prompts,\n",
    "        \"chosen\": chosen,\n",
    "        \"rejected\": rejected,\n",
    "        \"chosen_rewards\": chosen_rewards,\n",
    "        \"rejected_rewards\": rejected_rewards,\n",
    "        \"average_rewards\": average_rewards\n",
    "    }\n",
    "\n",
    "    if len(selected_src_encodec) == 1:\n",
    "        data = {\n",
    "            \"prompt\": prompts + prompts,\n",
    "            \"chosen\": chosen + chosen,\n",
    "            \"rejected\": rejected + rejected,\n",
    "            \"chosen_rewards\": chosen_rewards + chosen_rewards,\n",
    "            \"rejected_rewards\": rejected_rewards + rejected_rewards,\n",
    "            \"average_rewards\": average_rewards + average_rewards\n",
    "        }\n",
    "\n",
    "    with open(f\"{agent_output_dir}/data_iter_{iteration}.json\", \"w\") as outfile:\n",
    "        json.dump(data, outfile, indent=4)\n",
    "\n",
    "    data_for_dataset = {key: data[key] for key in [\"prompt\", \"chosen\", \"rejected\"]}\n",
    "    # else: \n",
    "    #     with open(f\"{base_path}/output/data_iter_len_long.json\", \"r\") as f:\n",
    "    #         data = json.load(f)\n",
    "    #     data_for_dataset = {key: data[key] for key in [\"prompt\", \"chosen\", \"rejected\"]}\n",
    "    #     chosen_rewards = data[\"chosen_rewards\"]\n",
    "    #     rejected_rewards = data[\"rejected_rewards\"]\n",
    "    return data_for_dataset, chosen_rewards, rejected_rewards\n",
    "\n",
    "def train_iteration(model_checkpoint, \n",
    "                    iteration, \n",
    "                    data_size, \n",
    "                    sample_size, \n",
    "                    ar_checkpoint, \n",
    "                    nar_checkpoint, \n",
    "                    all_src_encodec, \n",
    "                    all_instruction, \n",
    "                    args_predict, \n",
    "                    agent_output_dir,\n",
    "                    model_output_dir_base, \n",
    "                    beta = 0.1, \n",
    "                    temperature = 1.0,\n",
    "                    base_path=\"/work/b0990106x/trl\",\n",
    "                    resume_from_checkpoint = False\n",
    "):\n",
    "    \"\"\"\n",
    "    Executes one training iteration: generates data, trains the model, and saves the output.\n",
    "    \"\"\"\n",
    "    # print(f\"Iteration {iteration}\")\n",
    "\n",
    "    ar_model = BartForConditionalGeneration.from_pretrained(model_checkpoint)\n",
    "    ar_tokenizer = AutoTokenizer.from_pretrained(ar_checkpoint)\n",
    "    ar_tokenizer.pad_token = ar_tokenizer.eos_token\n",
    "    nar_model = NARBartForConditionalGeneration.from_pretrained(nar_checkpoint)\n",
    "    nar_tokenizer = AutoTokenizer.from_pretrained(nar_checkpoint)\n",
    "\n",
    "    selected_src_encodec = all_src_encodec[:data_size]\n",
    "    selected_instruction = all_instruction[:data_size]\n",
    "\n",
    "    data_for_dataset, chosen_rewards, rejected_rewards = generate_data(ar_model=ar_model,\n",
    "                                                                        ar_tokenizer=ar_tokenizer,\n",
    "                                                                        nar_model=nar_model,\n",
    "                                                                        nar_tokenizer=nar_tokenizer,\n",
    "                                                                        selected_src_encodec=selected_src_encodec,\n",
    "                                                                        selected_instruction=selected_instruction,\n",
    "                                                                        args_predict=args_predict,\n",
    "                                                                        sample_size=sample_size,\n",
    "                                                                        iteration=iteration,\n",
    "                                                                        agent_output_dir=agent_output_dir,\n",
    "                                                                        base_path=base_path,\n",
    "                                                                        temperature=temperature)\n",
    "\n",
    "    dataset = Dataset.from_dict(data_for_dataset)\n",
    "    dataset_dict = dataset.train_test_split(test_size=0.1)\n",
    "    train_dataset = dataset_dict[\"train\"]\n",
    "    val_dataset = dataset_dict[\"test\"]\n",
    "\n",
    "    model_output_dir = f\"{model_output_dir_base}/iter_{iteration}\"\n",
    "    os.makedirs(model_output_dir, exist_ok=True)\n",
    "\n",
    "    model = AutoModelForSeq2SeqLMWithValueHead.from_pretrained(model_checkpoint, return_dict=True)\n",
    "    model_ref = create_reference_model(model)\n",
    "\n",
    "    train_model(model=model,\n",
    "                model_ref=model_ref,\n",
    "                ar_tokenizer=ar_tokenizer,\n",
    "                train_dataset=train_dataset,\n",
    "                val_dataset=val_dataset,\n",
    "                model_output_dir=model_output_dir,\n",
    "                beta=beta,\n",
    "                resume_from_checkpoint=resume_from_checkpoint,\n",
    "                model_checkpoint=model_checkpoint)\n",
    "\n",
    "    return f\"{model_output_dir}/dpo_model\", chosen_rewards, rejected_rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "timestamp: 0725-2005\n",
      "length of all_src_encodec: 9252\n",
      "length of all_instruction: 9252\n"
     ]
    }
   ],
   "source": [
    "# Load all data\n",
    "all_src_encodec, _, all_tgt_encodec = extract_data_from_json('dpo_data/src_encodec.json')\n",
    "\n",
    "\n",
    "all_src_encodec = all_src_encodec[2:] # [2:] will remove the first two data\n",
    "\n",
    "length_instruction = \"Make the audio much faster than before.\"\n",
    "all_instruction = [length_instruction for _ in range(len(all_src_encodec))]\n",
    "\n",
    "# Define paths and device\n",
    "base_path = \"/work/b0990106x/trl\"\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# Define timestamp\n",
    "now = datetime.now()\n",
    "ts = now.strftime(\"%m%d-%H%M\")\n",
    "print(\"timestamp:\", ts)\n",
    "\n",
    "# Define paths\n",
    "model_output_dir = f\"{base_path}/model_output/{ts}\" # Location where the model are saved\n",
    "agent_input_dir = f\"{base_path}/data-encodec\" # Location of our original data(input) is stored\n",
    "agent_output_dir = f\"{base_path}/output/{ts}\" # Path of saving the generated audio for reward model to evaluate\n",
    "\n",
    "if not os.path.exists(model_output_dir):\n",
    "    os.makedirs(model_output_dir)\n",
    "\n",
    "if not os.path.exists(agent_output_dir):\n",
    "    os.makedirs(agent_output_dir)\n",
    "\n",
    "# Define arguments \n",
    "args_predict = SimpleNamespace(output_path=f\"{base_path}/output/{ts}/example.wav\", seed=0, device=device)\n",
    "ar_checkpoint = \"lca0503/speech-chatgpt-base-ar-v2-epoch10-wotrans\"\n",
    "nar_checkpoint = \"lca0503/speech-chatgpt-base-nar-v2-epoch4-wotrans\"\n",
    "\n",
    "# Define training parameters\n",
    "model_checkpoint = ar_checkpoint # set the initial model checkpoint\n",
    "initial_data_size =  1 # Training: data size for the first iteration\n",
    "data_size_per_iteration = 1 # Training: each iteration will train how many data\n",
    "total_data_size = 1 # Training: total data size that we want to train\n",
    "sample_size = 10 # Prepare Dataset: generate how many outputs to select max and min for chosen and rejected\n",
    "beta = 0.1 # Training: beta value for DPO\n",
    "num_iterations = 20  # Training: train how many iterations\n",
    "eval_selected_indices = [0] # Evaluation: evaluate which data from extract_data_from_json('dpo_data/src_encodec.json')\n",
    "eval_data_len = 1 # Evaluation: evaluate how many data\n",
    "\n",
    "# num_iterations = (total_data_size - initial_data_size) // data_size_per_iteration + 1 # Training: train how many iterations\n",
    "# eval_selected_indices = random.sample(range(len(all_src_encodec)), eval_data_len) # Evaluation: select 10 data for evaluation\n",
    "print(f\"length of all_src_encodec: {len(all_src_encodec)}\") # ~ 9000 data\n",
    "print(f\"length of all_instruction: {len(all_instruction)}\") # ~ 9000 data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_iterations: 20\n",
      "data_size_per_iteration: 1\n",
      "sample_size: 10\n",
      "beta: 0.1\n",
      "ar_checkpoint: lca0503/speech-chatgpt-base-ar-v2-epoch10-wotrans\n",
      "nar_checkpoint: lca0503/speech-chatgpt-base-nar-v2-epoch4-wotrans\n",
      "args_predict: namespace(output_path='/work/b0990106x/trl/output/0725-2005/example.wav', seed=0, device='cuda')\n",
      "model_output_dir: /work/b0990106x/trl/model_output/0725-2005\n",
      "agent_output_dir: /work/b0990106x/trl/output/0725-2005\n",
      "base_path: /work/b0990106x/trl\n",
      "device: cuda\n",
      "eval_data_len: 1\n",
      "eval_selected_indices: [0]\n",
      "Type of batch_src_encodec: <class 'list'>\n",
      "Type of batch_instruction: <class 'list'>\n",
      "[[[835, 339, 999, 629, 604, 462, 314, 600, 846, 562, 846, 358, 984, 393, 182, 453, 584, 535, 407, 1021, 701, 843, 945, 495, 563, 495, 495, 727, 317, 604, 475, 835, 835, 835, 339, 475, 339, 123, 254, 103, 561, 858, 646, 755, 375, 548, 435, 233, 323, 395, 819, 475, 339, 835, 779, 257, 339, 341, 170, 38, 38, 103, 408, 62, 141, 731, 73, 651, 143, 875, 321, 310, 310, 972, 679, 582, 808, 813, 808, 291, 722, 982, 627, 192, 764, 531, 291, 466, 567, 601, 771, 112, 688, 348, 793, 793, 11, 192, 23, 983, 1022, 23, 73, 73, 276, 537, 103, 53, 148, 148, 148, 463, 176, 148, 463, 463, 463, 463, 463, 463, 463, 433, 25, 472, 257, 228, 395, 133, 395, 475, 126], [646, 841, 168, 1023, 277, 820, 278, 215, 58, 592, 607, 607, 349, 346, 504, 632, 482, 14, 968, 588, 529, 904, 662, 662, 602, 1013, 662, 386, 617, 870, 648, 1023, 277, 277, 913, 200, 1007, 503, 807, 144, 132, 558, 984, 164, 610, 66, 830, 925, 744, 129, 87, 648, 391, 646, 424, 700, 646, 713, 702, 443, 4, 43, 648, 747, 335, 630, 460, 342, 462, 303, 969, 229, 386, 984, 820, 955, 654, 486, 632, 655, 632, 893, 355, 537, 459, 754, 303, 214, 529, 365, 879, 199, 946, 303, 593, 593, 593, 889, 94, 320, 269, 161, 102, 8, 363, 974, 43, 549, 973, 961, 973, 200, 857, 993, 200, 200, 200, 200, 200, 772, 133, 1023, 516, 92, 87, 837, 765, 700, 601, 571, 200], [937, 752, 989, 196, 852, 310, 498, 380, 650, 354, 648, 677, 677, 1001, 750, 737, 148, 68, 905, 613, 977, 598, 311, 901, 803, 810, 463, 425, 45, 471, 829, 423, 821, 937, 653, 936, 36, 217, 68, 959, 216, 516, 80, 516, 128, 614, 901, 360, 448, 898, 626, 758, 821, 937, 653, 228, 653, 620, 189, 10, 841, 870, 593, 678, 646, 1021, 454, 825, 743, 753, 618, 938, 675, 286, 831, 110, 96, 835, 648, 916, 519, 663, 977, 38, 660, 753, 932, 684, 192, 545, 962, 323, 143, 748, 545, 545, 545, 310, 545, 475, 442, 798, 981, 870, 843, 602, 918, 326, 893, 590, 555, 937, 934, 1013, 934, 188, 188, 813, 730, 653, 1019, 541, 918, 1019, 345, 253, 989, 829, 989, 819, 821], [1022, 762, 835, 651, 854, 446, 629, 1001, 796, 216, 489, 370, 657, 319, 361, 203, 177, 660, 106, 143, 177, 740, 854, 388, 212, 920, 920, 516, 62, 222, 594, 686, 215, 215, 741, 739, 239, 831, 388, 1002, 1002, 612, 602, 584, 256, 986, 986, 687, 624, 874, 1022, 739, 916, 916, 651, 762, 741, 874, 761, 940, 588, 239, 211, 215, 239, 574, 239, 694, 935, 528, 497, 749, 674, 23, 66, 823, 876, 379, 577, 971, 532, 856, 577, 590, 757, 344, 23, 608, 308, 786, 438, 246, 967, 502, 320, 944, 206, 535, 679, 597, 699, 125, 793, 991, 734, 866, 940, 212, 443, 443, 1001, 721, 885, 919, 885, 398, 727, 443, 961, 940, 983, 823, 242, 675, 255, 651, 919, 215, 961, 838, 1022], [528, 222, 904, 375, 885, 186, 427, 284, 95, 559, 225, 368, 737, 500, 698, 728, 122, 689, 566, 138, 946, 363, 958, 637, 163, 270, 949, 422, 430, 875, 809, 89, 505, 375, 222, 714, 407, 797, 435, 247, 142, 700, 242, 800, 860, 880, 867, 428, 660, 561, 919, 435, 336, 900, 885, 736, 448, 167, 247, 203, 433, 89, 683, 190, 195, 38, 439, 877, 100, 967, 211, 899, 167, 556, 39, 932, 426, 228, 728, 350, 961, 232, 984, 744, 379, 247, 924, 193, 156, 368, 980, 1017, 332, 824, 814, 654, 208, 869, 435, 456, 140, 958, 586, 944, 357, 357, 907, 472, 49, 190, 222, 694, 382, 379, 983, 373, 379, 540, 505, 540, 606, 373, 944, 903, 1007, 904, 604, 736, 882, 885, 884], [1011, 140, 140, 435, 692, 244, 5, 942, 125, 95, 898, 816, 159, 105, 345, 587, 241, 813, 419, 24, 830, 247, 991, 630, 54, 416, 583, 917, 200, 147, 781, 315, 692, 41, 41, 982, 881, 13, 124, 255, 615, 694, 672, 291, 574, 42, 691, 423, 35, 549, 478, 692, 386, 1011, 1011, 701, 881, 931, 382, 946, 96, 303, 692, 412, 324, 603, 186, 196, 640, 262, 10, 850, 763, 219, 692, 363, 363, 339, 875, 218, 989, 228, 476, 368, 423, 399, 510, 198, 526, 358, 667, 27, 613, 974, 604, 122, 834, 801, 657, 282, 931, 975, 562, 650, 567, 349, 1005, 825, 609, 632, 851, 935, 315, 917, 96, 995, 477, 530, 334, 23, 390, 929, 644, 692, 505, 903, 185, 982, 1005, 701, 140], [1002, 748, 1015, 764, 983, 330, 1, 102, 399, 330, 656, 392, 890, 560, 335, 44, 662, 684, 442, 19, 280, 806, 158, 635, 439, 825, 241, 439, 502, 491, 489, 447, 900, 562, 570, 562, 447, 793, 902, 1012, 146, 46, 41, 867, 912, 487, 558, 833, 557, 331, 180, 696, 380, 562, 983, 1015, 1015, 895, 560, 373, 872, 633, 772, 772, 1013, 503, 41, 694, 693, 180, 779, 994, 874, 443, 546, 377, 358, 847, 658, 258, 498, 980, 958, 109, 992, 33, 934, 497, 384, 296, 429, 160, 256, 588, 16, 51, 82, 40, 217, 978, 926, 755, 27, 675, 586, 586, 436, 665, 806, 819, 616, 1013, 782, 388, 1008, 616, 772, 464, 380, 927, 977, 860, 743, 944, 528, 41, 552, 291, 799, 755, 977], [899, 322, 467, 322, 1012, 63, 51, 537, 641, 194, 529, 308, 437, 957, 812, 498, 986, 901, 902, 889, 979, 745, 591, 8, 709, 590, 583, 160, 267, 693, 711, 880, 475, 948, 937, 518, 469, 330, 586, 859, 620, 499, 742, 323, 208, 602, 765, 18, 279, 283, 717, 322, 416, 832, 8, 989, 173, 194, 694, 225, 376, 237, 309, 989, 253, 630, 757, 412, 714, 835, 1018, 671, 136, 178, 146, 932, 625, 847, 618, 917, 153, 1018, 256, 250, 905, 168, 923, 312, 547, 903, 417, 1010, 398, 462, 592, 63, 580, 953, 761, 313, 961, 1012, 525, 16, 16, 837, 882, 828, 882, 595, 416, 468, 884, 988, 289, 467, 701, 916, 813, 173, 813, 1019, 1019, 1013, 346, 534, 534, 628, 173, 701, 701]]]\n",
      "8\n",
      "['Make the audio much faster than before.']\n"
     ]
    }
   ],
   "source": [
    "print(f\"num_iterations: {num_iterations}\")\n",
    "print(f\"data_size_per_iteration: {data_size_per_iteration}\")\n",
    "print(f\"sample_size: {sample_size}\")\n",
    "print(f\"beta: {beta}\")\n",
    "print(f\"ar_checkpoint: {ar_checkpoint}\")\n",
    "print(f\"nar_checkpoint: {nar_checkpoint}\")\n",
    "print(f\"args_predict: {args_predict}\")\n",
    "print(f\"model_output_dir: {model_output_dir}\")\n",
    "print(f\"agent_output_dir: {agent_output_dir}\")\n",
    "print(f\"base_path: {base_path}\")\n",
    "print(f\"device: {device}\")\n",
    "print(f\"eval_data_len: {eval_data_len}\")\n",
    "print(f\"eval_selected_indices: {eval_selected_indices}\")\n",
    "print(\"Type of batch_src_encodec:\", type(all_src_encodec))\n",
    "print(\"Type of batch_instruction:\", type(all_instruction))\n",
    "print(all_src_encodec[0:1])\n",
    "print(len(all_src_encodec[0]))\n",
    "print(all_instruction[0:1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/b0990106x/miniconda3/envs/trl/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Training Iterations:   0%|          | 0/20 [00:00<?, ?it/s]\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "Processing Samples: 100%|██████████| 10/10 [00:13<00:00,  1.38s/it]\n",
      "Processing Data: 100%|██████████| 1/1 [00:13<00:00, 13.82s/it]\n",
      "/home/b0990106x/miniconda3/envs/trl/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Map: 100%|██████████| 1/1 [00:00<00:00, 168.04 examples/s]\n",
      "Map: 100%|██████████| 1/1 [00:00<00:00, 206.65 examples/s]\n",
      "/home/b0990106x/miniconda3/envs/trl/lib/python3.10/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mb09901066\u001b[0m (\u001b[33mb09901066_alan\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.17.5 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/work/b0990106x/trl/wandb/run-20240725_200617-z828rn1x</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/b09901066_alan/huggingface/runs/z828rn1x' target=\"_blank\">mild-leaf-40</a></strong> to <a href='https://wandb.ai/b09901066_alan/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/b09901066_alan/huggingface' target=\"_blank\">https://wandb.ai/b09901066_alan/huggingface</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/b09901066_alan/huggingface/runs/z828rn1x' target=\"_blank\">https://wandb.ai/b09901066_alan/huggingface/runs/z828rn1x</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='100' max='100' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [100/100 00:06, Epoch 100/100]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/b0990106x/miniconda3/envs/trl/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Some weights of the model checkpoint at /work/b0990106x/trl/model_output/0725-2005/iter_0/dpo_model were not used when initializing BartForConditionalGeneration: ['v_head.summary.bias', 'v_head.summary.weight']\n",
      "- This IS expected if you are initializing BartForConditionalGeneration from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BartForConditionalGeneration from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Training Iterations:   5%|▌         | 1/20 [01:30<28:48, 90.95s/it]Some weights of the model checkpoint at /work/b0990106x/trl/model_output/0725-2005/iter_0/dpo_model were not used when initializing BartForConditionalGeneration: ['v_head.summary.bias', 'v_head.summary.weight']\n",
      "- This IS expected if you are initializing BartForConditionalGeneration from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BartForConditionalGeneration from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "Processing Samples: 100%|██████████| 10/10 [00:45<00:00,  4.59s/it]\n",
      "Processing Data: 100%|██████████| 1/1 [00:45<00:00, 45.86s/it]\n",
      "Some weights of the model checkpoint at /work/b0990106x/trl/model_output/0725-2005/iter_0/dpo_model were not used when initializing BartForConditionalGeneration: ['v_head.summary.bias', 'v_head.summary.weight']\n",
      "- This IS expected if you are initializing BartForConditionalGeneration from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BartForConditionalGeneration from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Map: 100%|██████████| 1/1 [00:00<00:00, 132.73 examples/s]\n",
      "Map: 100%|██████████| 1/1 [00:00<00:00, 93.52 examples/s]\n",
      "/home/b0990106x/miniconda3/envs/trl/lib/python3.10/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='100' max='100' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [100/100 00:21, Epoch 100/100]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/b0990106x/miniconda3/envs/trl/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Some weights of the model checkpoint at /work/b0990106x/trl/model_output/0725-2005/iter_1/dpo_model were not used when initializing BartForConditionalGeneration: ['v_head.summary.bias', 'v_head.summary.weight']\n",
      "- This IS expected if you are initializing BartForConditionalGeneration from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BartForConditionalGeneration from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Training Iterations:  10%|█         | 2/20 [03:14<29:34, 98.57s/it]Some weights of the model checkpoint at /work/b0990106x/trl/model_output/0725-2005/iter_1/dpo_model were not used when initializing BartForConditionalGeneration: ['v_head.summary.bias', 'v_head.summary.weight']\n",
      "- This IS expected if you are initializing BartForConditionalGeneration from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BartForConditionalGeneration from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "Processing Samples: 100%|██████████| 10/10 [00:18<00:00,  1.84s/it]\n",
      "Processing Data: 100%|██████████| 1/1 [00:18<00:00, 18.37s/it]\n",
      "Some weights of the model checkpoint at /work/b0990106x/trl/model_output/0725-2005/iter_1/dpo_model were not used when initializing BartForConditionalGeneration: ['v_head.summary.bias', 'v_head.summary.weight']\n",
      "- This IS expected if you are initializing BartForConditionalGeneration from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BartForConditionalGeneration from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Map: 100%|██████████| 1/1 [00:00<00:00, 128.20 examples/s]\n",
      "Map: 100%|██████████| 1/1 [00:00<00:00, 190.55 examples/s]\n",
      "/home/b0990106x/miniconda3/envs/trl/lib/python3.10/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='100' max='100' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [100/100 00:07, Epoch 100/100]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/b0990106x/miniconda3/envs/trl/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Some weights of the model checkpoint at /work/b0990106x/trl/model_output/0725-2005/iter_2/dpo_model were not used when initializing BartForConditionalGeneration: ['v_head.summary.bias', 'v_head.summary.weight']\n",
      "- This IS expected if you are initializing BartForConditionalGeneration from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BartForConditionalGeneration from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Training Iterations:  15%|█▌        | 3/20 [04:14<22:51, 80.68s/it]Some weights of the model checkpoint at /work/b0990106x/trl/model_output/0725-2005/iter_2/dpo_model were not used when initializing BartForConditionalGeneration: ['v_head.summary.bias', 'v_head.summary.weight']\n",
      "- This IS expected if you are initializing BartForConditionalGeneration from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BartForConditionalGeneration from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "Processing Samples: 100%|██████████| 10/10 [00:16<00:00,  1.65s/it]\n",
      "Processing Data: 100%|██████████| 1/1 [00:16<00:00, 16.52s/it]\n",
      "Some weights of the model checkpoint at /work/b0990106x/trl/model_output/0725-2005/iter_2/dpo_model were not used when initializing BartForConditionalGeneration: ['v_head.summary.bias', 'v_head.summary.weight']\n",
      "- This IS expected if you are initializing BartForConditionalGeneration from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BartForConditionalGeneration from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Map: 100%|██████████| 1/1 [00:00<00:00, 173.18 examples/s]\n",
      "Map: 100%|██████████| 1/1 [00:00<00:00, 185.94 examples/s]\n",
      "/home/b0990106x/miniconda3/envs/trl/lib/python3.10/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='100' max='100' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [100/100 00:06, Epoch 100/100]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/b0990106x/miniconda3/envs/trl/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Some weights of the model checkpoint at /work/b0990106x/trl/model_output/0725-2005/iter_3/dpo_model were not used when initializing BartForConditionalGeneration: ['v_head.summary.bias', 'v_head.summary.weight']\n",
      "- This IS expected if you are initializing BartForConditionalGeneration from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BartForConditionalGeneration from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Training Iterations:  20%|██        | 4/20 [05:57<23:51, 89.44s/it]Some weights of the model checkpoint at /work/b0990106x/trl/model_output/0725-2005/iter_3/dpo_model were not used when initializing BartForConditionalGeneration: ['v_head.summary.bias', 'v_head.summary.weight']\n",
      "- This IS expected if you are initializing BartForConditionalGeneration from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BartForConditionalGeneration from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "Processing Samples: 100%|██████████| 10/10 [01:02<00:00,  6.24s/it]\n",
      "Processing Data: 100%|██████████| 1/1 [01:02<00:00, 62.41s/it]\n",
      "Some weights of the model checkpoint at /work/b0990106x/trl/model_output/0725-2005/iter_3/dpo_model were not used when initializing BartForConditionalGeneration: ['v_head.summary.bias', 'v_head.summary.weight']\n",
      "- This IS expected if you are initializing BartForConditionalGeneration from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BartForConditionalGeneration from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Map: 100%|██████████| 1/1 [00:00<00:00, 131.22 examples/s]\n",
      "Map: 100%|██████████| 1/1 [00:00<00:00, 97.46 examples/s]\n",
      "/home/b0990106x/miniconda3/envs/trl/lib/python3.10/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='100' max='100' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [100/100 00:21, Epoch 100/100]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/b0990106x/miniconda3/envs/trl/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Some weights of the model checkpoint at /work/b0990106x/trl/model_output/0725-2005/iter_4/dpo_model were not used when initializing BartForConditionalGeneration: ['v_head.summary.bias', 'v_head.summary.weight']\n",
      "- This IS expected if you are initializing BartForConditionalGeneration from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BartForConditionalGeneration from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "# Set up logging\n",
    "logging.basicConfig(\n",
    "    filename=f'{model_output_dir}/log_training.log', \n",
    "    filemode='a', \n",
    "    format='%(asctime)s - %(levelname)s - %(message)s', \n",
    "    level=logging.INFO\n",
    ")\n",
    "\n",
    "logging.info(f\"Parameters:\")\n",
    "logging.info(f\"num_iterations: {num_iterations}\")\n",
    "logging.info(f\"data_size_per_iteration: {data_size_per_iteration}\")\n",
    "logging.info(f\"sample_size: {sample_size}\")\n",
    "logging.info(f\"beta: {beta}\")\n",
    "logging.info(f\"timestep: {ts}\")\n",
    "logging.info(f\"evaluate indices: {eval_selected_indices}\")\n",
    "for i in range(len(eval_selected_indices)):\n",
    "    logging.info(f\"evaluate src_encodec {i}: {all_src_encodec[eval_selected_indices[i]]}\")\n",
    "    logging.info(f\"evaluate instruction {i}: {all_instruction[eval_selected_indices[i]]}\")\n",
    "\n",
    "# Start time\n",
    "total_start_time = time.time()\n",
    "\n",
    "all_metrics = eval_dpo_token_length(ar_checkpoint=ar_checkpoint,\n",
    "                                    trained_model_checkpoint = ar_checkpoint, \n",
    "                                    args_predict = args_predict, \n",
    "                                    all_src_encodec = all_src_encodec,\n",
    "                                    all_instruction = all_instruction,\n",
    "                                    eval_data_len=eval_data_len,\n",
    "                                    num_evaluations = 10,\n",
    "                                    selected_indices=eval_selected_indices)\n",
    "                                    \n",
    "logging.info(f\"Initial Evaluation: {all_metrics}\")\n",
    "\n",
    "for iteration in tqdm(range(num_iterations), desc=\"Training Iterations\"):    \n",
    "    start_idx = 0\n",
    "    end_idx = data_size_per_iteration\n",
    "\n",
    "    batch_src_encodec = all_src_encodec[start_idx:end_idx] # select 'data_size_per_iteration' datas\n",
    "    batch_instruction = all_instruction[start_idx:end_idx]\n",
    "\n",
    "    resume = iteration > 0 # resume from the previous checkpoint when iteration > 0\n",
    "\n",
    "    logging.info(f\"Starting iteration {iteration}\")\n",
    "    logging.info(f\"Processing data from index {start_idx} to {end_idx}\")\n",
    "    \n",
    "    # model_checkpoint is the model checkpoint from the previous iteration\n",
    "    # chosen_rewards and rejected_rewards are the rewards of the data\n",
    "    model_checkpoint, chosen_rewards, rejected_rewards = train_iteration(model_checkpoint=model_checkpoint,\n",
    "                                       iteration=iteration,\n",
    "                                       data_size=data_size_per_iteration,\n",
    "                                       sample_size=sample_size,\n",
    "                                       ar_checkpoint=ar_checkpoint,\n",
    "                                       nar_checkpoint=nar_checkpoint,\n",
    "                                       all_src_encodec=batch_src_encodec,\n",
    "                                       all_instruction=batch_instruction,\n",
    "                                       args_predict=args_predict,\n",
    "                                       agent_output_dir=agent_output_dir,\n",
    "                                       model_output_dir_base=model_output_dir,\n",
    "                                       temperature = 1.0,\n",
    "                                       beta=beta,\n",
    "                                       base_path=base_path,\n",
    "                                       resume_from_checkpoint=resume)\n",
    "    \n",
    "    \n",
    "    logging.info(f\"Prepare Data: Chosen rewards for iteration {iteration}: {chosen_rewards}\")\n",
    "    logging.info(f\"Prepare Data: Rejected rewards for iteration {iteration}: {rejected_rewards}\")\n",
    "    logging.info(f\"Train: Finished training iteration {iteration}\")\n",
    "\n",
    "    # Evaluate the result of the current iteration\n",
    "    all_metrics = eval_dpo_token_length(ar_checkpoint=ar_checkpoint,\n",
    "                                    trained_model_checkpoint = model_checkpoint, \n",
    "                                    args_predict = args_predict, \n",
    "                                    all_src_encodec = all_src_encodec,\n",
    "                                    all_instruction = all_instruction,\n",
    "                                    eval_data_len=eval_data_len,\n",
    "                                    num_evaluations = 10,\n",
    "                                    selected_indices=eval_selected_indices)\n",
    "    \n",
    "    logging.info(f\"Evaluation: {all_metrics}\")\n",
    "\n",
    "total_end_time = time.time()\n",
    "\n",
    "# Calculate total time taken\n",
    "total_time_taken = total_end_time - total_start_time\n",
    "logging.info(f\"Total time taken for the entire process: {total_time_taken:.2f} seconds\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "trl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
