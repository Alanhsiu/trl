{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/b0990106x/miniconda3/envs/trl/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append(\"/work/b0990106x/trl/vc\")\n",
    "import importlib\n",
    "import vc\n",
    "importlib.reload(vc)\n",
    "import torch\n",
    "from vc.trainer_encodec_vc_inference import get_ar_prediction_v3, pack_inputs_v2\n",
    "from types import SimpleNamespace\n",
    "from transformers import BartForConditionalGeneration, AutoModelForCausalLM, AutoTokenizer\n",
    "from NISQA.nisqa.NISQA_model import nisqaModel\n",
    "from datasets import load_from_disk, Dataset\n",
    "from trl import DPOTrainer, DPOConfig, AutoModelForSeq2SeqLMWithValueHead, create_reference_model\n",
    "from vc.encodec_model.nar_bart_model import NARBartForConditionalGeneration\n",
    "from datetime import datetime\n",
    "import os\n",
    "import numpy as np\n",
    "from dpo_eval import get_reward, eval_dpo, eval_dpo_new, eval_dpo_even_token\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "import concurrent.futures\n",
    "import time\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "def token_reward(token_list):\n",
    "    print(token_list)\n",
    "    token_list = [int(token.split(\"_\")[2]) for token in token_list]\n",
    "    # print(\"token_list\",token_list)\n",
    "    even_count = 0\n",
    "    for token in token_list:\n",
    "        if token % 2 == 0:\n",
    "            even_count += 1\n",
    "    reward = (even_count/len(token_list)) * 5\n",
    "    percent = even_count/len(token_list)\n",
    "    total = even_count\n",
    "    return reward, percent, total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "# Function to generate a random token number within the given ranges\n",
    "def random_token_number(even=True):\n",
    "    ranges = [(0, 1024), (8192, 9215)]\n",
    "    range_choice = random.choice(ranges)\n",
    "    num = random.choice(range(range_choice[0], range_choice[1]+1))\n",
    "    while (even and num % 2 != 0) or (not even and num % 2 != 1):\n",
    "        num = random.choice(range(range_choice[0], range_choice[1]+1))\n",
    "    return num\n",
    "\n",
    "# Function to create a token string of random length with varying numbers\n",
    "def create_token_string(even=True):\n",
    "    length = random.randint(50, 1024)\n",
    "    tokens = []\n",
    "    current_length = 0\n",
    "    while current_length < length:\n",
    "        num = random_token_number(even)\n",
    "        token = f\"v_tok_{num}\"\n",
    "        token_length = len(token)\n",
    "        if current_length + token_length <= length:\n",
    "            tokens.append(token)\n",
    "            current_length += token_length\n",
    "        else:\n",
    "            break\n",
    "    return \"\".join(tokens)\n",
    "\n",
    "def generate_token_list(length=100):\n",
    "    chosen_rewards = []\n",
    "    chosen_even_percents = []\n",
    "    chosen_even_totals = []\n",
    "    rejected_rewards = []\n",
    "    rejected_even_percents = []\n",
    "    rejected_even_totals = []\n",
    "\n",
    "    # Define the number range    \n",
    "\n",
    "    # Create the chosen and rejected lists\n",
    "    chosen = [create_token_string(even=True) for _ in range(length)]\n",
    "    rejected = [create_token_string(even=False) for _ in range(length)]\n",
    "\n",
    "    print(\"Chosen:\", chosen)\n",
    "    print(\"Rejected:\", rejected)\n",
    "\n",
    "    for chosen_token in chosen:\n",
    "        # TODO: split chosen_token --> v_tok_620v_tok_362v_tok_100 into a list of tokens\n",
    "        separated_tokens = [f\"v_tok_{num}\" for num in chosen_token.split(\"v_tok_\") if num]\n",
    "        chosen_reward, chosen_even_percent, chosen_even_total = token_reward(separated_tokens)\n",
    "        chosen_rewards.append(chosen_reward)\n",
    "        chosen_even_percents.append(chosen_even_percent)\n",
    "        chosen_even_totals.append(chosen_even_total)\n",
    "\n",
    "    for rejected_token in rejected:\n",
    "        # TODO: split chosen_token --> v_tok_620v_tok_362v_tok_100 into a list of tokens\n",
    "        separated_tokens = [f\"v_tok_{num}\" for num in rejected_token.split(\"v_tok_\") if num]\n",
    "        rejected_reward, rejected_even_percent, rejected_even_total = token_reward(separated_tokens)\n",
    "        rejected_rewards.append(rejected_reward)\n",
    "        rejected_even_percents.append(rejected_even_percent)\n",
    "        rejected_even_totals.append(rejected_even_total)\n",
    "\n",
    "    return chosen, rejected, chosen_rewards, rejected_rewards, chosen_even_percents, rejected_even_percents, chosen_even_totals, rejected_even_totals\n",
    "\n",
    "\n",
    "\n",
    "def load_from_json(file_path):\n",
    "    with open(file_path, 'r') as f:\n",
    "        data = json.load(f)\n",
    "    \n",
    "    all_src_encodec = [item[\"src_encodec\"] for item in data]\n",
    "    all_instruction = [item[\"instruction\"] for item in data]\n",
    "    all_tgt_encodec = [item[\"tgt_encodec\"] for item in data]\n",
    "    \n",
    "    return all_src_encodec, all_instruction, all_tgt_encodec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "def train_model(model, \n",
    "                   model_ref, \n",
    "                   ar_tokenizer, \n",
    "                   train_dataset, \n",
    "                   val_dataset, \n",
    "                   model_output_dir, \n",
    "                   beta, \n",
    "                   resume_from_checkpoint, \n",
    "                   model_checkpoint,\n",
    "                   seed = 42\n",
    "                   ):\n",
    "    # The function trains the model and saves the model to the output directory\n",
    "    if resume_from_checkpoint:\n",
    "        training_args = DPOConfig(\n",
    "            beta=beta,\n",
    "            output_dir=model_output_dir,\n",
    "            generate_during_eval=True,\n",
    "            resume_from_checkpoint=model_checkpoint,\n",
    "            seed = seed,\n",
    "            per_device_train_batch_size=1,\n",
    "            num_train_epochs = 3,\n",
    "            gradient_accumulation_steps = 1\n",
    "        )\n",
    "    else:\n",
    "        training_args = DPOConfig(\n",
    "            beta=beta,\n",
    "            output_dir=model_output_dir,\n",
    "            generate_during_eval=True,\n",
    "            seed = seed, \n",
    "            per_device_train_batch_size=1,\n",
    "            num_train_epochs = 3,\n",
    "            gradient_accumulation_steps = 1\n",
    "        )\n",
    "        \n",
    "    trainer = DPOTrainer(\n",
    "        model=model,\n",
    "        ref_model=model_ref,\n",
    "        args=training_args,\n",
    "        tokenizer=ar_tokenizer,\n",
    "        train_dataset=train_dataset,\n",
    "        eval_dataset=val_dataset,\n",
    "    )\n",
    "    trainer.train()\n",
    "    trainer.save_model(f\"{model_output_dir}/dpo_model\")\n",
    "    model.config.to_json_file(f\"{model_output_dir}/dpo_model/config.json\")\n",
    "    ar_tokenizer.save_pretrained(f\"{model_output_dir}/dpo_model\")\n",
    "\n",
    "def train_iteration(model_checkpoint, \n",
    "                    iteration, \n",
    "                    data_size, \n",
    "                    times_per_data, \n",
    "                    ar_checkpoint, \n",
    "                    nar_checkpoint, \n",
    "                    all_src_encodec, \n",
    "                    all_instruction, \n",
    "                    args_predict, \n",
    "                    agent_output_dir,\n",
    "                    model_output_dir_base, \n",
    "                    beta = 0.1, \n",
    "                    resume_from_checkpoint = False\n",
    "                    ):\n",
    "    print(f\"Iteration {iteration}\")\n",
    "    if iteration == 0: \n",
    "        amount_of_data = data_size\n",
    "        chosen, rejected, chosen_rewards, rejected_rewards, chosen_even_percents, rejected_even_percents, chosen_even_totals, rejected_even_totals = generate_token_list(length = amount_of_data)\n",
    "        # compare and get chosen and rejected\n",
    "    \n",
    "        prompts = []\n",
    "        \n",
    "        # Load the latest model\n",
    "        # ar_model = BartForConditionalGeneration.from_pretrained(model_checkpoint)\n",
    "        ar_tokenizer = AutoTokenizer.from_pretrained(ar_checkpoint)\n",
    "        ar_tokenizer.pad_token = ar_tokenizer.eos_token\n",
    "\n",
    "        for i in range(amount_of_data):\n",
    "            obs_input = pack_inputs_v2(ar_tokenizer, all_src_encodec[i], all_instruction[i])\n",
    "            tokenize_input = ar_tokenizer.convert_ids_to_tokens(obs_input)\n",
    "            tokenize_input_str = ar_tokenizer.convert_tokens_to_string(tokenize_input)\n",
    "            prompts.append(tokenize_input_str)\n",
    "            # prompts.append(tokenize_input_str)\n",
    "\n",
    "    # data = {\n",
    "    #     \"prompt\": prompts,\n",
    "    #     \"chosen\": chosen+chosen,\n",
    "    #     \"rejected\": rejected+rejected,\n",
    "    #     \"chosen_rewards\": chosen_rewards+chosen_rewards,\n",
    "    #     \"chosen_even_percents\": chosen_even_percents+chosen_even_percents,\n",
    "    #     \"chosen_even_totals\": chosen_even_totals+chosen_even_totals,\n",
    "    #     \"rejected_rewards\": rejected_rewards+rejected_rewards,\n",
    "    #     \"rejected_even_percents\": rejected_even_percents+rejected_even_percents,\n",
    "    #     \"rejected_even_totals\": rejected_even_totals+rejected_even_totals\n",
    "    # }\n",
    "        if amount_of_data == 1:\n",
    "            data = {\n",
    "                \"prompt\": prompts+prompts,\n",
    "                \"chosen\": chosen+chosen,\n",
    "                \"rejected\": rejected+rejected,\n",
    "                \"chosen_rewards\": chosen_rewards+chosen_rewards,\n",
    "                \"chosen_even_percents\": chosen_even_percents+chosen_even_percents,\n",
    "                \"chosen_even_totals\": chosen_even_totals+chosen_even_totals,\n",
    "                \"rejected_rewards\": rejected_rewards+rejected_rewards,\n",
    "                \"rejected_even_percents\": rejected_even_percents+rejected_even_percents,\n",
    "                \"rejected_even_totals\": rejected_even_totals+rejected_even_totals\n",
    "            }\n",
    "        else:\n",
    "            data = {\n",
    "                \"prompt\": prompts,\n",
    "                \"chosen\": chosen,\n",
    "                \"rejected\": rejected,\n",
    "                \"chosen_rewards\": chosen_rewards,\n",
    "                \"chosen_even_percents\": chosen_even_percents,\n",
    "                \"chosen_even_totals\": chosen_even_totals,\n",
    "                \"rejected_rewards\": rejected_rewards,\n",
    "                \"rejected_even_percents\": rejected_even_percents,\n",
    "                \"rejected_even_totals\": rejected_even_totals\n",
    "            }           \n",
    "\n",
    "        # Save the JSON to a file\n",
    "        with open(f\"{agent_output_dir}/data_iter_{iteration}.json\", \"w\") as outfile:\n",
    "            json.dump(data, outfile, indent=4)\n",
    "            # Lawrance\n",
    "    else: \n",
    "        ar_tokenizer = AutoTokenizer.from_pretrained(ar_checkpoint)\n",
    "        ar_tokenizer.pad_token = ar_tokenizer.eos_token\n",
    "        with open(f\"{agent_output_dir}/data_iter_0.json\", \"r\") as f:\n",
    "            data = json.load(f)\n",
    "        chosen_rewards = data[\"chosen_rewards\"]\n",
    "        rejected_rewards = data[\"rejected_rewards\"]\n",
    "\n",
    "    data_for_dataset = {key: data[key] for key in [\"prompt\", \"chosen\", \"rejected\"]}\n",
    "\n",
    "    dataset = Dataset.from_dict(data_for_dataset)\n",
    "    dataset_dict = dataset.train_test_split(test_size=0.1)\n",
    "    train_dataset = dataset_dict[\"train\"]\n",
    "    val_dataset = dataset_dict[\"test\"]\n",
    "\n",
    "    # define output directory\n",
    "    model_output_dir = f\"{model_output_dir_base}/iter_{iteration}\"\n",
    "\n",
    "    if not os.path.exists(model_output_dir):\n",
    "        os.makedirs(model_output_dir)\n",
    "\n",
    "    model = AutoModelForSeq2SeqLMWithValueHead.from_pretrained(model_checkpoint, return_dict=True)\n",
    "    model_ref = create_reference_model(model)\n",
    "    \n",
    "    train_model(model=model,\n",
    "                model_ref=model_ref,\n",
    "                ar_tokenizer=ar_tokenizer,\n",
    "                train_dataset=train_dataset,\n",
    "                val_dataset=val_dataset,\n",
    "                model_output_dir=model_output_dir,\n",
    "                beta=beta,\n",
    "                resume_from_checkpoint=resume_from_checkpoint,\n",
    "                model_checkpoint=model_checkpoint)\n",
    "    \n",
    "    return f\"{model_output_dir}/dpo_model\", chosen_rewards, rejected_rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9254\n",
      "9254\n",
      "9254\n",
      "timestamp: 0718-2332\n",
      "length of all_src_encodec: 9254\n",
      "length of all_instruction: 9254\n"
     ]
    }
   ],
   "source": [
    "# Load all data\n",
    "all_src_encodec, all_instruction, all_tgt_encodec = load_from_json('src_encodec.json')\n",
    "print(len(all_src_encodec))\n",
    "print(len(all_instruction))\n",
    "print(len(all_tgt_encodec))\n",
    "\n",
    "# Define paths and device\n",
    "base_path = \"/work/b0990106x/trl\"\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "now = datetime.now()\n",
    "ts = now.strftime(\"%m%d-%H%M\")\n",
    "print(\"timestamp:\", ts)\n",
    "\n",
    "model_output_dir = f\"{base_path}/model_output/{ts}\" # Location where the model are saved\n",
    "agent_input_dir = f\"{base_path}/data-encodec\" # Location of our original data(input) is stored\n",
    "agent_output_dir = f\"{base_path}/output/{ts}\" # Path of saving the generated audio for reward model to evaluate\n",
    "\n",
    "if not os.path.exists(model_output_dir):\n",
    "    os.makedirs(model_output_dir)\n",
    "\n",
    "if not os.path.exists(agent_output_dir):\n",
    "    os.makedirs(agent_output_dir)\n",
    "    \n",
    "args_predict = SimpleNamespace(output_path=f\"{base_path}/output/{ts}/example.wav\", seed=0, device=device)\n",
    "\n",
    "ar_checkpoint = \"lca0503/speech-chatgpt-base-ar-v2-epoch10-wotrans\"\n",
    "nar_checkpoint = \"lca0503/speech-chatgpt-base-nar-v2-epoch4-wotrans\"\n",
    "# Run the iterative training process\n",
    "model_checkpoint = ar_checkpoint # set the initial model checkpoint\n",
    "initial_data_size = 1 # Training: data size for the first iteration\n",
    "data_size_per_iteration = 1 # Training: each iteration will train how many data\n",
    "total_data_size = 1 # Training: total data size that we want to train\n",
    "times_per_data = 5 # doesn't matter here\n",
    "beta = 0.1 # Training: beta value for DPO\n",
    "\n",
    "# num_iterations = (total_data_size - initial_data_size) // data_size_per_iteration + 1 # Training: train how many iterations\n",
    "# randomly select 10 numbers from 0 to len(all_src_encodec)\n",
    "eval_data_len = 1 # Evaluation: evaluate how many data\n",
    "# eval_selected_indices = random.sample(range(len(all_src_encodec)), eval_data_len) # Evaluation: select 10 data for evaluation\n",
    "\n",
    "\n",
    "print(f\"length of all_src_encodec: {len(all_src_encodec)}\") # ~ 9000 data\n",
    "print(f\"length of all_instruction: {len(all_instruction)}\") # ~ 9000 data\n",
    "\n",
    "num_iterations = 30\n",
    "eval_selected_indices = [0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_iterations: 30\n",
      "data_size_per_iteration: 1\n",
      "times_per_data: 5\n",
      "beta: 0.1\n",
      "ar_checkpoint: lca0503/speech-chatgpt-base-ar-v2-epoch10-wotrans\n",
      "nar_checkpoint: lca0503/speech-chatgpt-base-nar-v2-epoch4-wotrans\n",
      "args_predict: namespace(output_path='/work/b0990106x/trl/output/0718-2332/example.wav', seed=0, device='cuda')\n",
      "model_output_dir: /work/b0990106x/trl/model_output/0718-2332\n",
      "agent_output_dir: /work/b0990106x/trl/output/0718-2332\n",
      "base_path: /work/b0990106x/trl\n",
      "device: cuda\n",
      "eval_data_len: 1\n",
      "eval_selected_indices: [0]\n"
     ]
    }
   ],
   "source": [
    "print(f\"num_iterations: {num_iterations}\")\n",
    "print(f\"data_size_per_iteration: {data_size_per_iteration}\")\n",
    "print(f\"times_per_data: {times_per_data}\")\n",
    "print(f\"beta: {beta}\")\n",
    "print(f\"ar_checkpoint: {ar_checkpoint}\")\n",
    "print(f\"nar_checkpoint: {nar_checkpoint}\")\n",
    "print(f\"args_predict: {args_predict}\")\n",
    "print(f\"model_output_dir: {model_output_dir}\")\n",
    "print(f\"agent_output_dir: {agent_output_dir}\")\n",
    "print(f\"base_path: {base_path}\")\n",
    "print(f\"device: {device}\")\n",
    "print(f\"eval_data_len: {eval_data_len}\")\n",
    "print(f\"eval_selected_indices: {eval_selected_indices}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/b0990106x/miniconda3/envs/trl/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Training Iterations:   0%|          | 0/30 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0\n",
      "Chosen: ['v_tok_6v_tok_584v_tok_454v_tok_208v_tok_8498v_tok_8604v_tok_918v_tok_788v_tok_8936v_tok_8776v_tok_8346v_tok_8298v_tok_8504v_tok_988v_tok_8572v_tok_704v_tok_260v_tok_8862v_tok_8520v_tok_8934v_tok_8246v_tok_9198v_tok_9206v_tok_8684v_tok_6v_tok_8726v_tok_890v_tok_954v_tok_250v_tok_104v_tok_8856v_tok_8624v_tok_8908v_tok_9082v_tok_978v_tok_250v_tok_862v_tok_8762v_tok_38v_tok_188v_tok_8290v_tok_344v_tok_714v_tok_696v_tok_348v_tok_578v_tok_9078v_tok_784v_tok_226v_tok_574v_tok_8754v_tok_8672v_tok_8572v_tok_8886v_tok_156v_tok_8940']\n",
      "Rejected: ['v_tok_699v_tok_8773v_tok_8735v_tok_405v_tok_8229v_tok_423v_tok_211v_tok_8763v_tok_669v_tok_357v_tok_811v_tok_8775v_tok_53v_tok_565v_tok_8671v_tok_921v_tok_9071v_tok_8881v_tok_8985v_tok_231v_tok_997v_tok_321v_tok_8851v_tok_8485v_tok_699v_tok_8951v_tok_8887v_tok_8869v_tok_8243v_tok_949v_tok_9203v_tok_8813v_tok_9065v_tok_8399v_tok_8689v_tok_763v_tok_467v_tok_105v_tok_8881v_tok_657v_tok_231v_tok_77v_tok_735v_tok_805v_tok_119v_tok_8677v_tok_45v_tok_723v_tok_8681v_tok_9065v_tok_95v_tok_383v_tok_215v_tok_8573v_tok_331v_tok_375v_tok_555v_tok_903v_tok_485v_tok_8705v_tok_8643v_tok_477v_tok_8325v_tok_8567v_tok_8543v_tok_111v_tok_159v_tok_8283v_tok_8569v_tok_373v_tok_459v_tok_217v_tok_8193v_tok_8925v_tok_8351v_tok_9133v_tok_681v_tok_783v_tok_735v_tok_905v_tok_9215v_tok_231v_tok_689v_tok_15v_tok_19v_tok_8483v_tok_235v_tok_937v_tok_8567v_tok_8299v_tok_8949v_tok_107v_tok_8299v_tok_9079v_tok_323v_tok_599v_tok_169v_tok_8497v_tok_8677v_tok_8701']\n",
      "['v_tok_6', 'v_tok_584', 'v_tok_454', 'v_tok_208', 'v_tok_8498', 'v_tok_8604', 'v_tok_918', 'v_tok_788', 'v_tok_8936', 'v_tok_8776', 'v_tok_8346', 'v_tok_8298', 'v_tok_8504', 'v_tok_988', 'v_tok_8572', 'v_tok_704', 'v_tok_260', 'v_tok_8862', 'v_tok_8520', 'v_tok_8934', 'v_tok_8246', 'v_tok_9198', 'v_tok_9206', 'v_tok_8684', 'v_tok_6', 'v_tok_8726', 'v_tok_890', 'v_tok_954', 'v_tok_250', 'v_tok_104', 'v_tok_8856', 'v_tok_8624', 'v_tok_8908', 'v_tok_9082', 'v_tok_978', 'v_tok_250', 'v_tok_862', 'v_tok_8762', 'v_tok_38', 'v_tok_188', 'v_tok_8290', 'v_tok_344', 'v_tok_714', 'v_tok_696', 'v_tok_348', 'v_tok_578', 'v_tok_9078', 'v_tok_784', 'v_tok_226', 'v_tok_574', 'v_tok_8754', 'v_tok_8672', 'v_tok_8572', 'v_tok_8886', 'v_tok_156', 'v_tok_8940']\n",
      "['v_tok_699', 'v_tok_8773', 'v_tok_8735', 'v_tok_405', 'v_tok_8229', 'v_tok_423', 'v_tok_211', 'v_tok_8763', 'v_tok_669', 'v_tok_357', 'v_tok_811', 'v_tok_8775', 'v_tok_53', 'v_tok_565', 'v_tok_8671', 'v_tok_921', 'v_tok_9071', 'v_tok_8881', 'v_tok_8985', 'v_tok_231', 'v_tok_997', 'v_tok_321', 'v_tok_8851', 'v_tok_8485', 'v_tok_699', 'v_tok_8951', 'v_tok_8887', 'v_tok_8869', 'v_tok_8243', 'v_tok_949', 'v_tok_9203', 'v_tok_8813', 'v_tok_9065', 'v_tok_8399', 'v_tok_8689', 'v_tok_763', 'v_tok_467', 'v_tok_105', 'v_tok_8881', 'v_tok_657', 'v_tok_231', 'v_tok_77', 'v_tok_735', 'v_tok_805', 'v_tok_119', 'v_tok_8677', 'v_tok_45', 'v_tok_723', 'v_tok_8681', 'v_tok_9065', 'v_tok_95', 'v_tok_383', 'v_tok_215', 'v_tok_8573', 'v_tok_331', 'v_tok_375', 'v_tok_555', 'v_tok_903', 'v_tok_485', 'v_tok_8705', 'v_tok_8643', 'v_tok_477', 'v_tok_8325', 'v_tok_8567', 'v_tok_8543', 'v_tok_111', 'v_tok_159', 'v_tok_8283', 'v_tok_8569', 'v_tok_373', 'v_tok_459', 'v_tok_217', 'v_tok_8193', 'v_tok_8925', 'v_tok_8351', 'v_tok_9133', 'v_tok_681', 'v_tok_783', 'v_tok_735', 'v_tok_905', 'v_tok_9215', 'v_tok_231', 'v_tok_689', 'v_tok_15', 'v_tok_19', 'v_tok_8483', 'v_tok_235', 'v_tok_937', 'v_tok_8567', 'v_tok_8299', 'v_tok_8949', 'v_tok_107', 'v_tok_8299', 'v_tok_9079', 'v_tok_323', 'v_tok_599', 'v_tok_169', 'v_tok_8497', 'v_tok_8677', 'v_tok_8701']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/b0990106x/miniconda3/envs/trl/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "/work/b0990106x/trl/trl/trainer/dpo_trainer.py:362: UserWarning: `max_length` is not set in the DPOConfig's init it will default to `512` by default, but you should do it yourself in the future.\n",
      "  warnings.warn(\n",
      "/work/b0990106x/trl/trl/trainer/dpo_trainer.py:375: UserWarning: `max_prompt_length` is not set in the DPOConfig's init it will default to `128` by default, but you should do it yourself in the future.\n",
      "  warnings.warn(\n",
      "/work/b0990106x/trl/trl/trainer/dpo_trainer.py:388: UserWarning: When using an encoder decoder architecture, you should set `max_target_length` in the DPOConfig's init it will default to `128` by default, but you should do it yourself in the future.\n",
      "  warnings.warn(\n",
      "/work/b0990106x/trl/trl/trainer/dpo_trainer.py:410: UserWarning: When using DPODataCollatorWithPadding, you should set `remove_unused_columns=False` in your TrainingArguments we have set it for you, but you should do it yourself in the future.\n",
      "  warnings.warn(\n",
      "Map: 100%|██████████| 1/1 [00:00<00:00, 159.61 examples/s]\n",
      "Map: 100%|██████████| 1/1 [00:00<00:00, 93.65 examples/s]\n",
      "/home/b0990106x/miniconda3/envs/trl/lib/python3.10/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mb09901066\u001b[0m (\u001b[33mb09901066_alan\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.17.4 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/work/b0990106x/trl/wandb/run-20240718_233331-ytf541g0</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/b09901066_alan/huggingface/runs/ytf541g0' target=\"_blank\">worldly-serenity-26</a></strong> to <a href='https://wandb.ai/b09901066_alan/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/b09901066_alan/huggingface' target=\"_blank\">https://wandb.ai/b09901066_alan/huggingface</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/b09901066_alan/huggingface/runs/ytf541g0' target=\"_blank\">https://wandb.ai/b09901066_alan/huggingface/runs/ytf541g0</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3' max='3' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3/3 00:00, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/b0990106x/miniconda3/envs/trl/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Some weights of the model checkpoint at /work/b0990106x/trl/model_output/0718-2332/iter_0/dpo_model were not used when initializing BartForConditionalGeneration: ['v_head.summary.bias', 'v_head.summary.weight']\n",
      "- This IS expected if you are initializing BartForConditionalGeneration from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BartForConditionalGeneration from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Training Iterations:   3%|▎         | 1/30 [01:04<31:18, 64.77s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at /work/b0990106x/trl/model_output/0718-2332/iter_0/dpo_model were not used when initializing BartForConditionalGeneration: ['v_head.summary.bias', 'v_head.summary.weight']\n",
      "- This IS expected if you are initializing BartForConditionalGeneration from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BartForConditionalGeneration from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "/work/b0990106x/trl/trl/trainer/dpo_trainer.py:362: UserWarning: `max_length` is not set in the DPOConfig's init it will default to `512` by default, but you should do it yourself in the future.\n",
      "  warnings.warn(\n",
      "/work/b0990106x/trl/trl/trainer/dpo_trainer.py:375: UserWarning: `max_prompt_length` is not set in the DPOConfig's init it will default to `128` by default, but you should do it yourself in the future.\n",
      "  warnings.warn(\n",
      "/work/b0990106x/trl/trl/trainer/dpo_trainer.py:388: UserWarning: When using an encoder decoder architecture, you should set `max_target_length` in the DPOConfig's init it will default to `128` by default, but you should do it yourself in the future.\n",
      "  warnings.warn(\n",
      "/work/b0990106x/trl/trl/trainer/dpo_trainer.py:410: UserWarning: When using DPODataCollatorWithPadding, you should set `remove_unused_columns=False` in your TrainingArguments we have set it for you, but you should do it yourself in the future.\n",
      "  warnings.warn(\n",
      "Map: 100%|██████████| 1/1 [00:00<00:00, 158.50 examples/s]\n",
      "Map: 100%|██████████| 1/1 [00:00<00:00, 186.36 examples/s]\n",
      "/home/b0990106x/miniconda3/envs/trl/lib/python3.10/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3' max='3' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3/3 00:00, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/b0990106x/miniconda3/envs/trl/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Some weights of the model checkpoint at /work/b0990106x/trl/model_output/0718-2332/iter_1/dpo_model were not used when initializing BartForConditionalGeneration: ['v_head.summary.bias', 'v_head.summary.weight']\n",
      "- This IS expected if you are initializing BartForConditionalGeneration from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BartForConditionalGeneration from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Training Iterations:   7%|▋         | 2/30 [02:03<28:27, 60.99s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at /work/b0990106x/trl/model_output/0718-2332/iter_1/dpo_model were not used when initializing BartForConditionalGeneration: ['v_head.summary.bias', 'v_head.summary.weight']\n",
      "- This IS expected if you are initializing BartForConditionalGeneration from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BartForConditionalGeneration from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "/work/b0990106x/trl/trl/trainer/dpo_trainer.py:362: UserWarning: `max_length` is not set in the DPOConfig's init it will default to `512` by default, but you should do it yourself in the future.\n",
      "  warnings.warn(\n",
      "/work/b0990106x/trl/trl/trainer/dpo_trainer.py:375: UserWarning: `max_prompt_length` is not set in the DPOConfig's init it will default to `128` by default, but you should do it yourself in the future.\n",
      "  warnings.warn(\n",
      "/work/b0990106x/trl/trl/trainer/dpo_trainer.py:388: UserWarning: When using an encoder decoder architecture, you should set `max_target_length` in the DPOConfig's init it will default to `128` by default, but you should do it yourself in the future.\n",
      "  warnings.warn(\n",
      "/work/b0990106x/trl/trl/trainer/dpo_trainer.py:410: UserWarning: When using DPODataCollatorWithPadding, you should set `remove_unused_columns=False` in your TrainingArguments we have set it for you, but you should do it yourself in the future.\n",
      "  warnings.warn(\n",
      "Map: 100%|██████████| 1/1 [00:00<00:00, 188.59 examples/s]\n",
      "Map: 100%|██████████| 1/1 [00:00<00:00, 208.82 examples/s]\n",
      "/home/b0990106x/miniconda3/envs/trl/lib/python3.10/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3' max='3' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3/3 00:00, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/b0990106x/miniconda3/envs/trl/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Some weights of the model checkpoint at /work/b0990106x/trl/model_output/0718-2332/iter_2/dpo_model were not used when initializing BartForConditionalGeneration: ['v_head.summary.bias', 'v_head.summary.weight']\n",
      "- This IS expected if you are initializing BartForConditionalGeneration from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BartForConditionalGeneration from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Training Iterations:  10%|█         | 3/30 [03:01<26:49, 59.59s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at /work/b0990106x/trl/model_output/0718-2332/iter_2/dpo_model were not used when initializing BartForConditionalGeneration: ['v_head.summary.bias', 'v_head.summary.weight']\n",
      "- This IS expected if you are initializing BartForConditionalGeneration from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BartForConditionalGeneration from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "/work/b0990106x/trl/trl/trainer/dpo_trainer.py:362: UserWarning: `max_length` is not set in the DPOConfig's init it will default to `512` by default, but you should do it yourself in the future.\n",
      "  warnings.warn(\n",
      "/work/b0990106x/trl/trl/trainer/dpo_trainer.py:375: UserWarning: `max_prompt_length` is not set in the DPOConfig's init it will default to `128` by default, but you should do it yourself in the future.\n",
      "  warnings.warn(\n",
      "/work/b0990106x/trl/trl/trainer/dpo_trainer.py:388: UserWarning: When using an encoder decoder architecture, you should set `max_target_length` in the DPOConfig's init it will default to `128` by default, but you should do it yourself in the future.\n",
      "  warnings.warn(\n",
      "/work/b0990106x/trl/trl/trainer/dpo_trainer.py:410: UserWarning: When using DPODataCollatorWithPadding, you should set `remove_unused_columns=False` in your TrainingArguments we have set it for you, but you should do it yourself in the future.\n",
      "  warnings.warn(\n",
      "Map: 100%|██████████| 1/1 [00:00<00:00, 186.80 examples/s]\n",
      "Map: 100%|██████████| 1/1 [00:00<00:00, 197.89 examples/s]\n",
      "/home/b0990106x/miniconda3/envs/trl/lib/python3.10/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3' max='3' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3/3 00:00, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/b0990106x/miniconda3/envs/trl/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Some weights of the model checkpoint at /work/b0990106x/trl/model_output/0718-2332/iter_3/dpo_model were not used when initializing BartForConditionalGeneration: ['v_head.summary.bias', 'v_head.summary.weight']\n",
      "- This IS expected if you are initializing BartForConditionalGeneration from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BartForConditionalGeneration from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Training Iterations:  13%|█▎        | 4/30 [03:58<25:26, 58.70s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at /work/b0990106x/trl/model_output/0718-2332/iter_3/dpo_model were not used when initializing BartForConditionalGeneration: ['v_head.summary.bias', 'v_head.summary.weight']\n",
      "- This IS expected if you are initializing BartForConditionalGeneration from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BartForConditionalGeneration from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "/work/b0990106x/trl/trl/trainer/dpo_trainer.py:362: UserWarning: `max_length` is not set in the DPOConfig's init it will default to `512` by default, but you should do it yourself in the future.\n",
      "  warnings.warn(\n",
      "/work/b0990106x/trl/trl/trainer/dpo_trainer.py:375: UserWarning: `max_prompt_length` is not set in the DPOConfig's init it will default to `128` by default, but you should do it yourself in the future.\n",
      "  warnings.warn(\n",
      "/work/b0990106x/trl/trl/trainer/dpo_trainer.py:388: UserWarning: When using an encoder decoder architecture, you should set `max_target_length` in the DPOConfig's init it will default to `128` by default, but you should do it yourself in the future.\n",
      "  warnings.warn(\n",
      "/work/b0990106x/trl/trl/trainer/dpo_trainer.py:410: UserWarning: When using DPODataCollatorWithPadding, you should set `remove_unused_columns=False` in your TrainingArguments we have set it for you, but you should do it yourself in the future.\n",
      "  warnings.warn(\n",
      "Map: 100%|██████████| 1/1 [00:00<00:00, 171.74 examples/s]\n",
      "Map: 100%|██████████| 1/1 [00:00<00:00, 157.86 examples/s]\n",
      "/home/b0990106x/miniconda3/envs/trl/lib/python3.10/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3' max='3' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3/3 00:00, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/b0990106x/miniconda3/envs/trl/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Some weights of the model checkpoint at /work/b0990106x/trl/model_output/0718-2332/iter_4/dpo_model were not used when initializing BartForConditionalGeneration: ['v_head.summary.bias', 'v_head.summary.weight']\n",
      "- This IS expected if you are initializing BartForConditionalGeneration from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BartForConditionalGeneration from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Training Iterations:  17%|█▋        | 5/30 [04:56<24:22, 58.51s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at /work/b0990106x/trl/model_output/0718-2332/iter_4/dpo_model were not used when initializing BartForConditionalGeneration: ['v_head.summary.bias', 'v_head.summary.weight']\n",
      "- This IS expected if you are initializing BartForConditionalGeneration from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BartForConditionalGeneration from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "/work/b0990106x/trl/trl/trainer/dpo_trainer.py:362: UserWarning: `max_length` is not set in the DPOConfig's init it will default to `512` by default, but you should do it yourself in the future.\n",
      "  warnings.warn(\n",
      "/work/b0990106x/trl/trl/trainer/dpo_trainer.py:375: UserWarning: `max_prompt_length` is not set in the DPOConfig's init it will default to `128` by default, but you should do it yourself in the future.\n",
      "  warnings.warn(\n",
      "/work/b0990106x/trl/trl/trainer/dpo_trainer.py:388: UserWarning: When using an encoder decoder architecture, you should set `max_target_length` in the DPOConfig's init it will default to `128` by default, but you should do it yourself in the future.\n",
      "  warnings.warn(\n",
      "/work/b0990106x/trl/trl/trainer/dpo_trainer.py:410: UserWarning: When using DPODataCollatorWithPadding, you should set `remove_unused_columns=False` in your TrainingArguments we have set it for you, but you should do it yourself in the future.\n",
      "  warnings.warn(\n",
      "Map: 100%|██████████| 1/1 [00:00<00:00, 179.34 examples/s]\n",
      "Map: 100%|██████████| 1/1 [00:00<00:00, 206.21 examples/s]\n",
      "/home/b0990106x/miniconda3/envs/trl/lib/python3.10/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3' max='3' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3/3 00:00, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/b0990106x/miniconda3/envs/trl/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Some weights of the model checkpoint at /work/b0990106x/trl/model_output/0718-2332/iter_5/dpo_model were not used when initializing BartForConditionalGeneration: ['v_head.summary.bias', 'v_head.summary.weight']\n",
      "- This IS expected if you are initializing BartForConditionalGeneration from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BartForConditionalGeneration from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Training Iterations:  20%|██        | 6/30 [05:54<23:20, 58.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at /work/b0990106x/trl/model_output/0718-2332/iter_5/dpo_model were not used when initializing BartForConditionalGeneration: ['v_head.summary.bias', 'v_head.summary.weight']\n",
      "- This IS expected if you are initializing BartForConditionalGeneration from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BartForConditionalGeneration from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "/work/b0990106x/trl/trl/trainer/dpo_trainer.py:362: UserWarning: `max_length` is not set in the DPOConfig's init it will default to `512` by default, but you should do it yourself in the future.\n",
      "  warnings.warn(\n",
      "/work/b0990106x/trl/trl/trainer/dpo_trainer.py:375: UserWarning: `max_prompt_length` is not set in the DPOConfig's init it will default to `128` by default, but you should do it yourself in the future.\n",
      "  warnings.warn(\n",
      "/work/b0990106x/trl/trl/trainer/dpo_trainer.py:388: UserWarning: When using an encoder decoder architecture, you should set `max_target_length` in the DPOConfig's init it will default to `128` by default, but you should do it yourself in the future.\n",
      "  warnings.warn(\n",
      "/work/b0990106x/trl/trl/trainer/dpo_trainer.py:410: UserWarning: When using DPODataCollatorWithPadding, you should set `remove_unused_columns=False` in your TrainingArguments we have set it for you, but you should do it yourself in the future.\n",
      "  warnings.warn(\n",
      "Map: 100%|██████████| 1/1 [00:00<00:00, 173.56 examples/s]\n",
      "Map: 100%|██████████| 1/1 [00:00<00:00, 188.47 examples/s]\n",
      "/home/b0990106x/miniconda3/envs/trl/lib/python3.10/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3' max='3' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3/3 00:00, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/b0990106x/miniconda3/envs/trl/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Some weights of the model checkpoint at /work/b0990106x/trl/model_output/0718-2332/iter_6/dpo_model were not used when initializing BartForConditionalGeneration: ['v_head.summary.bias', 'v_head.summary.weight']\n",
      "- This IS expected if you are initializing BartForConditionalGeneration from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BartForConditionalGeneration from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Training Iterations:  23%|██▎       | 7/30 [06:52<22:20, 58.28s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at /work/b0990106x/trl/model_output/0718-2332/iter_6/dpo_model were not used when initializing BartForConditionalGeneration: ['v_head.summary.bias', 'v_head.summary.weight']\n",
      "- This IS expected if you are initializing BartForConditionalGeneration from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BartForConditionalGeneration from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "/work/b0990106x/trl/trl/trainer/dpo_trainer.py:362: UserWarning: `max_length` is not set in the DPOConfig's init it will default to `512` by default, but you should do it yourself in the future.\n",
      "  warnings.warn(\n",
      "/work/b0990106x/trl/trl/trainer/dpo_trainer.py:375: UserWarning: `max_prompt_length` is not set in the DPOConfig's init it will default to `128` by default, but you should do it yourself in the future.\n",
      "  warnings.warn(\n",
      "/work/b0990106x/trl/trl/trainer/dpo_trainer.py:388: UserWarning: When using an encoder decoder architecture, you should set `max_target_length` in the DPOConfig's init it will default to `128` by default, but you should do it yourself in the future.\n",
      "  warnings.warn(\n",
      "/work/b0990106x/trl/trl/trainer/dpo_trainer.py:410: UserWarning: When using DPODataCollatorWithPadding, you should set `remove_unused_columns=False` in your TrainingArguments we have set it for you, but you should do it yourself in the future.\n",
      "  warnings.warn(\n",
      "Map: 100%|██████████| 1/1 [00:00<00:00, 100.90 examples/s]\n",
      "Map: 100%|██████████| 1/1 [00:00<00:00, 131.91 examples/s]\n",
      "/home/b0990106x/miniconda3/envs/trl/lib/python3.10/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3' max='3' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3/3 00:00, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/b0990106x/miniconda3/envs/trl/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Some weights of the model checkpoint at /work/b0990106x/trl/model_output/0718-2332/iter_7/dpo_model were not used when initializing BartForConditionalGeneration: ['v_head.summary.bias', 'v_head.summary.weight']\n",
      "- This IS expected if you are initializing BartForConditionalGeneration from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BartForConditionalGeneration from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Training Iterations:  27%|██▋       | 8/30 [07:50<21:19, 58.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at /work/b0990106x/trl/model_output/0718-2332/iter_7/dpo_model were not used when initializing BartForConditionalGeneration: ['v_head.summary.bias', 'v_head.summary.weight']\n",
      "- This IS expected if you are initializing BartForConditionalGeneration from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BartForConditionalGeneration from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "/work/b0990106x/trl/trl/trainer/dpo_trainer.py:362: UserWarning: `max_length` is not set in the DPOConfig's init it will default to `512` by default, but you should do it yourself in the future.\n",
      "  warnings.warn(\n",
      "/work/b0990106x/trl/trl/trainer/dpo_trainer.py:375: UserWarning: `max_prompt_length` is not set in the DPOConfig's init it will default to `128` by default, but you should do it yourself in the future.\n",
      "  warnings.warn(\n",
      "/work/b0990106x/trl/trl/trainer/dpo_trainer.py:388: UserWarning: When using an encoder decoder architecture, you should set `max_target_length` in the DPOConfig's init it will default to `128` by default, but you should do it yourself in the future.\n",
      "  warnings.warn(\n",
      "/work/b0990106x/trl/trl/trainer/dpo_trainer.py:410: UserWarning: When using DPODataCollatorWithPadding, you should set `remove_unused_columns=False` in your TrainingArguments we have set it for you, but you should do it yourself in the future.\n",
      "  warnings.warn(\n",
      "Map: 100%|██████████| 1/1 [00:00<00:00, 182.42 examples/s]\n",
      "Map: 100%|██████████| 1/1 [00:00<00:00, 197.22 examples/s]\n",
      "/home/b0990106x/miniconda3/envs/trl/lib/python3.10/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3' max='3' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3/3 00:00, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/b0990106x/miniconda3/envs/trl/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Some weights of the model checkpoint at /work/b0990106x/trl/model_output/0718-2332/iter_8/dpo_model were not used when initializing BartForConditionalGeneration: ['v_head.summary.bias', 'v_head.summary.weight']\n",
      "- This IS expected if you are initializing BartForConditionalGeneration from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BartForConditionalGeneration from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Training Iterations:  30%|███       | 9/30 [08:48<20:21, 58.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at /work/b0990106x/trl/model_output/0718-2332/iter_8/dpo_model were not used when initializing BartForConditionalGeneration: ['v_head.summary.bias', 'v_head.summary.weight']\n",
      "- This IS expected if you are initializing BartForConditionalGeneration from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BartForConditionalGeneration from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "/work/b0990106x/trl/trl/trainer/dpo_trainer.py:362: UserWarning: `max_length` is not set in the DPOConfig's init it will default to `512` by default, but you should do it yourself in the future.\n",
      "  warnings.warn(\n",
      "/work/b0990106x/trl/trl/trainer/dpo_trainer.py:375: UserWarning: `max_prompt_length` is not set in the DPOConfig's init it will default to `128` by default, but you should do it yourself in the future.\n",
      "  warnings.warn(\n",
      "/work/b0990106x/trl/trl/trainer/dpo_trainer.py:388: UserWarning: When using an encoder decoder architecture, you should set `max_target_length` in the DPOConfig's init it will default to `128` by default, but you should do it yourself in the future.\n",
      "  warnings.warn(\n",
      "/work/b0990106x/trl/trl/trainer/dpo_trainer.py:410: UserWarning: When using DPODataCollatorWithPadding, you should set `remove_unused_columns=False` in your TrainingArguments we have set it for you, but you should do it yourself in the future.\n",
      "  warnings.warn(\n",
      "Map: 100%|██████████| 1/1 [00:00<00:00, 105.05 examples/s]\n",
      "Map: 100%|██████████| 1/1 [00:00<00:00, 101.85 examples/s]\n",
      "/home/b0990106x/miniconda3/envs/trl/lib/python3.10/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3' max='3' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3/3 00:00, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/b0990106x/miniconda3/envs/trl/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Some weights of the model checkpoint at /work/b0990106x/trl/model_output/0718-2332/iter_9/dpo_model were not used when initializing BartForConditionalGeneration: ['v_head.summary.bias', 'v_head.summary.weight']\n",
      "- This IS expected if you are initializing BartForConditionalGeneration from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BartForConditionalGeneration from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Training Iterations:  33%|███▎      | 10/30 [09:47<19:26, 58.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at /work/b0990106x/trl/model_output/0718-2332/iter_9/dpo_model were not used when initializing BartForConditionalGeneration: ['v_head.summary.bias', 'v_head.summary.weight']\n",
      "- This IS expected if you are initializing BartForConditionalGeneration from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BartForConditionalGeneration from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "/work/b0990106x/trl/trl/trainer/dpo_trainer.py:362: UserWarning: `max_length` is not set in the DPOConfig's init it will default to `512` by default, but you should do it yourself in the future.\n",
      "  warnings.warn(\n",
      "/work/b0990106x/trl/trl/trainer/dpo_trainer.py:375: UserWarning: `max_prompt_length` is not set in the DPOConfig's init it will default to `128` by default, but you should do it yourself in the future.\n",
      "  warnings.warn(\n",
      "/work/b0990106x/trl/trl/trainer/dpo_trainer.py:388: UserWarning: When using an encoder decoder architecture, you should set `max_target_length` in the DPOConfig's init it will default to `128` by default, but you should do it yourself in the future.\n",
      "  warnings.warn(\n",
      "/work/b0990106x/trl/trl/trainer/dpo_trainer.py:410: UserWarning: When using DPODataCollatorWithPadding, you should set `remove_unused_columns=False` in your TrainingArguments we have set it for you, but you should do it yourself in the future.\n",
      "  warnings.warn(\n",
      "Map: 100%|██████████| 1/1 [00:00<00:00, 102.83 examples/s]\n",
      "Map: 100%|██████████| 1/1 [00:00<00:00, 155.64 examples/s]\n",
      "/home/b0990106x/miniconda3/envs/trl/lib/python3.10/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3' max='3' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3/3 00:00, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/b0990106x/miniconda3/envs/trl/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Some weights of the model checkpoint at /work/b0990106x/trl/model_output/0718-2332/iter_10/dpo_model were not used when initializing BartForConditionalGeneration: ['v_head.summary.bias', 'v_head.summary.weight']\n",
      "- This IS expected if you are initializing BartForConditionalGeneration from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BartForConditionalGeneration from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Training Iterations:  37%|███▋      | 11/30 [10:46<18:29, 58.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 11\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at /work/b0990106x/trl/model_output/0718-2332/iter_10/dpo_model were not used when initializing BartForConditionalGeneration: ['v_head.summary.bias', 'v_head.summary.weight']\n",
      "- This IS expected if you are initializing BartForConditionalGeneration from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BartForConditionalGeneration from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "/work/b0990106x/trl/trl/trainer/dpo_trainer.py:362: UserWarning: `max_length` is not set in the DPOConfig's init it will default to `512` by default, but you should do it yourself in the future.\n",
      "  warnings.warn(\n",
      "/work/b0990106x/trl/trl/trainer/dpo_trainer.py:375: UserWarning: `max_prompt_length` is not set in the DPOConfig's init it will default to `128` by default, but you should do it yourself in the future.\n",
      "  warnings.warn(\n",
      "/work/b0990106x/trl/trl/trainer/dpo_trainer.py:388: UserWarning: When using an encoder decoder architecture, you should set `max_target_length` in the DPOConfig's init it will default to `128` by default, but you should do it yourself in the future.\n",
      "  warnings.warn(\n",
      "/work/b0990106x/trl/trl/trainer/dpo_trainer.py:410: UserWarning: When using DPODataCollatorWithPadding, you should set `remove_unused_columns=False` in your TrainingArguments we have set it for you, but you should do it yourself in the future.\n",
      "  warnings.warn(\n",
      "Map: 100%|██████████| 1/1 [00:00<00:00, 111.83 examples/s]\n",
      "Map: 100%|██████████| 1/1 [00:00<00:00, 103.60 examples/s]\n",
      "/home/b0990106x/miniconda3/envs/trl/lib/python3.10/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3' max='3' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3/3 00:00, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/b0990106x/miniconda3/envs/trl/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Some weights of the model checkpoint at /work/b0990106x/trl/model_output/0718-2332/iter_11/dpo_model were not used when initializing BartForConditionalGeneration: ['v_head.summary.bias', 'v_head.summary.weight']\n",
      "- This IS expected if you are initializing BartForConditionalGeneration from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BartForConditionalGeneration from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "\n",
    "# Set up logging\n",
    "logging.basicConfig(\n",
    "    filename=f'{model_output_dir}/log_training.log', \n",
    "    filemode='a', \n",
    "    format='%(asctime)s - %(levelname)s - %(message)s', \n",
    "    level=logging.INFO\n",
    ")\n",
    "\n",
    "logging.info(f\"Parameters:\")\n",
    "logging.info(f\"num_iterations: {num_iterations}\")\n",
    "logging.info(f\"data_size_per_iteration: {data_size_per_iteration}\")\n",
    "logging.info(f\"times_per_data: {times_per_data}\")\n",
    "logging.info(f\"beta: {beta}\")\n",
    "logging.info(f\"timestep: {ts}\")\n",
    "\n",
    "# Start time\n",
    "total_start_time = time.time()\n",
    "\n",
    "all_metrics = eval_dpo_even_token(ar_checkpoint=ar_checkpoint,\n",
    "                                    nar_checkpoint=nar_checkpoint,\n",
    "                                    trained_model_checkpoint=ar_checkpoint, # original model\n",
    "                                    all_src_encodec=all_src_encodec,\n",
    "                                    all_instruction=all_instruction,\n",
    "                                    eval_data_len=eval_data_len,\n",
    "                                    selected_indices=eval_selected_indices,\n",
    "                                    device=device,\n",
    "                                    iteration = -1,\n",
    "                                    args_predict=args_predict)\n",
    "\n",
    "logging.info(f\"Initial Evaluation: {all_metrics}\")\n",
    "\n",
    "for iteration in tqdm(range(num_iterations), desc=\"Training Iterations\"):\n",
    "    start_idx = 0\n",
    "    end_idx = data_size_per_iteration\n",
    "\n",
    "    batch_src_encodec = all_src_encodec[start_idx:end_idx] # select 'data_size_per_iteration' datas\n",
    "    batch_instruction = all_instruction[start_idx:end_idx]\n",
    "    \n",
    "    resume = iteration > 0 # resume from the previous checkpoint when iteration > 0\n",
    "\n",
    "    logging.info(f\"Starting iteration {iteration}\")\n",
    "    logging.info(f\"Processing data from index {start_idx} to {end_idx}\")\n",
    "    \n",
    "    # model_checkpoint is the model checkpoint from the previous iteration\n",
    "    # chosen_rewards and rejected_rewards are the rewards of the data\n",
    "    model_checkpoint, chosen_rewards, rejected_rewards = train_iteration(model_checkpoint=model_checkpoint,\n",
    "                                                                        iteration=iteration,\n",
    "                                                                        data_size=data_size_per_iteration,\n",
    "                                                                        times_per_data=times_per_data,\n",
    "                                                                        ar_checkpoint=ar_checkpoint,\n",
    "                                                                        nar_checkpoint=nar_checkpoint,\n",
    "                                                                        all_src_encodec=batch_src_encodec,\n",
    "                                                                        all_instruction=batch_instruction,\n",
    "                                                                        args_predict=args_predict,\n",
    "                                                                        agent_output_dir=agent_output_dir,\n",
    "                                                                        model_output_dir_base=model_output_dir,\n",
    "                                                                        beta=beta,\n",
    "                                                                        resume_from_checkpoint=resume)\n",
    "    \n",
    "    \n",
    "    logging.info(f\"Chosen rewards for iteration {iteration}: {chosen_rewards}\")\n",
    "    logging.info(f\"Rejected rewards for iteration {iteration}: {rejected_rewards}\")\n",
    "    logging.info(f\"Finished training iteration {iteration}\")\n",
    "\n",
    "    # # Evaluate the result of the current iteration\n",
    "    logging.info(f\"Evaluation Indices: {eval_selected_indices}\")\n",
    "    all_metrics = eval_dpo_even_token(ar_checkpoint=ar_checkpoint,\n",
    "                                    nar_checkpoint=nar_checkpoint,\n",
    "                                    trained_model_checkpoint=model_checkpoint,\n",
    "                                    all_src_encodec=all_src_encodec,\n",
    "                                    all_instruction=all_instruction,\n",
    "                                    eval_data_len=eval_data_len,\n",
    "                                    selected_indices=eval_selected_indices,\n",
    "                                    num_evaluations = 10,\n",
    "                                    device=device,\n",
    "                                    iteration = iteration,\n",
    "                                    args_predict=args_predict)\n",
    "    # Evaluation \n",
    "    logging.info(f\"Evaluation: {all_metrics}\")\n",
    "\n",
    "total_end_time = time.time()\n",
    "\n",
    "# Calculate total time taken\n",
    "total_time_taken = total_end_time - total_start_time\n",
    "logging.info(f\"Total time taken for the entire process: {total_time_taken:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Continue Training from Checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch\n",
    "# from transformers import AutoModelForSeq2SeqLM, AutoTokenizer\n",
    "# from datasets import Dataset\n",
    "# import json\n",
    "# from trl import DPOTrainer, DPOConfig, AutoModelForSeq2SeqLMWithValueHead, create_reference_model\n",
    "\n",
    "# base_path = \"/work/b0990106x/trl\"\n",
    "# model_output_dir = f\"{base_path}/model_output/0718-1514/iter_1_same_data\" # Location where the model are saved\n",
    "# model_input_dir = f\"{base_path}/model_output/0718-1514/iter_0\" # Location where the model are saved\n",
    "# model_checkpoint = f\"{model_input_dir}/dpo_model\"\n",
    "# agent_output_dir = f\"{base_path}/output/0718-1514\" # Path of saving the generated audio for reward model to evaluate\n",
    "# ar_checkpoint = \"lca0503/speech-chatgpt-base-ar-v2-epoch10-wotrans\"\n",
    "# beta = 0.1\n",
    "# # iteration = 0 # for name and number only\n",
    "# # data_size_per_iteration = 100 # Training: each iteration will train how many data\n",
    "# # times_per_data = 5 # doesn't matter here\n",
    "# ar_checkpoint = \"lca0503/speech-chatgpt-base-ar-v2-epoch10-wotrans\"\n",
    "# # beta = 0.1 # Training: beta value for DPO\n",
    "\n",
    "# ar_tokenizer = AutoTokenizer.from_pretrained(ar_checkpoint)\n",
    "# ar_tokenizer.pad_token = ar_tokenizer.eos_token\n",
    "\n",
    "# with open(f\"{agent_output_dir}/data_iter_0.json\", \"r\") as f:\n",
    "#     data = json.load(f)\n",
    "\n",
    "# data_for_dataset = {key: data[key] for key in [\"prompt\", \"chosen\", \"rejected\"]}\n",
    "# dataset = Dataset.from_dict(data_for_dataset)\n",
    "# dataset_dict = dataset.train_test_split(test_size=0.1)\n",
    "# train_dataset = dataset_dict[\"train\"]\n",
    "# val_dataset = dataset_dict[\"test\"]\n",
    "\n",
    "# model = AutoModelForSeq2SeqLMWithValueHead.from_pretrained(model_checkpoint, return_dict=True)\n",
    "# model_ref = create_reference_model(model)\n",
    "\n",
    "# train_model(model=model,\n",
    "#             model_ref=model_ref,\n",
    "#             ar_tokenizer=ar_tokenizer,\n",
    "#             train_dataset=train_dataset,\n",
    "#             val_dataset=val_dataset,\n",
    "#             model_output_dir=model_output_dir,\n",
    "#             beta=beta,\n",
    "#             resume_from_checkpoint=True,\n",
    "#             model_checkpoint=model_checkpoint)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch\n",
    "# from types import SimpleNamespace\n",
    "# import random \n",
    "# import json\n",
    "# from dpo_eval import eval_dpo_token_length\n",
    "\n",
    "# def load_from_json(file_path):\n",
    "#     with open(file_path, 'r') as f:\n",
    "#         data = json.load(f)\n",
    "    \n",
    "#     all_src_encodec = [item[\"src_encodec\"] for item in data]\n",
    "#     all_instruction = [item[\"instruction\"] for item in data]\n",
    "#     all_tgt_encodec = [item[\"tgt_encodec\"] for item in data]\n",
    "    \n",
    "#     return all_src_encodec, all_instruction, all_tgt_encodec\n",
    "\n",
    "\n",
    "# base_path = \"/work/b0990106x/trl\"\n",
    "# ts = \"0718-1514\"\n",
    "# ar_checkpoint = \"lca0503/speech-chatgpt-base-ar-v2-epoch10-wotrans\"\n",
    "# nar_checkpoint = \"lca0503/speech-chatgpt-base-nar-v2-epoch4-wotrans\"\n",
    "# model_checkpoint = f\"{base_path}/model_output/{ts}/iter_1_same_data/dpo_model\"\n",
    "# # model_checkpoint = \"lca0503/speech-chatgpt-base-ar-v2-epoch10-wotrans\"\n",
    "# eval_data_len = 10 # Evaluation: evaluate how many data\n",
    "# # eval_selected_indices from 0 to 99\n",
    "# eval_selected_indices = list(range(10))\n",
    "# # eval_selected_indices = random.sample(range(100), eval_data_len) # Evaluation: select 10 data for evaluation\n",
    "# iteration = 0\n",
    "\n",
    "# #################################### Fixed\n",
    "# all_src_encodec, all_instruction, all_tgt_encodec = load_from_json('src_encodec.json')\n",
    "# print(len(all_src_encodec))\n",
    "# print(len(all_instruction))\n",
    "# print(len(all_tgt_encodec))\n",
    "# device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "# args_predict = SimpleNamespace(output_path=f\"{base_path}/output/{ts}/example.wav\", seed=0, device=device)\n",
    "# ####################################\n",
    "\n",
    "\n",
    "# all_metrics = eval_dpo_token_length(ar_checkpoint=ar_checkpoint,\n",
    "#                                     nar_checkpoint=nar_checkpoint,\n",
    "#                                     trained_model_checkpoint=model_checkpoint,\n",
    "#                                     all_src_encodec=all_src_encodec,\n",
    "#                                     all_instruction=all_instruction,\n",
    "#                                     eval_data_len=eval_data_len,\n",
    "#                                     selected_indices=eval_selected_indices,\n",
    "#                                     num_evaluations = 10,\n",
    "#                                     device=device,\n",
    "#                                     iteration = iteration,\n",
    "#                                     args_predict=args_predict)\n",
    "\n",
    "# print(all_metrics)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "trl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
