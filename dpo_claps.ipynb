{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/b0990106x/miniconda3/envs/trl/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/home/b0990106x/.local/lib/python3.10/site-packages/s3prl/upstream/byol_s/byol_a/common.py:20: UserWarning: torchaudio._backend.set_audio_backend has been deprecated. With dispatcher enabled, this function is no-op. You can remove the function call.\n",
      "  torchaudio.set_audio_backend(\"sox_io\")\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append(\"/work/b0990106x/trl/vc\")\n",
    "import importlib\n",
    "import vc\n",
    "importlib.reload(vc)\n",
    "import torch\n",
    "from vc.trainer_encodec_vc_inference import pack_inputs_v2, get_ar_prediction_get_audio, get_ar_prediction_audio_batch\n",
    "from types import SimpleNamespace\n",
    "from transformers import BartForConditionalGeneration, AutoTokenizer\n",
    "from datasets import Dataset\n",
    "from trl import DPOTrainer, DPOConfig, AutoModelForSeq2SeqLMWithValueHead, create_reference_model\n",
    "from vc.encodec_model.nar_bart_model import NARBartForConditionalGeneration\n",
    "from datetime import datetime\n",
    "import os\n",
    "import numpy as np\n",
    "from dpo_eval import get_reward_claps ,eval_dpo_claps_batch, convert_array_to_tensor_format\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "from typing import List, Tuple\n",
    "import random\n",
    "import argparse\n",
    "\n",
    "sys.path.append('/work/b0990106x/trl/CLAPS')\n",
    "from CLAPS.inference import load_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seed(seed):\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(seed)\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "def generate_output_batch(\n",
    "        ar_model, \n",
    "        nar_model, \n",
    "        ar_tokenizer, \n",
    "        nar_tokenizer, \n",
    "        clap_model,\n",
    "        accelerator,\n",
    "        src_encodec: list, \n",
    "        instruction: list, \n",
    "        args_predict: SimpleNamespace, \n",
    "        episode_counter: int = 0, \n",
    "        base_path: str = \"/work/b0990106x/trl\", \n",
    "        temperature: float = 1.0\n",
    ") -> tuple[float, str]:\n",
    "    '''\n",
    "    Generates output from AR model, synthesize the audio, and evaluate the audio using NISQA.\n",
    "    Returns:\n",
    "        tuple:\n",
    "            reward(float): The reward of the audio.\n",
    "            tokenized_decode_ar(str): The tokenized output of the AR model - first layer.\n",
    "    '''\n",
    "    # Generate predictions using the AR model\n",
    "    audio_list, decode_ar_list = get_ar_prediction_audio_batch(\n",
    "        args_predict, ar_model, nar_model, ar_tokenizer, nar_tokenizer, src_encodec, instruction, episode_counter, temperature=temperature\n",
    "    )\n",
    "    # extract the instruction from the list \n",
    "    reward_list,tokenized_decode_ar_list = [], []\n",
    "\n",
    "    for i, audio in enumerate(audio_list): \n",
    "        # audio ---> tensor([])\n",
    "        if audio is not None:\n",
    "            tensor_audio = convert_array_to_tensor_format(audio)\n",
    "            if tensor_audio[0].shape[0]==1:\n",
    "                tensor_audio[0] = tensor_audio[0].squeeze(0)\n",
    "            # print(tensor_audio)\n",
    "            reward = get_reward_claps(clap_model=clap_model, accelerator=accelerator, prompts = instruction[i], wavs = tensor_audio)\n",
    "        else: \n",
    "            reward = 0\n",
    "        reward_list.append(reward)\n",
    "    \n",
    "    for decode_ar in decode_ar_list:\n",
    "        list_decode_ar = decode_ar.flatten().tolist()   \n",
    "        filtered_decode_ar_list = list_decode_ar[2:-1]\n",
    "        decode_ar_tokens = ar_tokenizer.convert_ids_to_tokens(filtered_decode_ar_list)\n",
    "        tokenized_decode_ar = ar_tokenizer.convert_tokens_to_string(decode_ar_tokens)\n",
    "        tokenized_decode_ar_list.append(tokenized_decode_ar)\n",
    "        \n",
    "    return reward_list, tokenized_decode_ar_list\n",
    "\n",
    "def extract_data_from_json(file_path: str) -> Tuple[List[list], List[str], List[list]]:\n",
    "    \"\"\"\n",
    "    Loads data from a JSON file and extracts 'src_encodec', 'instruction', and 'tgt_encodec'.\n",
    "\n",
    "    Args:\n",
    "        file_path (str): The path to the JSON file.\n",
    "\n",
    "    Returns:\n",
    "        tuple:\n",
    "            all_src_encodec (List[list]): A list containing the 'src_encodec' data from each item in the JSON file.\n",
    "            all_instruction (List[str]): A list containing the 'instruction' data from each item in the JSON file.\n",
    "            all_tgt_encodec (List[list]): A list containing the 'tgt_encodec' data from each item in the JSON file.\n",
    "    \"\"\"\n",
    "    with open(file_path, 'r') as f:\n",
    "        data = json.load(f)\n",
    "\n",
    "    all_src_encodec = [item[\"src_encodec\"] for item in data]\n",
    "    all_instruction = [item[\"instruction\"] for item in data]\n",
    "\n",
    "    return all_src_encodec, all_instruction\n",
    "\n",
    "def train_model(\n",
    "        model,\n",
    "        model_ref,\n",
    "        ar_tokenizer,\n",
    "        train_dataset: Dataset,\n",
    "        val_dataset: Dataset,\n",
    "        model_output_dir: str,\n",
    "        beta: float,\n",
    "        resume_from_checkpoint: bool,\n",
    "        model_checkpoint: str,\n",
    "        learning_rate: float = 5e-07,\n",
    "        num_train_epochs: int = 200,\n",
    "        max_length: int = 1024*9,\n",
    "        max_prompt_length: int = 1024*9,\n",
    "        max_target_length: int = 1024*9,\n",
    "        per_device_train_batch_size: int = 1,\n",
    "        gradient_accumulation_steps: int = 1,\n",
    "        seed: int = 42\n",
    ") -> None:\n",
    "    '''\n",
    "    Train the DPO model and save the model.\n",
    "\n",
    "    Args:\n",
    "        model(AutoModelForSeq2SeqLMWithValueHead): The DPO model.\n",
    "        model_ref(AutoModelForCausalLM): The reference model.\n",
    "        ar_tokenizer(AutoTokenizer): The tokenizer.\n",
    "        train_dataset(Dataset): The training dataset.\n",
    "        val_dataset(Dataset): The validation dataset.\n",
    "        model_output_dir(str): The output directory for the model.\n",
    "        beta(float): The beta value.\n",
    "        resume_from_checkpoint(bool): Whether to resume from a checkpoint.\n",
    "        model_checkpoint(str): The path to the model\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    '''\n",
    "\n",
    "    training_args = DPOConfig(\n",
    "        beta = beta,\n",
    "        output_dir = model_output_dir,\n",
    "        resume_from_checkpoint = model_checkpoint if resume_from_checkpoint else None,\n",
    "        seed = seed,\n",
    "        per_device_train_batch_size = per_device_train_batch_size,\n",
    "        num_train_epochs = num_train_epochs,\n",
    "        gradient_accumulation_steps = gradient_accumulation_steps,\n",
    "        learning_rate = learning_rate,\n",
    "        max_length = max_length,\n",
    "        max_prompt_length = max_prompt_length,\n",
    "        max_target_length = max_target_length,\n",
    "        evaluation_strategy=\"steps\",\n",
    "        save_steps = 5000,\n",
    "        logging_dir = f\"{model_output_dir}/logs\"\n",
    "    )\n",
    "    \n",
    "    trainer = DPOTrainer(\n",
    "        model=model,\n",
    "        ref_model=model_ref,\n",
    "        args=training_args,\n",
    "        tokenizer=ar_tokenizer,\n",
    "        train_dataset=train_dataset,\n",
    "        eval_dataset=val_dataset,\n",
    "    )\n",
    "    # Train the model\n",
    "    trainer.train()\n",
    "\n",
    "    # Save the model\n",
    "    trainer.save_model(f\"{model_output_dir}/dpo_model\")\n",
    "    model.config.to_json_file(f\"{model_output_dir}/dpo_model/config.json\")\n",
    "    ar_tokenizer.save_pretrained(f\"{model_output_dir}/dpo_model\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "def process_data_batch(sample_size: int, \n",
    "                        ar_model, \n",
    "                        nar_model, \n",
    "                        ar_tokenizer, \n",
    "                        nar_tokenizer, \n",
    "                        clap_model,\n",
    "                        accelerator,\n",
    "                        all_src_encodec: List[list], \n",
    "                        all_instruction: List[str],\n",
    "                        args_predict: SimpleNamespace, \n",
    "                        base_path: str = \"/work/b0990106x/trl\", \n",
    "                        temperature: float = 1.0, \n",
    "                        iteration: int = 0\n",
    ") -> Tuple[List[str], List[str], List[str], List[float], List[float], List[float]]:\n",
    "    # If sample size is 1, we cannot choose the best and worst outputs\n",
    "    if sample_size < 2:\n",
    "        raise ValueError(\"Parameter 'sample_size' must be greater than 1.\")\n",
    "\n",
    "    chosen, rejected, prompts, chosen_rewards, rejected_rewards, average_rewards = [], [], [], [], [], []\n",
    "\n",
    "\n",
    "    for i in tqdm(range(len(all_src_encodec)), desc=\"Processing Data\"):\n",
    "        rewards, tokenized_outputs = [], []\n",
    "        size_of_packed_input = (\n",
    "            len(all_src_encodec[i][0]) +\n",
    "            len(ar_tokenizer(all_instruction[i])[\"input_ids\"][1:-1]) +\n",
    "            3\n",
    "        )\n",
    "        if 4 < size_of_packed_input <= 1024:\n",
    "            all_src_encodec_list = [all_src_encodec[i]]*sample_size\n",
    "            all_instruction_list = [all_instruction[i]]*sample_size\n",
    "            rewards, tokenized_outputs = generate_output_batch(\n",
    "                ar_model=ar_model, \n",
    "                nar_model=nar_model, \n",
    "                ar_tokenizer=ar_tokenizer, \n",
    "                nar_tokenizer=nar_tokenizer,\n",
    "                src_encodec = all_src_encodec_list,\n",
    "                instruction=all_instruction_list, \n",
    "                clap_model=clap_model,\n",
    "                accelerator=accelerator,\n",
    "                args_predict=args_predict,\n",
    "                episode_counter=f\"data_{i}\",\n",
    "                base_path=base_path, \n",
    "                temperature=temperature\n",
    "            )\n",
    "\n",
    "        valid_rewards = [r for r in rewards if r is not None]\n",
    "        valid_outputs = [tokenized_outputs[j] for j in range(len(rewards)) if rewards[j] is not None]\n",
    "\n",
    "        if len(valid_rewards) >= 2:\n",
    "            max_reward_index = np.argmax(valid_rewards)\n",
    "            min_reward_index = np.argmin(valid_rewards)\n",
    "            average_reward = np.mean(valid_rewards)\n",
    "            chosen_output = valid_outputs[max_reward_index]\n",
    "            rejected_output = valid_outputs[min_reward_index]\n",
    "\n",
    "            obs_input = pack_inputs_v2(ar_tokenizer, all_src_encodec[i], all_instruction[i])\n",
    "            tokenize_input = ar_tokenizer.convert_ids_to_tokens(obs_input)\n",
    "            tokenize_input_str = ar_tokenizer.convert_tokens_to_string(tokenize_input)\n",
    "            prompts.append(tokenize_input_str)\n",
    "\n",
    "            chosen.append(chosen_output)\n",
    "            chosen_rewards.append(valid_rewards[max_reward_index])\n",
    "            rejected.append(rejected_output)\n",
    "            rejected_rewards.append(valid_rewards[min_reward_index])\n",
    "            average_rewards.append(average_reward)\n",
    "        else:\n",
    "            print(f\"Not enough valid rewards for data index {i}\")\n",
    "\n",
    "    # If there is only one data, we need to double the data because we need it for training set and validation set\n",
    "    if len(all_src_encodec) == 1:\n",
    "        chosen *= 2\n",
    "        rejected *= 2\n",
    "        prompts *= 2\n",
    "        chosen_rewards *= 2\n",
    "        rejected_rewards *= 2\n",
    "        average_rewards *= 2    \n",
    "    \n",
    "    return chosen, rejected, prompts, chosen_rewards, rejected_rewards, average_rewards\n",
    "\n",
    "def generate_data(ar_model, \n",
    "                  ar_tokenizer, \n",
    "                  nar_model, \n",
    "                  nar_tokenizer, \n",
    "                  clap_model,\n",
    "                  accelerator,\n",
    "                  selected_src_encodec: List[list], \n",
    "                  selected_instruction: List[str],\n",
    "                  args_predict: SimpleNamespace, \n",
    "                  sample_size: int, \n",
    "                  iteration: int, \n",
    "                  agent_output_dir: str, \n",
    "                  base_path: str = \"/work/b0990106x/trl\", \n",
    "                  temperature: float = 1.0\n",
    ") -> Tuple[dict, List[float], List[float]]:\n",
    "    \"\"\"\n",
    "    Generates data for the dataset and saves info to a JSON file.\n",
    "    Returns:\n",
    "        tuple:\n",
    "            data_for_dataset (dict): A dictionary containing the data for the dataset.\n",
    "            chosen_rewards (List[float]): A list of rewards for the chosen outputs.\n",
    "            rejected_rewards (List[float]): A list of rewards for the rejected outputs.\n",
    "    \"\"\"\n",
    "    chosen, rejected, prompts, chosen_rewards, rejected_rewards, average_rewards = process_data_batch(\n",
    "        sample_size=sample_size,\n",
    "        ar_model=ar_model,\n",
    "        nar_model=nar_model,\n",
    "        ar_tokenizer=ar_tokenizer,\n",
    "        nar_tokenizer=nar_tokenizer,\n",
    "        all_src_encodec=selected_src_encodec,\n",
    "        all_instruction=selected_instruction,\n",
    "        args_predict=args_predict,\n",
    "        base_path=base_path,\n",
    "        temperature=temperature,\n",
    "        iteration = iteration,\n",
    "        clap_model=clap_model,\n",
    "        accelerator=accelerator\n",
    "    )\n",
    "\n",
    "    data = {\n",
    "        \"prompt\": prompts,\n",
    "        \"chosen\": chosen,\n",
    "        \"rejected\": rejected,\n",
    "        \"chosen_rewards\": chosen_rewards,\n",
    "        \"rejected_rewards\": rejected_rewards,\n",
    "        \"average_rewards\": average_rewards\n",
    "    }\n",
    "\n",
    "    with open(f\"{agent_output_dir}/data_iter_{iteration}.json\", \"w\") as outfile:\n",
    "        json.dump(data, outfile, indent=4)\n",
    "\n",
    "    data_for_dataset = {key: data[key] for key in [\"prompt\", \"chosen\", \"rejected\"]}\n",
    "\n",
    "    return data_for_dataset, chosen_rewards, rejected_rewards\n",
    "\n",
    "def train_iteration(model_checkpoint, \n",
    "                    iteration, \n",
    "                    data_size, \n",
    "                    sample_size, \n",
    "                    ar_checkpoint, \n",
    "                    nar_checkpoint, \n",
    "                    all_src_encodec, \n",
    "                    all_instruction, \n",
    "                    args_predict, \n",
    "                    agent_output_dir,\n",
    "                    model_output_dir_base, \n",
    "                    clap_model,\n",
    "                    accelerator,\n",
    "                    beta = 0.1, \n",
    "                    temperature = 1.0,\n",
    "                    base_path=\"/work/b0990106x/trl\",\n",
    "                    resume_from_checkpoint = False,\n",
    "                    learning_rate = 5e-07,\n",
    "                    num_train_epochs = 100,\n",
    "                    max_length = 1024*9,\n",
    "                    max_prompt_length = 1024*9,\n",
    "                    max_target_length = 1024*9,\n",
    "                    per_device_train_batch_size = 1,\n",
    "                    gradient_accumulation_steps = 1,\n",
    "                    seed = 42,\n",
    "):\n",
    "    \"\"\"\n",
    "    Executes one training iteration: generates data, trains the model, and saves the output.\n",
    "    \"\"\"\n",
    "    # print(f\"Iteration {iteration}\")\n",
    "\n",
    "    ar_model = BartForConditionalGeneration.from_pretrained(model_checkpoint)\n",
    "    ar_tokenizer = AutoTokenizer.from_pretrained(ar_checkpoint)\n",
    "    # ar_tokenizer.pad_token = ar_tokenizer.eos_token\n",
    "    nar_model = NARBartForConditionalGeneration.from_pretrained(nar_checkpoint)\n",
    "    nar_tokenizer = AutoTokenizer.from_pretrained(nar_checkpoint)\n",
    "\n",
    "    selected_src_encodec = all_src_encodec[:data_size]\n",
    "    selected_instruction = all_instruction[:data_size]\n",
    "\n",
    "    data_for_dataset, chosen_rewards, rejected_rewards = generate_data(ar_model=ar_model,\n",
    "                                                                        ar_tokenizer=ar_tokenizer,\n",
    "                                                                        nar_model=nar_model,\n",
    "                                                                        nar_tokenizer=nar_tokenizer,\n",
    "                                                                        selected_src_encodec=selected_src_encodec,\n",
    "                                                                        selected_instruction=selected_instruction,\n",
    "                                                                        args_predict=args_predict,\n",
    "                                                                        sample_size=sample_size,\n",
    "                                                                        iteration=iteration,\n",
    "                                                                        agent_output_dir=agent_output_dir,\n",
    "                                                                        base_path=base_path,\n",
    "                                                                        temperature=temperature,\n",
    "                                                                        clap_model=clap_model,\n",
    "                                                                        accelerator=accelerator)\n",
    "\n",
    "    dataset = Dataset.from_dict(data_for_dataset)\n",
    "    dataset_dict = dataset.train_test_split(test_size=0.1)\n",
    "    train_dataset = dataset_dict[\"train\"]\n",
    "    val_dataset = dataset_dict[\"test\"]\n",
    "\n",
    "    model_output_dir = f\"{model_output_dir_base}/iter_{iteration}\"\n",
    "    os.makedirs(model_output_dir, exist_ok=True)\n",
    "\n",
    "    model = AutoModelForSeq2SeqLMWithValueHead.from_pretrained(model_checkpoint, return_dict=True)\n",
    "    model_ref = create_reference_model(model)\n",
    "    \n",
    "    train_model(model=model,\n",
    "                model_ref=model_ref,\n",
    "                ar_tokenizer=ar_tokenizer,\n",
    "                train_dataset=train_dataset,\n",
    "                val_dataset=val_dataset,\n",
    "                model_output_dir=model_output_dir,\n",
    "                beta=beta,\n",
    "                resume_from_checkpoint=resume_from_checkpoint,\n",
    "                model_checkpoint=model_checkpoint,\n",
    "                learning_rate = learning_rate,\n",
    "                num_train_epochs = num_train_epochs,\n",
    "                max_length = max_length,\n",
    "                max_prompt_length = max_prompt_length,\n",
    "                max_target_length = max_target_length,\n",
    "                per_device_train_batch_size = per_device_train_batch_size,\n",
    "                gradient_accumulation_steps = gradient_accumulation_steps,\n",
    "                seed = seed)\n",
    "\n",
    "    return f\"{model_output_dir}/dpo_model\", chosen_rewards, rejected_rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "timestamp: 1015-0010\n",
      "length of all_src_encodec: 9254\n",
      "length of all_instruction: 9254\n"
     ]
    }
   ],
   "source": [
    "# Load all data\n",
    "all_src_encodec, all_instruction = extract_data_from_json('dpo_data/src_encodec.json')\n",
    "\n",
    "# Define paths and device\n",
    "base_path = \"/work/b0990106x/trl\"\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# Define timestamp\n",
    "now = datetime.now()\n",
    "ts = now.strftime(\"%m%d-%H%M\")\n",
    "print(\"timestamp:\", ts)\n",
    "\n",
    "# Define paths\n",
    "model_output_dir = os.path.join(base_path, \"model_output\", ts) # Location where the model are saved\n",
    "agent_output_dir = os.path.join(base_path, \"output\", ts) # Path of saving the generated audio for reward model to evaluate\n",
    "os.makedirs(model_output_dir, exist_ok=True)\n",
    "os.makedirs(agent_output_dir, exist_ok=True)\n",
    "\n",
    "seed = 42 # Training: seed\n",
    "\n",
    "# Define arguments \n",
    "args_predict = SimpleNamespace(output_path=f\"{base_path}/output/{ts}/example.wav\", seed=seed, device=device)\n",
    "ar_checkpoint = \"lca0503/speech-chatgpt-base-ar-v2-epoch10-wotrans\"\n",
    "nar_checkpoint = \"lca0503/speech-chatgpt-base-nar-v2-epoch4-wotrans\"\n",
    "\n",
    "# Models and Iterations\n",
    "model_checkpoint = ar_checkpoint # Prepare: set the initial model checkpoint\n",
    "sample_size = 5 # Prepare Dataset: generate how many outputs to select max and min for chosen and rejected\n",
    "num_iterations = 10  # Training: train how many iterations\n",
    "train_selected_indices = [x for x in range(2)] # Training: train on selected data indicies from all_src_encodec\n",
    " # Training: train on selected data indicies from all_src_encodec\n",
    "data_size_per_iteration = 2 # Training: each iteration will train how many data\n",
    "\n",
    "# Define Training Configuration\n",
    "beta = 0.1 # Training: beta value for DPO\n",
    "learning_rate = 5e-07 # Training: learning rate\n",
    "num_train_epochs = 100 # Training: number of training epochs\n",
    "max_length = 1024*9 # Training: max length of the model\n",
    "max_prompt_length = 1024*9 # Training: max length of the prompt\n",
    "max_target_length = 1024*9 # Training: max length of the target\n",
    "per_device_train_batch_size = 1 # Training: batch size\n",
    "gradient_accumulation_steps = 1 # Training: gradient accumulation steps\n",
    "\n",
    "\n",
    "# Evaluation Configuration\n",
    "eval_train = True # Evaluation: evaluate on training data or not\n",
    "eval_test = False # Evaluation: evaluate on testing data or not\n",
    "eval_train_indices = train_selected_indices # Evaluation: evaluate on training data indicies from all_src_encodec\n",
    "eval_test_indices = [10,11] # Evaluation: evaluate on testing data indicies from all_src_encodec\n",
    "eval_train_data_len = 1000 # Evaluation: evaluate how many training data\n",
    "eval_test_data_len = 1 # Evaluation: evaluate how many testing data\n",
    "num_eval = 10 # Evaluation: evaluate how many times per data\n",
    "eval_frequency = 1 # Evaluation: evaluate every how many iterations\n",
    "# Define temperature\n",
    "# eval_selected_indices = random.sample(range(len(all_src_encodec)), eval_data_len) # Evaluation: select 10 data for evaluation\n",
    "print(f\"length of all_src_encodec: {len(all_src_encodec)}\") # ~ 9000 data\n",
    "print(f\"length of all_instruction: {len(all_instruction)}\") # ~ 9000 data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Detected kernel version 3.10.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/b0990106x/.local/lib/python3.10/site-packages/huggingface_hub/file_download.py:1150: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "/home/b0990106x/.local/lib/python3.10/site-packages/s3prl/upstream/wavlm/expert.py:37: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(ckpt)\n",
      "/home/b0990106x/.local/lib/python3.10/site-packages/torch/nn/utils/weight_norm.py:134: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n",
      "/home/b0990106x/.local/lib/python3.10/site-packages/torch/nn/functional.py:5193: UserWarning: Support for mismatched key_padding_mask and attn_mask is deprecated. Use same type for both instead.\n",
      "  warnings.warn(\n",
      "/work/b0990106x/trl/CLAPS/inference.py:33: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(checkpoint_path, map_location=device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded checkpoint from /work/b0990106x/trl/CLAPS/pretrained/7d/cp_claps_blstm_m_50k_v3/cp_0045000/pytorch_model.bin\n"
     ]
    }
   ],
   "source": [
    "sr = 24000\n",
    "text_enc_name = \"google/flan-t5-large\"\n",
    "text_enc_dim = 1024\n",
    "text_blstm_dim = 256\n",
    "speech_enc_name = \"wavlm\"\n",
    "speech_enc_dim = 768\n",
    "speech_blstm_dim = 256\n",
    "rep_dim = 512\n",
    "sub_dim = 0\n",
    "n_sub = 1\n",
    "ckpt_pth=f'{base_path}/CLAPS/pretrained/7d/cp_claps_blstm_m_50k_v3/cp_0045000'\n",
    "project_dir = \"cp_claps\"\n",
    "\n",
    "\n",
    "a = argparse.Namespace(\n",
    "        sr=sr,\n",
    "        text_enc_name=text_enc_name,\n",
    "        text_enc_dim=text_enc_dim,\n",
    "        text_blstm_dim=text_blstm_dim,\n",
    "        speech_enc_name=speech_enc_name,\n",
    "        speech_enc_dim=speech_enc_dim,\n",
    "        speech_blstm_dim=speech_blstm_dim,\n",
    "        rep_dim=rep_dim,\n",
    "        sub_dim=sub_dim,\n",
    "        n_sub=n_sub,  # Number of subspaces, if any\n",
    "        ckpt_pth=ckpt_pth,  # Set your checkpoint path\n",
    "        project_dir=project_dir  # Example project directory\n",
    "    )\n",
    "\n",
    "clap_model, accelerator = load_model(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_iterations: 10\n",
      "data_size_per_iteration: 2\n",
      "sample_size: 5\n",
      "beta: 0.1\n",
      "ar_checkpoint: lca0503/speech-chatgpt-base-ar-v2-epoch10-wotrans\n",
      "nar_checkpoint: lca0503/speech-chatgpt-base-nar-v2-epoch4-wotrans\n",
      "args_predict: namespace(output_path='/work/b0990106x/trl/output/1015-0010/example.wav', seed=42, device='cuda')\n",
      "model_output_dir: /work/b0990106x/trl/model_output/1015-0010\n",
      "agent_output_dir: /work/b0990106x/trl/output/1015-0010\n",
      "base_path: /work/b0990106x/trl\n",
      "device: cuda\n",
      "eval_train_data_len: 1000\n",
      "eval_test_data_len: 1\n",
      "eval_train_indices: [0, 1]\n",
      "eval_test_indices: [10, 11]\n",
      "eval_train: True\n",
      "eval_test: False\n",
      "num_eval: 10\n",
      "['Play the audio twice.', 'Mildly decrease the emphasis on the higher frequencies.']\n"
     ]
    }
   ],
   "source": [
    "print(f\"num_iterations: {num_iterations}\")\n",
    "print(f\"data_size_per_iteration: {data_size_per_iteration}\")\n",
    "print(f\"sample_size: {sample_size}\")\n",
    "print(f\"beta: {beta}\")\n",
    "print(f\"ar_checkpoint: {ar_checkpoint}\")\n",
    "print(f\"nar_checkpoint: {nar_checkpoint}\")\n",
    "print(f\"args_predict: {args_predict}\")\n",
    "print(f\"model_output_dir: {model_output_dir}\")\n",
    "print(f\"agent_output_dir: {agent_output_dir}\")\n",
    "print(f\"base_path: {base_path}\")\n",
    "print(f\"device: {device}\")\n",
    "print(f\"eval_train_data_len: {eval_train_data_len}\")\n",
    "print(f\"eval_test_data_len: {eval_test_data_len}\")\n",
    "print(f\"eval_train_indices: {eval_train_indices}\")\n",
    "print(f\"eval_test_indices: {eval_test_indices}\")\n",
    "print(f\"eval_train: {eval_train}\")\n",
    "print(f\"eval_test: {eval_test}\")\n",
    "print(f\"num_eval: {num_eval}\")\n",
    "\n",
    "print(all_instruction[0:2])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/b0990106x/.local/lib/python3.10/site-packages/transformers/modeling_utils.py:460: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  return torch.load(checkpoint_file, map_location=\"cpu\")\n",
      "Processing Data: 100%|██████████| 2/2 [00:12<00:00,  6.25s/it]\n",
      "/work/b0990106x/trl/trl/models/modeling_base.py:328: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  state_dict = loading_func(filename if not use_safe else safe_filename, **load_kwargs)\n",
      "/work/b0990106x/trl/trl/trainer/dpo_trainer.py:410: UserWarning: When using DPODataCollatorWithPadding, you should set `remove_unused_columns=False` in your TrainingArguments we have set it for you, but you should do it yourself in the future.\n",
      "  warnings.warn(\n",
      "Map: 100%|██████████| 1/1 [00:00<00:00, 21.82 examples/s]\n",
      "Map: 100%|██████████| 1/1 [00:00<00:00, 112.30 examples/s]\n",
      "/home/b0990106x/.local/lib/python3.10/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mb09901066\u001b[0m (\u001b[33mb09901066_alan\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.18.3 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/work/b0990106x/trl/wandb/run-20241015_001131-bhix10an</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/b09901066_alan/huggingface/runs/bhix10an' target=\"_blank\">devout-fog-74</a></strong> to <a href='https://wandb.ai/b09901066_alan/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/b09901066_alan/huggingface' target=\"_blank\">https://wandb.ai/b09901066_alan/huggingface</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/b09901066_alan/huggingface/runs/bhix10an' target=\"_blank\">https://wandb.ai/b09901066_alan/huggingface/runs/bhix10an</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='100' max='100' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [100/100 00:17, Epoch 100/100]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/b0990106x/.local/lib/python3.10/site-packages/huggingface_hub/file_download.py:1150: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "/home/b0990106x/.local/lib/python3.10/site-packages/transformers/modeling_utils.py:460: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  return torch.load(checkpoint_file, map_location=\"cpu\")\n",
      "Some weights of the model checkpoint at /work/b0990106x/trl/model_output/1015-0010/iter_0/dpo_model were not used when initializing BartForConditionalGeneration: ['v_head.summary.bias', 'v_head.summary.weight']\n",
      "- This IS expected if you are initializing BartForConditionalGeneration from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BartForConditionalGeneration from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "/home/b0990106x/.local/lib/python3.10/site-packages/torch/nn/utils/weight_norm.py:134: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n",
      "/home/b0990106x/.local/lib/python3.10/site-packages/torch/nn/functional.py:5193: UserWarning: Support for mismatched key_padding_mask and attn_mask is deprecated. Use same type for both instead.\n",
      "  warnings.warn(\n",
      "Training Iterations:  10%|█         | 1/10 [01:21<12:12, 81.43s/it]Some weights of the model checkpoint at /work/b0990106x/trl/model_output/1015-0010/iter_0/dpo_model were not used when initializing BartForConditionalGeneration: ['v_head.summary.bias', 'v_head.summary.weight']\n",
      "- This IS expected if you are initializing BartForConditionalGeneration from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BartForConditionalGeneration from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Processing Data: 100%|██████████| 2/2 [00:12<00:00,  6.27s/it]\n",
      "Some weights of the model checkpoint at /work/b0990106x/trl/model_output/1015-0010/iter_0/dpo_model were not used when initializing BartForConditionalGeneration: ['v_head.summary.bias', 'v_head.summary.weight']\n",
      "- This IS expected if you are initializing BartForConditionalGeneration from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BartForConditionalGeneration from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "/work/b0990106x/trl/trl/models/modeling_base.py:328: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  state_dict = loading_func(filename if not use_safe else safe_filename, **load_kwargs)\n",
      "/work/b0990106x/trl/trl/trainer/dpo_trainer.py:410: UserWarning: When using DPODataCollatorWithPadding, you should set `remove_unused_columns=False` in your TrainingArguments we have set it for you, but you should do it yourself in the future.\n",
      "  warnings.warn(\n",
      "Map: 100%|██████████| 1/1 [00:00<00:00, 113.84 examples/s]\n",
      "Map: 100%|██████████| 1/1 [00:00<00:00, 156.47 examples/s]\n",
      "/home/b0990106x/.local/lib/python3.10/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='100' max='100' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [100/100 00:17, Epoch 100/100]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/b0990106x/.local/lib/python3.10/site-packages/huggingface_hub/file_download.py:1150: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "/home/b0990106x/.local/lib/python3.10/site-packages/transformers/modeling_utils.py:460: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  return torch.load(checkpoint_file, map_location=\"cpu\")\n",
      "Some weights of the model checkpoint at /work/b0990106x/trl/model_output/1015-0010/iter_1/dpo_model were not used when initializing BartForConditionalGeneration: ['v_head.summary.bias', 'v_head.summary.weight']\n",
      "- This IS expected if you are initializing BartForConditionalGeneration from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BartForConditionalGeneration from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "/home/b0990106x/.local/lib/python3.10/site-packages/torch/nn/utils/weight_norm.py:134: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n",
      "/home/b0990106x/.local/lib/python3.10/site-packages/torch/nn/functional.py:5193: UserWarning: Support for mismatched key_padding_mask and attn_mask is deprecated. Use same type for both instead.\n",
      "  warnings.warn(\n",
      "Training Iterations:  20%|██        | 2/10 [02:34<10:12, 76.62s/it]Some weights of the model checkpoint at /work/b0990106x/trl/model_output/1015-0010/iter_1/dpo_model were not used when initializing BartForConditionalGeneration: ['v_head.summary.bias', 'v_head.summary.weight']\n",
      "- This IS expected if you are initializing BartForConditionalGeneration from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BartForConditionalGeneration from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Processing Data: 100%|██████████| 2/2 [00:13<00:00,  6.53s/it]\n",
      "Some weights of the model checkpoint at /work/b0990106x/trl/model_output/1015-0010/iter_1/dpo_model were not used when initializing BartForConditionalGeneration: ['v_head.summary.bias', 'v_head.summary.weight']\n",
      "- This IS expected if you are initializing BartForConditionalGeneration from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BartForConditionalGeneration from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "/work/b0990106x/trl/trl/models/modeling_base.py:328: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  state_dict = loading_func(filename if not use_safe else safe_filename, **load_kwargs)\n",
      "/work/b0990106x/trl/trl/trainer/dpo_trainer.py:410: UserWarning: When using DPODataCollatorWithPadding, you should set `remove_unused_columns=False` in your TrainingArguments we have set it for you, but you should do it yourself in the future.\n",
      "  warnings.warn(\n",
      "Map: 100%|██████████| 1/1 [00:00<00:00, 128.50 examples/s]\n",
      "Map: 100%|██████████| 1/1 [00:00<00:00, 162.77 examples/s]\n",
      "/home/b0990106x/.local/lib/python3.10/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='100' max='100' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [100/100 00:17, Epoch 100/100]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/b0990106x/.local/lib/python3.10/site-packages/huggingface_hub/file_download.py:1150: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "/home/b0990106x/.local/lib/python3.10/site-packages/transformers/modeling_utils.py:460: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  return torch.load(checkpoint_file, map_location=\"cpu\")\n",
      "Some weights of the model checkpoint at /work/b0990106x/trl/model_output/1015-0010/iter_2/dpo_model were not used when initializing BartForConditionalGeneration: ['v_head.summary.bias', 'v_head.summary.weight']\n",
      "- This IS expected if you are initializing BartForConditionalGeneration from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BartForConditionalGeneration from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "/home/b0990106x/.local/lib/python3.10/site-packages/torch/nn/utils/weight_norm.py:134: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n",
      "/home/b0990106x/.local/lib/python3.10/site-packages/torch/nn/functional.py:5193: UserWarning: Support for mismatched key_padding_mask and attn_mask is deprecated. Use same type for both instead.\n",
      "  warnings.warn(\n",
      "Training Iterations:  30%|███       | 3/10 [03:48<08:46, 75.17s/it]Some weights of the model checkpoint at /work/b0990106x/trl/model_output/1015-0010/iter_2/dpo_model were not used when initializing BartForConditionalGeneration: ['v_head.summary.bias', 'v_head.summary.weight']\n",
      "- This IS expected if you are initializing BartForConditionalGeneration from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BartForConditionalGeneration from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Processing Data: 100%|██████████| 2/2 [00:12<00:00,  6.28s/it]\n",
      "Some weights of the model checkpoint at /work/b0990106x/trl/model_output/1015-0010/iter_2/dpo_model were not used when initializing BartForConditionalGeneration: ['v_head.summary.bias', 'v_head.summary.weight']\n",
      "- This IS expected if you are initializing BartForConditionalGeneration from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BartForConditionalGeneration from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "/work/b0990106x/trl/trl/models/modeling_base.py:328: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  state_dict = loading_func(filename if not use_safe else safe_filename, **load_kwargs)\n",
      "/work/b0990106x/trl/trl/trainer/dpo_trainer.py:410: UserWarning: When using DPODataCollatorWithPadding, you should set `remove_unused_columns=False` in your TrainingArguments we have set it for you, but you should do it yourself in the future.\n",
      "  warnings.warn(\n",
      "Map: 100%|██████████| 1/1 [00:00<00:00, 118.34 examples/s]\n",
      "Map: 100%|██████████| 1/1 [00:00<00:00, 158.73 examples/s]\n",
      "/home/b0990106x/.local/lib/python3.10/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='100' max='100' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [100/100 00:17, Epoch 100/100]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/b0990106x/.local/lib/python3.10/site-packages/huggingface_hub/file_download.py:1150: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "/home/b0990106x/.local/lib/python3.10/site-packages/transformers/modeling_utils.py:460: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  return torch.load(checkpoint_file, map_location=\"cpu\")\n",
      "Some weights of the model checkpoint at /work/b0990106x/trl/model_output/1015-0010/iter_3/dpo_model were not used when initializing BartForConditionalGeneration: ['v_head.summary.bias', 'v_head.summary.weight']\n",
      "- This IS expected if you are initializing BartForConditionalGeneration from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BartForConditionalGeneration from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "/home/b0990106x/.local/lib/python3.10/site-packages/torch/nn/utils/weight_norm.py:134: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n",
      "/home/b0990106x/.local/lib/python3.10/site-packages/torch/nn/functional.py:5193: UserWarning: Support for mismatched key_padding_mask and attn_mask is deprecated. Use same type for both instead.\n",
      "  warnings.warn(\n",
      "Training Iterations:  40%|████      | 4/10 [05:01<07:26, 74.38s/it]Some weights of the model checkpoint at /work/b0990106x/trl/model_output/1015-0010/iter_3/dpo_model were not used when initializing BartForConditionalGeneration: ['v_head.summary.bias', 'v_head.summary.weight']\n",
      "- This IS expected if you are initializing BartForConditionalGeneration from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BartForConditionalGeneration from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Processing Data: 100%|██████████| 2/2 [00:12<00:00,  6.29s/it]\n",
      "Some weights of the model checkpoint at /work/b0990106x/trl/model_output/1015-0010/iter_3/dpo_model were not used when initializing BartForConditionalGeneration: ['v_head.summary.bias', 'v_head.summary.weight']\n",
      "- This IS expected if you are initializing BartForConditionalGeneration from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BartForConditionalGeneration from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "/work/b0990106x/trl/trl/models/modeling_base.py:328: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  state_dict = loading_func(filename if not use_safe else safe_filename, **load_kwargs)\n",
      "/work/b0990106x/trl/trl/trainer/dpo_trainer.py:410: UserWarning: When using DPODataCollatorWithPadding, you should set `remove_unused_columns=False` in your TrainingArguments we have set it for you, but you should do it yourself in the future.\n",
      "  warnings.warn(\n",
      "Map: 100%|██████████| 1/1 [00:00<00:00, 127.98 examples/s]\n",
      "Map: 100%|██████████| 1/1 [00:00<00:00, 116.77 examples/s]\n",
      "/home/b0990106x/.local/lib/python3.10/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='100' max='100' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [100/100 00:17, Epoch 100/100]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/b0990106x/.local/lib/python3.10/site-packages/huggingface_hub/file_download.py:1150: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "/home/b0990106x/.local/lib/python3.10/site-packages/transformers/modeling_utils.py:460: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  return torch.load(checkpoint_file, map_location=\"cpu\")\n",
      "Some weights of the model checkpoint at /work/b0990106x/trl/model_output/1015-0010/iter_4/dpo_model were not used when initializing BartForConditionalGeneration: ['v_head.summary.bias', 'v_head.summary.weight']\n",
      "- This IS expected if you are initializing BartForConditionalGeneration from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BartForConditionalGeneration from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "/home/b0990106x/.local/lib/python3.10/site-packages/torch/nn/utils/weight_norm.py:134: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n",
      "/home/b0990106x/.local/lib/python3.10/site-packages/torch/nn/functional.py:5193: UserWarning: Support for mismatched key_padding_mask and attn_mask is deprecated. Use same type for both instead.\n",
      "  warnings.warn(\n",
      "Training Iterations:  50%|█████     | 5/10 [06:14<06:10, 74.03s/it]Some weights of the model checkpoint at /work/b0990106x/trl/model_output/1015-0010/iter_4/dpo_model were not used when initializing BartForConditionalGeneration: ['v_head.summary.bias', 'v_head.summary.weight']\n",
      "- This IS expected if you are initializing BartForConditionalGeneration from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BartForConditionalGeneration from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Processing Data: 100%|██████████| 2/2 [00:12<00:00,  6.31s/it]\n",
      "Some weights of the model checkpoint at /work/b0990106x/trl/model_output/1015-0010/iter_4/dpo_model were not used when initializing BartForConditionalGeneration: ['v_head.summary.bias', 'v_head.summary.weight']\n",
      "- This IS expected if you are initializing BartForConditionalGeneration from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BartForConditionalGeneration from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "/work/b0990106x/trl/trl/models/modeling_base.py:328: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  state_dict = loading_func(filename if not use_safe else safe_filename, **load_kwargs)\n",
      "/work/b0990106x/trl/trl/trainer/dpo_trainer.py:410: UserWarning: When using DPODataCollatorWithPadding, you should set `remove_unused_columns=False` in your TrainingArguments we have set it for you, but you should do it yourself in the future.\n",
      "  warnings.warn(\n",
      "Map: 100%|██████████| 1/1 [00:00<00:00, 116.20 examples/s]\n",
      "Map: 100%|██████████| 1/1 [00:00<00:00, 155.17 examples/s]\n",
      "/home/b0990106x/.local/lib/python3.10/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='100' max='100' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [100/100 00:17, Epoch 100/100]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/b0990106x/.local/lib/python3.10/site-packages/huggingface_hub/file_download.py:1150: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "/home/b0990106x/.local/lib/python3.10/site-packages/transformers/modeling_utils.py:460: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  return torch.load(checkpoint_file, map_location=\"cpu\")\n",
      "Some weights of the model checkpoint at /work/b0990106x/trl/model_output/1015-0010/iter_5/dpo_model were not used when initializing BartForConditionalGeneration: ['v_head.summary.bias', 'v_head.summary.weight']\n",
      "- This IS expected if you are initializing BartForConditionalGeneration from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BartForConditionalGeneration from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "# Set up logging\n",
    "logging.basicConfig(\n",
    "    filename=f'{model_output_dir}/log_training.log', \n",
    "    filemode='a', \n",
    "    format='%(asctime)s - %(levelname)s - %(message)s', \n",
    "    level=logging.INFO\n",
    ")\n",
    "\n",
    "logging.info(\n",
    "    f\"Parameters:\\n\"\n",
    "    f\"Prepare Data: sample_size: {sample_size}\\n\"\n",
    "    f\"Training: num_iterations: {num_iterations}\\n\"\n",
    "    f\"Training: data_size_per_iteration: {data_size_per_iteration}\\n\"\n",
    "    f\"Training: train_selected_indices: {train_selected_indices}\\n\"\n",
    "    f\"Training: beta: {beta}\\n\"\n",
    "    f\"Training: learning_rate: {learning_rate}\\n\"\n",
    "    f\"Training: num_train_epochs: {num_train_epochs}\\n\"\n",
    "    f\"Training: max_length: {max_length}\\n\"\n",
    "    f\"Training: max_prompt_length: {max_prompt_length}\\n\"\n",
    "    f\"Training: max_target_length: {max_target_length}\\n\"\n",
    "    f\"Training: per_device_train_batch_size: {per_device_train_batch_size}\\n\"\n",
    "    f\"Training: gradient_accumulation_steps: {gradient_accumulation_steps}\\n\"\n",
    "    f\"Training: seed: {seed}\\n\"\n",
    "    f\"Training: ar_checkpoint: {ar_checkpoint}\\n\"\n",
    "    f\"Training: nar_checkpoint: {nar_checkpoint}\\n\"\n",
    "    f\"Training: args_predict: {args_predict}\\n\"\n",
    "    f\"Training: model_output_dir: {model_output_dir}\\n\"\n",
    "    f\"Training: agent_output_dir: {agent_output_dir}\\n\"\n",
    "    f\"Training: base_path: {base_path}\\n\"\n",
    "    f\"Training: device: {device}\\n\"\n",
    "    f\"Evaluation: eval_train_data_len: {eval_train_data_len}\\n\"\n",
    "    f\"Evaluation: eval_test_data_len: {eval_test_data_len}\\n\"\n",
    "    f\"Evaluation: eval_train_indices: {eval_train_indices}\\n\"\n",
    "    f\"Evaluation: eval_test_indices: {eval_test_indices}\\n\"\n",
    "    f\"Evaluation: eval_train: {eval_train}\\n\"\n",
    "    f\"Evaluation: eval_test: {eval_test}\\n\"\n",
    "    f\"Evaluation: num_eval: {num_eval}\"\n",
    ")\n",
    "\n",
    "# Start time\n",
    "total_start_time = time.time()\n",
    "if eval_train:\n",
    "    original_model_metrics, original_model_rewards = eval_dpo_claps_batch(ar_checkpoint=ar_checkpoint,\n",
    "                                                                    nar_checkpoint=nar_checkpoint,\n",
    "                                                                    trained_model_checkpoint=ar_checkpoint, # original model\n",
    "                                                                    args_predict=args_predict,\n",
    "                                                                    all_src_encodec=all_src_encodec,\n",
    "                                                                    all_instruction=all_instruction,\n",
    "                                                                    iteration = -1,\n",
    "                                                                    num_evaluations = num_eval,\n",
    "                                                                    eval_data_len=eval_train_data_len,\n",
    "                                                                    selected_indices=eval_train_indices,\n",
    "                                                                    device=device,\n",
    "                                                                    clap_model=clap_model,\n",
    "                                                                    accelerator=accelerator\n",
    "                                                                    )\n",
    "    logging.info(f\"Original Model Train Set Evaluation: \")\n",
    "    logging.info(f\"Original model metrics on training set: {original_model_metrics}\")\n",
    "    logging.info(f\"Original model rewards on training set: {original_model_rewards}\")\n",
    "    reward_list = []\n",
    "    for rewards in original_model_rewards:\n",
    "        filter_rewards = [r for r in rewards if r is not None]\n",
    "        if len(filter_rewards) == 0:\n",
    "            reward_list.append(None)\n",
    "        else:\n",
    "            reward_list.append(np.mean(filter_rewards))\n",
    "    logging.info(f\"Original model reward list on training set: {reward_list}\")\n",
    "    filter_reward_list = [r for r in reward_list if r is not None]\n",
    "    if len(filter_reward_list) != 0:\n",
    "        logging.info(f\"Original model average rewards on training set: {np.mean(filter_reward_list)}\")\n",
    "    else: \n",
    "        logging.info(f\"Original model average rewards on training set: None\")\n",
    "    \n",
    "if eval_test:\n",
    "    original_model_metrics, original_model_rewards = eval_dpo_claps_batch(ar_checkpoint=ar_checkpoint,\n",
    "                                                                    nar_checkpoint=nar_checkpoint,\n",
    "                                                                    trained_model_checkpoint=ar_checkpoint, # original model\n",
    "                                                                    args_predict=args_predict,\n",
    "                                                                    all_src_encodec=all_src_encodec,\n",
    "                                                                    all_instruction=all_instruction,\n",
    "                                                                    iteration = -1,\n",
    "                                                                    num_evaluations = num_eval,\n",
    "                                                                    eval_data_len=eval_test_data_len,\n",
    "                                                                    selected_indices=eval_test_indices,\n",
    "                                                                    device=device,\n",
    "                                                                    clap_model=clap_model,\n",
    "                                                                    accelerator=accelerator\n",
    "                                                                    )\n",
    "    logging.info(f\"Original Model Test Set Evaluation: \")\n",
    "    logging.info(f\"Original model metrics on testing set: {original_model_metrics}\")\n",
    "    logging.info(f\"Original model rewards on testing set: {original_model_rewards}\")\n",
    "    reward_list = []\n",
    "    for rewards in original_model_rewards:\n",
    "        filter_rewards = [r for r in rewards if r is not None]\n",
    "        if len(filter_rewards) == 0:\n",
    "            reward_list.append(None)\n",
    "        else:\n",
    "            reward_list.append(np.mean(filter_rewards))\n",
    "    logging.info(f\"Original model reward list on testing set: {reward_list}\")\n",
    "    filter_reward_list = [r for r in reward_list if r is not None]\n",
    "    if len(filter_reward_list) != 0:\n",
    "        logging.info(f\"Original model average rewards on testing set: {np.mean(filter_reward_list)}\")\n",
    "    else: \n",
    "        logging.info(f\"Original model average rewards on testing set: None\")\n",
    "    \n",
    "# If train_selected_indices is not empty, we will use the selected indices for training\n",
    "if train_selected_indices:\n",
    "    batch_src_encodec = [all_src_encodec[i] for i in train_selected_indices]\n",
    "    batch_instruction = [all_instruction[i] for i in train_selected_indices]\n",
    "    logging.info(f\"Processing data from selected indices: {train_selected_indices}\")\n",
    "else:\n",
    "    start_idx = 0\n",
    "    end_idx = data_size_per_iteration\n",
    "    batch_src_encodec = all_src_encodec[start_idx:end_idx] \n",
    "    batch_instruction = all_instruction[start_idx:end_idx]\n",
    "    logging.info(f\"Processing data from index {start_idx} to {end_idx}\")\n",
    "\n",
    "for iteration in tqdm(range(num_iterations), desc=\"Training Iterations\"):\n",
    "    logging.info(f\"-----------Starting iteration {iteration}-----------\")\n",
    "    \n",
    "    resume = iteration > 0 # resume from the previous checkpoint when iteration > 0\n",
    "    \n",
    "    # model_checkpoint is the model checkpoint from the previous iteration\n",
    "    # chosen_rewards and rejected_rewards are the rewards of the data\n",
    "    model_checkpoint, chosen_rewards, rejected_rewards = train_iteration(model_checkpoint=model_checkpoint,\n",
    "                                iteration=iteration,\n",
    "                                data_size=data_size_per_iteration,\n",
    "                                sample_size=sample_size,\n",
    "                                ar_checkpoint=ar_checkpoint,\n",
    "                                nar_checkpoint=nar_checkpoint,\n",
    "                                all_src_encodec=batch_src_encodec,\n",
    "                                all_instruction=batch_instruction,\n",
    "                                args_predict=args_predict,\n",
    "                                agent_output_dir=agent_output_dir,\n",
    "                                model_output_dir_base=model_output_dir,\n",
    "                                temperature = 1.0,\n",
    "                                beta=beta,\n",
    "                                base_path=base_path,\n",
    "                                resume_from_checkpoint=resume, \n",
    "                                learning_rate=learning_rate,\n",
    "                                num_train_epochs=num_train_epochs,\n",
    "                                max_length=max_length,\n",
    "                                max_prompt_length=max_prompt_length,\n",
    "                                max_target_length=max_target_length,\n",
    "                                per_device_train_batch_size=per_device_train_batch_size,\n",
    "                                gradient_accumulation_steps=gradient_accumulation_steps,\n",
    "                                seed=seed,\n",
    "                                clap_model=clap_model,\n",
    "                                accelerator=accelerator\n",
    "                                )       \n",
    "\n",
    "    logging.info(f\"Chosen rewards for iteration {iteration}: {chosen_rewards}\")\n",
    "    logging.info(f\"Rejected rewards for iteration {iteration}: {rejected_rewards}\")\n",
    "    logging.info(f\"Finished training iteration {iteration}\")\n",
    "\n",
    "    if (iteration+1) % eval_frequency == 0:\n",
    "    # Evaluate the result of the current iteration\n",
    "        if eval_train:\n",
    "            trained_model_metrics, trained_model_rewards = eval_dpo_claps_batch(ar_checkpoint=ar_checkpoint,\n",
    "                                                                        nar_checkpoint=nar_checkpoint,\n",
    "                                                                        trained_model_checkpoint=model_checkpoint,\n",
    "                                                                        args_predict=args_predict,\n",
    "                                                                        all_src_encodec=all_src_encodec,\n",
    "                                                                        all_instruction=all_instruction,\n",
    "                                                                        iteration = iteration,\n",
    "                                                                        num_evaluations = num_eval,\n",
    "                                                                        eval_data_len=eval_train_data_len,\n",
    "                                                                        selected_indices=eval_train_indices,\n",
    "                                                                        device=device,\n",
    "                                                                        clap_model=clap_model,\n",
    "                                                                        accelerator=accelerator\n",
    "                                                                        )\n",
    "            logging.info(f\"Trained Model Iteration {iteration} Train Set Evaluation: \")\n",
    "            logging.info(f\"EVAL: Cosine_Sim metrics Training Set for iteration {iteration}: {trained_model_metrics}\")\n",
    "            logging.info(f\"EVAL: Cosine_Sim score Training Set for iteration {iteration}: {trained_model_rewards}\")\n",
    "\n",
    "            reward_list = []\n",
    "            for rewards in trained_model_rewards:\n",
    "                filter_rewards = [r for r in rewards if r is not None]\n",
    "                if len(filter_rewards) == 0:\n",
    "                    reward_list.append(None)\n",
    "                else:\n",
    "                    reward_list.append(np.mean(filter_rewards))\n",
    "            logging.info(f\"EVAL: Trained model Cosine_Sim score list on training set: {reward_list}\")\n",
    "            filter_reward_list = [r for r in reward_list if r is not None]\n",
    "            if len(filter_reward_list) != 0:\n",
    "                logging.info(f\"EVAL: Trained model average Cosine_Sim score on training set: {np.mean(filter_reward_list)}\")\n",
    "            else:\n",
    "                logging.info(f\"EVAL: Trained model average Cosine_Sim score on training set: None\")\n",
    "\n",
    "        if eval_test:\n",
    "            trained_model_metrics, trained_model_rewards = eval_dpo_claps_batch(ar_checkpoint=ar_checkpoint,\n",
    "                                                                        nar_checkpoint=nar_checkpoint,\n",
    "                                                                        trained_model_checkpoint=model_checkpoint,\n",
    "                                                                        args_predict=args_predict,\n",
    "                                                                        all_src_encodec=all_src_encodec,\n",
    "                                                                        all_instruction=all_instruction,\n",
    "                                                                        iteration = iteration,\n",
    "                                                                        num_evaluations = num_eval,\n",
    "                                                                        eval_data_len=eval_test_data_len,\n",
    "                                                                        selected_indices=eval_test_indices,\n",
    "                                                                        device=device,\n",
    "                                                                        clap_model=clap_model,\n",
    "                                                                        accelerator=accelerator\n",
    "                                                                        )\n",
    "            logging.info(f\"Trained Model Iteration {iteration} Test Set Evaluation: \")\n",
    "            logging.info(f\"EVAL: Cosine_Sim metrics Testing Set for iteration {iteration}: {trained_model_metrics}\")\n",
    "            logging.info(f\"EVAL: Cosine_Sim score Testing Set for iteration {iteration}: {trained_model_rewards}\")\n",
    "\n",
    "            reward_list = []\n",
    "            for rewards in trained_model_rewards:\n",
    "                filter_rewards = [r for r in rewards if r is not None]\n",
    "                if len(filter_rewards) == 0:\n",
    "                    reward_list.append(None)\n",
    "                else:\n",
    "                    reward_list.append(np.mean(filter_rewards))\n",
    "            logging.info(f\"EVAL: Trained model Cosine_Sim score list on testing set: {reward_list}\")\n",
    "            filter_reward_list = [r for r in reward_list if r is not None]\n",
    "            if len(filter_reward_list) != 0:\n",
    "                logging.info(f\"EVAL: Trained model average Cosine_Sim score on testing set: {np.mean(filter_reward_list)}\")\n",
    "            else:\n",
    "                logging.info(f\"EVAL: Trained model average Cosine_Sim score on testing set: None\")\n",
    "\n",
    "    logging.info(f\"-----------Finished iteration {iteration}-----------\")\n",
    "total_end_time = time.time()\n",
    "\n",
    "# Calculate total time taken\n",
    "total_time_taken = total_end_time - total_start_time\n",
    "logging.info(f\"Total time taken for the entire process: {total_time_taken:.2f} seconds\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "trl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
