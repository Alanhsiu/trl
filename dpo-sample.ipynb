{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/b0990106x/miniconda3/envs/trl/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append(\"/work/b0990106x/trl/vc\")\n",
    "import importlib\n",
    "import vc\n",
    "importlib.reload(vc)\n",
    "import torch\n",
    "from vc.trainer_encodec_vc_inference import get_ar_prediction_v3\n",
    "from types import SimpleNamespace\n",
    "from transformers import BartForConditionalGeneration, AutoModelForCausalLM, AutoTokenizer\n",
    "from NISQA.nisqa.NISQA_model import nisqaModel\n",
    "from datasets import load_from_disk\n",
    "from trl import DPOTrainer, DPOConfig, AutoModelForSeq2SeqLMWithValueHead, create_reference_model\n",
    "from vc.encodec_model.nar_bart_model import NARBartForConditionalGeneration\n",
    "from datetime import datetime\n",
    "import os\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "timestamp: 0628-1927\n"
     ]
    }
   ],
   "source": [
    "# Define paths and device\n",
    "base_path = \"/work/b0990106x/trl\"\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "now = datetime.now()\n",
    "ts = now.strftime(\"%m%d-%H%M\")\n",
    "print(\"timestamp:\", ts)\n",
    "\n",
    "model_output_dir = f\"{base_path}/model_output/{ts}\"\n",
    "agent_input_dir = f\"{base_path}/data-encodec\"\n",
    "agent_output_dir = f\"{base_path}/output/{ts}\"\n",
    "env_input_dir = agent_output_dir\n",
    "env_output_dir = agent_input_dir\n",
    "\n",
    "if not os.path.exists(model_output_dir):\n",
    "    os.makedirs(model_output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/b0990106x/miniconda3/envs/trl/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "ar_checkpoint = \"lca0503/speech-chatgpt-base-ar-v2-epoch10-wotrans\"\n",
    "nar_checkpoint = \"lca0503/speech-chatgpt-base-nar-v2-epoch4-wotrans\"\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "model = AutoModelForSeq2SeqLMWithValueHead.from_pretrained(ar_checkpoint, return_dict=True)\n",
    "model_ref = create_reference_model(model)\n",
    "nar_model = NARBartForConditionalGeneration.from_pretrained(nar_checkpoint)\n",
    "ar_tokenizer = AutoTokenizer.from_pretrained(ar_checkpoint)\n",
    "nar_tokenizer = AutoTokenizer.from_pretrained(nar_checkpoint)\n",
    "ar_tokenizer.pad_token = ar_tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['prompt', 'chosen', 'rejected'],\n",
      "    num_rows: 3\n",
      "})\n",
      "train_dataset Dataset({\n",
      "    features: ['prompt', 'chosen', 'rejected'],\n",
      "    num_rows: 2\n",
      "})\n",
      "val_dataset Dataset({\n",
      "    features: ['prompt', 'chosen', 'rejected'],\n",
      "    num_rows: 1\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from datasets import Dataset\n",
    "\n",
    "# load the dpo_data.json file\n",
    "with open(\"dpo_data_sample.json\") as f:\n",
    "    dpo_data = json.load(f)\n",
    "\n",
    "# load the dataset\n",
    "dataset = Dataset.from_dict(dpo_data)\n",
    "print(dataset)\n",
    "\n",
    "## TODO: Split the dataset into training and validation sets\n",
    "dataset_dict = dataset.train_test_split(test_size=0.1)\n",
    "\n",
    "# load the training and validation datasets\n",
    "train_dataset = dataset_dict[\"train\"]\n",
    "val_dataset = dataset_dict[\"test\"]\n",
    "print(\"train_dataset\", train_dataset)\n",
    "print(\"val_dataset\", val_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mb09901066\u001b[0m (\u001b[33mb09901066_alan\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Changes to your `wandb` environment variables will be ignored because your `wandb` session has already started. For more information on how to modify your settings with `wandb.init()` arguments, please refer to <a href='https://wandb.me/wandb-init' target=\"_blank\">the W&B docs</a>."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.17.3 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/work/b0990106x/trl/wandb/run-20240628_192742-ku3wmup0</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/b09901066_alan/trl/runs/ku3wmup0' target=\"_blank\">generous-tree-15</a></strong> to <a href='https://wandb.ai/b09901066_alan/trl' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/b09901066_alan/trl' target=\"_blank\">https://wandb.ai/b09901066_alan/trl</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/b09901066_alan/trl/runs/ku3wmup0' target=\"_blank\">https://wandb.ai/b09901066_alan/trl/runs/ku3wmup0</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src='https://wandb.ai/b09901066_alan/trl/runs/ku3wmup0?jupyter=true' style='border:none;width:100%;height:420px;display:none;'></iframe>"
      ],
      "text/plain": [
       "<wandb.sdk.wandb_run.Run at 0x7f82f34c3580>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import wandb\n",
    "wandb.login()\n",
    "\n",
    "os.environ[\"WANDB_NOTEBOOK_NAME\"] = \"trl\"\n",
    "wandb.init(project=\"trl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/work/b0990106x/trl/trl/trainer/dpo_trainer.py:362: UserWarning: `max_length` is not set in the DPOConfig's init it will default to `512` by default, but you should do it yourself in the future.\n",
      "  warnings.warn(\n",
      "/work/b0990106x/trl/trl/trainer/dpo_trainer.py:375: UserWarning: `max_prompt_length` is not set in the DPOConfig's init it will default to `128` by default, but you should do it yourself in the future.\n",
      "  warnings.warn(\n",
      "/work/b0990106x/trl/trl/trainer/dpo_trainer.py:388: UserWarning: When using an encoder decoder architecture, you should set `max_target_length` in the DPOConfig's init it will default to `128` by default, but you should do it yourself in the future.\n",
      "  warnings.warn(\n",
      "/work/b0990106x/trl/trl/trainer/dpo_trainer.py:410: UserWarning: When using DPODataCollatorWithPadding, you should set `remove_unused_columns=False` in your TrainingArguments we have set it for you, but you should do it yourself in the future.\n",
      "  warnings.warn(\n",
      "Map: 100%|██████████| 2/2 [00:00<00:00, 198.72 examples/s]\n",
      "Map: 100%|██████████| 1/1 [00:00<00:00, 137.97 examples/s]\n",
      "Detected kernel version 3.10.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n",
      "/home/b0990106x/miniconda3/envs/trl/lib/python3.10/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3' max='3' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3/3 00:00, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "('/work/b0990106x/trl/model_output/0628-1927/dpo_model/tokenizer_config.json',\n",
       " '/work/b0990106x/trl/model_output/0628-1927/dpo_model/special_tokens_map.json',\n",
       " '/work/b0990106x/trl/model_output/0628-1927/dpo_model/vocab.json',\n",
       " '/work/b0990106x/trl/model_output/0628-1927/dpo_model/merges.txt',\n",
       " '/work/b0990106x/trl/model_output/0628-1927/dpo_model/added_tokens.json',\n",
       " '/work/b0990106x/trl/model_output/0628-1927/dpo_model/tokenizer.json')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_args = DPOConfig(\n",
    "    beta=0.02,\n",
    "    output_dir=f\"{model_output_dir}\",\n",
    "    generate_during_eval = True,\n",
    ")\n",
    "\n",
    "trainer = DPOTrainer(\n",
    "    model=model,\n",
    "    ref_model=model_ref,\n",
    "    args=training_args,\n",
    "    tokenizer=ar_tokenizer,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    ")\n",
    "\n",
    "# train\n",
    "trainer.train()\n",
    "\n",
    "# save the model\n",
    "trainer.save_model(f\"{model_output_dir}/dpo_model\")\n",
    "\n",
    "model.config.to_json_file(f\"{model_output_dir}/dpo_model/config.json\")\n",
    "ar_tokenizer.save_pretrained(f\"{model_output_dir}/dpo_model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"/work/b0990106x/trl/vc\")\n",
    "import importlib\n",
    "import vc\n",
    "importlib.reload(vc)\n",
    "import torch\n",
    "from vc.trainer_encodec_vc_inference import get_ar_prediction_v3\n",
    "from types import SimpleNamespace\n",
    "from transformers import BartForConditionalGeneration, AutoModelForCausalLM, AutoTokenizer\n",
    "from NISQA.nisqa.NISQA_model import nisqaModel\n",
    "from datasets import load_from_disk\n",
    "from trl import DPOTrainer, DPOConfig, AutoModelForSeq2SeqLMWithValueHead, create_reference_model\n",
    "from vc.encodec_model.nar_bart_model import NARBartForConditionalGeneration\n",
    "from datetime import datetime\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "timestamp: 0628-1927\n"
     ]
    }
   ],
   "source": [
    "# Define paths and device\n",
    "base_path = \"/work/b0990106x/trl\"\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "now = datetime.now()\n",
    "ts = now.strftime(\"%m%d-%H%M\")\n",
    "print(\"timestamp:\", ts)\n",
    "# ts = \"beta2_v2\"\n",
    "\n",
    "agent_output_dir = f\"{base_path}/output/{ts}\"\n",
    "\n",
    "if not os.path.exists(agent_output_dir):\n",
    "    os.makedirs(agent_output_dir)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data_len: 3\n"
     ]
    }
   ],
   "source": [
    "# Prepare Model\n",
    "\n",
    "all_src_encodec_layers = []\n",
    "all_src_encodec = []\n",
    "all_instruction = []\n",
    "\n",
    "layer_len = 8\n",
    "num_samples = 3\n",
    "\n",
    "args_predict = SimpleNamespace(output_path=f\"{base_path}/output/{ts}/example.wav\", seed=0, device=device)\n",
    "agent_input_dir = f\"{base_path}/data-encodec\"\n",
    "test_dataset = load_from_disk(agent_input_dir)\n",
    "data_len = min(len(test_dataset), num_samples)\n",
    "print(\"data_len:\", data_len)\n",
    "\n",
    "for i in range(layer_len):\n",
    "    all_src_encodec_layers.append(test_dataset[f\"src_encodec_{i}\"])\n",
    "\n",
    "for i in range(data_len):\n",
    "    src_encodec = []\n",
    "    for j in range(layer_len):\n",
    "        src_encodec.append(all_src_encodec_layers[j][i])\n",
    "    all_src_encodec.append(src_encodec)\n",
    "\n",
    "    all_instruction.append(test_dataset[\"instruction\"][i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/b0990106x/miniconda3/envs/trl/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "ar_checkpoint = \"lca0503/speech-chatgpt-base-ar-v2-epoch10-wotrans\"\n",
    "nar_checkpoint = \"lca0503/speech-chatgpt-base-nar-v2-epoch4-wotrans\"\n",
    "\n",
    "nar_model = NARBartForConditionalGeneration.from_pretrained(nar_checkpoint)\n",
    "ar_tokenizer = AutoTokenizer.from_pretrained(ar_checkpoint)\n",
    "nar_tokenizer = AutoTokenizer.from_pretrained(nar_checkpoint)\n",
    "ar_tokenizer.pad_token = ar_tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to calculate reward\n",
    "import time\n",
    "def get_reward(output_path):\n",
    "    args_nisqa = {\n",
    "        \"mode\": \"predict_file\",\n",
    "        \"pretrained_model\": f\"{base_path}/NISQA/weights/nisqa.tar\",\n",
    "        \"deg\": output_path,\n",
    "        \"data_dir\": None,\n",
    "        \"output_dir\": f\"{base_path}/NISQA/result/\",\n",
    "        \"csv_file\": None,\n",
    "        \"csv_deg\": None,\n",
    "        \"num_workers\": 0,\n",
    "        \"bs\": 1,\n",
    "        \"ms_channel\": None,\n",
    "    }\n",
    "    args_nisqa[\"tr_bs_val\"] = args_nisqa[\"bs\"]\n",
    "    args_nisqa[\"tr_num_workers\"] = args_nisqa[\"num_workers\"]\n",
    "    nisqa = nisqaModel(args_nisqa)\n",
    "    try:\n",
    "        prediction = nisqa.predict()\n",
    "        reward = float(prediction[\"mos_pred\"].iloc[0])\n",
    "        print(\"Reward:\", reward)\n",
    "        return reward\n",
    "    except Exception as e:\n",
    "        print(\"Error:\", e)\n",
    "        print(\"get_reward function end ___________________________\")\n",
    "        return None\n",
    "    \n",
    "def process_and_get_scores(model, nar_model, ar_tokenizer, nar_tokenizer, src_encodec, instruction, episode_counter=0):\n",
    "    temp, decode_ar,output_path_ckpt = get_ar_prediction_v3(args_predict, model, nar_model, ar_tokenizer, nar_tokenizer, src_encodec, instruction, episode_counter)\n",
    "    list_decode_ar = decode_ar.flatten().tolist()\n",
    "    time.sleep(0.5)\n",
    "    reward = get_reward(output_path_ckpt)\n",
    "    return reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at /work/b0990106x/trl/model_output/0628-1927/dpo_model were not used when initializing BartForConditionalGeneration: ['v_head.summary.weight', 'v_head.summary.bias']\n",
      "- This IS expected if you are initializing BartForConditionalGeneration from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BartForConditionalGeneration from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "# Load trained model\n",
    "# trained_model = BartForConditionalGeneration.from_pretrained(f\"{base_path}/model_output/beta2/dpo_model\")\n",
    "model = BartForConditionalGeneration.from_pretrained(ar_checkpoint, return_dict=True)\n",
    "trained_model = AutoModelForSeq2SeqLMWithValueHead.from_pretrained(f\"{model_output_dir}/dpo_model\")\n",
    "# model = trained_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize lists to store rewards\n",
    "old_model_rewards = []\n",
    "trained_model_rewards = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Play the audio twice.\n",
      "Mildly decrease the emphasis on the higher frequencies.\n",
      "Considerably abate the bass frequencies.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/b0990106x/miniconda3/envs/trl/lib/python3.10/site-packages/torch/nn/utils/weight_norm.py:28: UserWarning: torch.nn.utils.weight_norm is deprecated in favor of torch.nn.utils.parametrizations.weight_norm.\n",
      "  warnings.warn(\"torch.nn.utils.weight_norm is deprecated in favor of torch.nn.utils.parametrizations.weight_norm.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 0 : audio saved to  /work/b0990106x/trl/output/0628-1927/example_save_0.wav\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/b0990106x/miniconda3/envs/trl/lib/python3.10/site-packages/librosa/feature/spectral.py:2143: UserWarning: Empty filters detected in mel frequency basis. Some channels will produce empty responses. Try increasing your sampling rate (and fmax) or reducing n_mels.\n",
      "  mel_basis = filters.mel(sr=sr, n_fft=n_fft, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reward: 2.2465121746063232\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/b0990106x/miniconda3/envs/trl/lib/python3.10/site-packages/torch/nn/utils/weight_norm.py:28: UserWarning: torch.nn.utils.weight_norm is deprecated in favor of torch.nn.utils.parametrizations.weight_norm.\n",
      "  warnings.warn(\"torch.nn.utils.weight_norm is deprecated in favor of torch.nn.utils.parametrizations.weight_norm.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 0 : audio saved to  /work/b0990106x/trl/output/0628-1927/example_save_0.wav\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/b0990106x/miniconda3/envs/trl/lib/python3.10/site-packages/librosa/feature/spectral.py:2143: UserWarning: Empty filters detected in mel frequency basis. Some channels will produce empty responses. Try increasing your sampling rate (and fmax) or reducing n_mels.\n",
      "  mel_basis = filters.mel(sr=sr, n_fft=n_fft, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reward: 2.208418369293213\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/b0990106x/miniconda3/envs/trl/lib/python3.10/site-packages/torch/nn/utils/weight_norm.py:28: UserWarning: torch.nn.utils.weight_norm is deprecated in favor of torch.nn.utils.parametrizations.weight_norm.\n",
      "  warnings.warn(\"torch.nn.utils.weight_norm is deprecated in favor of torch.nn.utils.parametrizations.weight_norm.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1 : audio saved to  /work/b0990106x/trl/output/0628-1927/example_save_1.wav\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/b0990106x/miniconda3/envs/trl/lib/python3.10/site-packages/librosa/feature/spectral.py:2143: UserWarning: Empty filters detected in mel frequency basis. Some channels will produce empty responses. Try increasing your sampling rate (and fmax) or reducing n_mels.\n",
      "  mel_basis = filters.mel(sr=sr, n_fft=n_fft, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reward: 2.018082618713379\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/b0990106x/miniconda3/envs/trl/lib/python3.10/site-packages/torch/nn/utils/weight_norm.py:28: UserWarning: torch.nn.utils.weight_norm is deprecated in favor of torch.nn.utils.parametrizations.weight_norm.\n",
      "  warnings.warn(\"torch.nn.utils.weight_norm is deprecated in favor of torch.nn.utils.parametrizations.weight_norm.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1 : audio saved to  /work/b0990106x/trl/output/0628-1927/example_save_1.wav\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/b0990106x/miniconda3/envs/trl/lib/python3.10/site-packages/librosa/feature/spectral.py:2143: UserWarning: Empty filters detected in mel frequency basis. Some channels will produce empty responses. Try increasing your sampling rate (and fmax) or reducing n_mels.\n",
      "  mel_basis = filters.mel(sr=sr, n_fft=n_fft, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reward: 2.253736972808838\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/b0990106x/miniconda3/envs/trl/lib/python3.10/site-packages/torch/nn/utils/weight_norm.py:28: UserWarning: torch.nn.utils.weight_norm is deprecated in favor of torch.nn.utils.parametrizations.weight_norm.\n",
      "  warnings.warn(\"torch.nn.utils.weight_norm is deprecated in favor of torch.nn.utils.parametrizations.weight_norm.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 2 : audio saved to  /work/b0990106x/trl/output/0628-1927/example_save_2.wav\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/b0990106x/miniconda3/envs/trl/lib/python3.10/site-packages/librosa/feature/spectral.py:2143: UserWarning: Empty filters detected in mel frequency basis. Some channels will produce empty responses. Try increasing your sampling rate (and fmax) or reducing n_mels.\n",
      "  mel_basis = filters.mel(sr=sr, n_fft=n_fft, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reward: 2.7493767738342285\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/b0990106x/miniconda3/envs/trl/lib/python3.10/site-packages/torch/nn/utils/weight_norm.py:28: UserWarning: torch.nn.utils.weight_norm is deprecated in favor of torch.nn.utils.parametrizations.weight_norm.\n",
      "  warnings.warn(\"torch.nn.utils.weight_norm is deprecated in favor of torch.nn.utils.parametrizations.weight_norm.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 2 : audio saved to  /work/b0990106x/trl/output/0628-1927/example_save_2.wav\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/b0990106x/miniconda3/envs/trl/lib/python3.10/site-packages/librosa/feature/spectral.py:2143: UserWarning: Empty filters detected in mel frequency basis. Some channels will produce empty responses. Try increasing your sampling rate (and fmax) or reducing n_mels.\n",
      "  mel_basis = filters.mel(sr=sr, n_fft=n_fft, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reward: 2.5731265544891357\n"
     ]
    }
   ],
   "source": [
    "i = 0 \n",
    "count_rewards = 0\n",
    "target_rewards = 3\n",
    "\n",
    "# print all the instructions\n",
    "for j in range(target_rewards):\n",
    "    print(all_instruction[j])\n",
    "\n",
    "while count_rewards < target_rewards:\n",
    "    if i >= data_len:\n",
    "        print(\"Exceeded initial data length.\")\n",
    "        break\n",
    "    instruction = all_instruction[i]\n",
    "    src_encodec = all_src_encodec[i]\n",
    "    size_of_packed_input = (len(src_encodec[0]) + len(ar_tokenizer(instruction)[\"input_ids\"][1:-1]) + 3)\n",
    "    \n",
    "    if size_of_packed_input <= 1024 and size_of_packed_input > 4:\n",
    "        # Process with old modl\n",
    "        model.to(device)\n",
    "        old_model_reward = process_and_get_scores(model, nar_model, ar_tokenizer, nar_tokenizer, src_encodec, instruction, episode_counter=i)\n",
    "        old_model_rewards.append(old_model_reward)\n",
    "\n",
    "        # Process with trained model\n",
    "        trained_model.to(device)\n",
    "        trained_model_reward = process_and_get_scores(trained_model, nar_model, ar_tokenizer, nar_tokenizer, src_encodec, instruction, episode_counter=i)\n",
    "        trained_model_rewards.append(trained_model_reward)\n",
    "\n",
    "        count_rewards += 1\n",
    "    else:\n",
    "        print(f\"Skipping data point {i} due to insufficient packed input size.\")\n",
    "    i += 1\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Old Model Rewards: [2.3901169300079346, 2.2465121746063232, 2.018082618713379, 2.7493767738342285]\n",
      "Trained Model Rewards: [3.0239880084991455, 2.208418369293213, 2.253736972808838, 2.5731265544891357]\n"
     ]
    }
   ],
   "source": [
    "print(\"Old Model Rewards:\", old_model_rewards)\n",
    "print(\"Trained Model Rewards:\", trained_model_rewards)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Old Model Metrics: {'mean': 2.3901169300079346, 'median': 2.3901169300079346, 'std_dev': 0.0, 'variance': 0.0, 'min': 2.3901169300079346, 'max': 2.3901169300079346, '25th_percentile': 2.3901169300079346, '75th_percentile': 2.3901169300079346}\n",
      "Trained Model Metrics: {'mean': 3.0239880084991455, 'median': 3.0239880084991455, 'std_dev': 0.0, 'variance': 0.0, 'min': 3.0239880084991455, 'max': 3.0239880084991455, '25th_percentile': 3.0239880084991455, '75th_percentile': 3.0239880084991455}\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import json\n",
    "# Filter out None values\n",
    "filtered_old_model_rewards = [r for r in old_model_rewards if r is not None]\n",
    "filtered_trained_model_rewards = [r for r in trained_model_rewards if r is not None]\n",
    "\n",
    "# Calculate and print metrics\n",
    "def calculate_metrics(rewards):\n",
    "    metrics = {\n",
    "        \"mean\": np.mean(rewards),\n",
    "        \"median\": np.median(rewards),\n",
    "        \"std_dev\": np.std(rewards),\n",
    "        \"variance\": np.var(rewards),\n",
    "        \"min\": np.min(rewards),\n",
    "        \"max\": np.max(rewards),\n",
    "        \"25th_percentile\": np.percentile(rewards, 25),\n",
    "        \"75th_percentile\": np.percentile(rewards, 75),\n",
    "    }\n",
    "    return metrics\n",
    "\n",
    "old_model_metrics = calculate_metrics(filtered_old_model_rewards)\n",
    "trained_model_metrics = calculate_metrics(filtered_trained_model_rewards)\n",
    "\n",
    "print(\"Old Model Metrics:\", old_model_metrics)\n",
    "print(\"Trained Model Metrics:\", trained_model_metrics)\n",
    "metrics = {\n",
    "    \"old_model\": old_model_metrics,\n",
    "    \"trained_model\": trained_model_metrics,\n",
    "    \"old_model_rewards\": old_model_rewards,\n",
    "    \"trained_model_rewards\": trained_model_rewards,\n",
    "}\n",
    "\n",
    "with open(f\"{base_path}/output/{ts}/metrics.json\", \"w\") as f:\n",
    "    json.dump(metrics, f, indent=4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# temp_reward = get_reward(f\"/work/b0990106x/trl/output/{ts}/example_save_1.wav\")\n",
    "# print(\"temp_reward:\", temp_reward)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "trl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
