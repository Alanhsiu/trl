{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"/work/b0990106x/trl/vc\")\n",
    "import importlib\n",
    "import vc\n",
    "importlib.reload(vc)\n",
    "import torch\n",
    "from vc.trainer_encodec_vc_inference import pack_inputs_v2, get_ar_prediction_get_audio, get_ar_prediction_audio_batch\n",
    "from types import SimpleNamespace\n",
    "from transformers import BartForConditionalGeneration, AutoTokenizer\n",
    "from datasets import Dataset\n",
    "from trl import DPOTrainer, DPOConfig, AutoModelForSeq2SeqLMWithValueHead, create_reference_model\n",
    "from vc.encodec_model.nar_bart_model import NARBartForConditionalGeneration\n",
    "from datetime import datetime\n",
    "import os\n",
    "import numpy as np\n",
    "from dpo_eval import get_reward_claps ,eval_dpo_claps_batch, convert_array_to_tensor_format\n",
    "from dpo_eval import get_reward_mos, process_and_get_mos_reward, eval_dpo_mos\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "from typing import List, Tuple\n",
    "import random\n",
    "import argparse\n",
    "import soundfile as sf\n",
    "import math\n",
    "\n",
    "sys.path.append('/work/b0990106x/trl/CLAPS')\n",
    "from CLAPS.inference import load_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Utility Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seed(seed):\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(seed)\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "def generate_output_batch(\n",
    "        model,\n",
    "        ar_model, \n",
    "        nar_model, \n",
    "        ar_tokenizer, \n",
    "        nar_tokenizer, \n",
    "        clap_model,\n",
    "        accelerator,\n",
    "        src_encodec: list, \n",
    "        instruction: list, \n",
    "        args_predict: SimpleNamespace, \n",
    "        episode_counter: int = 0, \n",
    "        base_path: str = \"/work/b0990106x/trl\", \n",
    "        temperature: float = 1.0\n",
    ") -> tuple[float, str]:\n",
    "    '''\n",
    "    Generates output from AR model, synthesize the audio, and evaluate the audio using NISQA.\n",
    "    Returns:\n",
    "        tuple:\n",
    "            reward(float): The reward of the audio.\n",
    "            tokenized_decode_ar(str): The tokenized output of the AR model - first layer.\n",
    "    '''\n",
    "    # Generate predictions using the AR model\n",
    "    audio_list, decode_ar_list = get_ar_prediction_audio_batch(\n",
    "        args_predict, model, nar_model, ar_tokenizer, nar_tokenizer, src_encodec, instruction, episode_counter, temperature=temperature\n",
    "    )\n",
    "    # extract the instruction from the list \n",
    "    reward_list,tokenized_decode_ar_list = [], []\n",
    "\n",
    "    for i, audio in enumerate(audio_list): \n",
    "        # audio ---> tensor([])\n",
    "        if audio is not None:\n",
    "            # tensor_audio = convert_array_to_tensor_format(audio)\n",
    "            # if tensor_audio[0].shape[0]==1:\n",
    "            #     tensor_audio[0] = tensor_audio[0].squeeze(0)\n",
    "            # print(tensor_audio)\n",
    "            # reward_claps = get_reward_claps(clap_model=clap_model, accelerator=accelerator, prompts = instruction[i], wavs = tensor_audio)\n",
    "            # reward_mos = process_and_get_mos_reward(model=model, nar_model=nar_model, ar_tokenizer=ar_tokenizer, nar_tokenizer=nar_tokenizer, src_encodec=src_encodec[i], instruction=instruction[i], args_predict=args_predict, episode_counter=episode_counter, base_path=base_path)\n",
    "            output_path_ckpt = args_predict.output_path.replace(\".wav\", f\"_generate_{episode_counter}_item_{i}.wav\")\n",
    "            sf.write(output_path_ckpt, np.ravel(audio), samplerate=24000)\n",
    "            reward_mos = get_reward_mos(output_path=output_path_ckpt, base_path=base_path)\n",
    "            reward = reward_mos / 5\n",
    "        else: \n",
    "            reward = 0\n",
    "        reward_list.append(reward)\n",
    "    \n",
    "    for decode_ar in decode_ar_list:\n",
    "        list_decode_ar = decode_ar.flatten().tolist()   \n",
    "        filtered_decode_ar_list = list_decode_ar[2:-1]\n",
    "        decode_ar_tokens = ar_tokenizer.convert_ids_to_tokens(filtered_decode_ar_list)\n",
    "        tokenized_decode_ar = ar_tokenizer.convert_tokens_to_string(decode_ar_tokens)\n",
    "        tokenized_decode_ar_list.append(tokenized_decode_ar)\n",
    "        \n",
    "    return reward_list, tokenized_decode_ar_list\n",
    "\n",
    "def extract_data_from_json(file_path: str) -> Tuple[List[list], List[str], List[list]]:\n",
    "    \"\"\"\n",
    "    Loads data from a JSON file and extracts 'src_encodec', 'instruction', and 'tgt_encodec'.\n",
    "\n",
    "    Args:\n",
    "        file_path (str): The path to the JSON file.\n",
    "\n",
    "    Returns:\n",
    "        tuple:\n",
    "            all_src_encodec (List[list]): A list containing the 'src_encodec' data from each item in the JSON file.\n",
    "            all_instruction (List[str]): A list containing the 'instruction' data from each item in the JSON file.\n",
    "            all_tgt_encodec (List[list]): A list containing the 'tgt_encodec' data from each item in the JSON file.\n",
    "    \"\"\"\n",
    "    with open(file_path, 'r') as f:\n",
    "        data = json.load(f)\n",
    "\n",
    "    all_src_encodec = [item[\"src_encodec\"] for item in data]\n",
    "    all_instruction = [item[\"instruction\"] for item in data]\n",
    "\n",
    "    return all_src_encodec, all_instruction\n",
    "\n",
    "def train_model(\n",
    "        model,\n",
    "        model_ref,\n",
    "        ar_tokenizer,\n",
    "        train_dataset: Dataset,\n",
    "        val_dataset: Dataset,\n",
    "        model_output_dir: str,\n",
    "        beta: float,\n",
    "        resume_from_checkpoint: bool,\n",
    "        model_checkpoint: str,\n",
    "        learning_rate: float = 5e-07,\n",
    "        num_train_epochs: int = 200,\n",
    "        max_length: int = 1024*9,\n",
    "        max_prompt_length: int = 1024*9,\n",
    "        max_target_length: int = 1024*9,\n",
    "        per_device_train_batch_size: int = 1,\n",
    "        gradient_accumulation_steps: int = 1,\n",
    "        seed: int = 42\n",
    ") -> None:\n",
    "    '''\n",
    "    Train the DPO model and save the model.\n",
    "\n",
    "    Args:\n",
    "        model(AutoModelForSeq2SeqLMWithValueHead): The DPO model.\n",
    "        model_ref(AutoModelForCausalLM): The reference model.\n",
    "        ar_tokenizer(AutoTokenizer): The tokenizer.\n",
    "        train_dataset(Dataset): The training dataset.\n",
    "        val_dataset(Dataset): The validation dataset.\n",
    "        model_output_dir(str): The output directory for the model.\n",
    "        beta(float): The beta value.\n",
    "        resume_from_checkpoint(bool): Whether to resume from a checkpoint.\n",
    "        model_checkpoint(str): The path to the model\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    '''\n",
    "\n",
    "    training_args = DPOConfig(\n",
    "        beta = beta,\n",
    "        output_dir = model_output_dir,\n",
    "        resume_from_checkpoint = model_checkpoint if resume_from_checkpoint else None,\n",
    "        seed = seed,\n",
    "        per_device_train_batch_size = per_device_train_batch_size,\n",
    "        num_train_epochs = num_train_epochs,\n",
    "        gradient_accumulation_steps = gradient_accumulation_steps,\n",
    "        learning_rate = learning_rate,\n",
    "        max_length = max_length,\n",
    "        max_prompt_length = max_prompt_length,\n",
    "        max_target_length = max_target_length,\n",
    "        evaluation_strategy=\"steps\",\n",
    "        save_steps = 5000,\n",
    "        logging_dir = f\"{model_output_dir}/logs\"\n",
    "    )\n",
    "    \n",
    "    trainer = DPOTrainer(\n",
    "        model=model,\n",
    "        ref_model=model_ref,\n",
    "        args=training_args,\n",
    "        tokenizer=ar_tokenizer,\n",
    "        train_dataset=train_dataset,\n",
    "        eval_dataset=val_dataset,\n",
    "    )\n",
    "    # Train the model\n",
    "    trainer.train()\n",
    "\n",
    "    # Save the model\n",
    "    # trainer.save_model(f\"{model_output_dir}/dpo_model\")\n",
    "    model.config.to_json_file(f\"{model_output_dir}/config.json\")\n",
    "    # ar_tokenizer.save_pretrained(f\"{model_output_dir}/dpo_model\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "def process_data_batch(sample_size: int, \n",
    "                       model,\n",
    "                        ar_model, \n",
    "                        nar_model, \n",
    "                        ar_tokenizer, \n",
    "                        nar_tokenizer, \n",
    "                        clap_model,\n",
    "                        accelerator,\n",
    "                        selected_src_encodec: List[list], \n",
    "                        selected_instruction: List[str],\n",
    "                        args_predict: SimpleNamespace, \n",
    "                        base_path: str = \"/work/b0990106x/trl\", \n",
    "                        temperature: float = 1.0, \n",
    "                        iteration: int = 0\n",
    ") -> Tuple[List[str], List[str], List[str], List[float], List[float], List[float]]:\n",
    "    # If sample size is 1, we cannot choose the best and worst outputs\n",
    "    if sample_size < 2:\n",
    "        raise ValueError(\"Parameter 'sample_size' must be greater than 1.\")\n",
    "\n",
    "    chosen, rejected, prompts, chosen_rewards, rejected_rewards, average_rewards = [], [], [], [], [], []\n",
    "\n",
    "    disable_tqdm = not os.isatty(1)\n",
    "    for i in tqdm(range(len(selected_src_encodec)), desc=\"Processing Data\", disable=disable_tqdm):\n",
    "        rewards, tokenized_outputs = [], []\n",
    "        size_of_packed_input = (\n",
    "            len(selected_src_encodec[i][0]) +\n",
    "            len(ar_tokenizer(selected_instruction[i])[\"input_ids\"][1:-1]) +\n",
    "            3\n",
    "        )\n",
    "        if 4 < size_of_packed_input <= 1024:\n",
    "            selected_src_encodec_list = [selected_src_encodec[i]]*sample_size\n",
    "            selected_instruction_list = [selected_instruction[i]]*sample_size\n",
    "            rewards, tokenized_outputs = generate_output_batch(\n",
    "                model=model,\n",
    "                ar_model=ar_model, \n",
    "                nar_model=nar_model, \n",
    "                ar_tokenizer=ar_tokenizer, \n",
    "                nar_tokenizer=nar_tokenizer,\n",
    "                src_encodec = selected_src_encodec_list,\n",
    "                instruction=selected_instruction_list, \n",
    "                clap_model=clap_model,\n",
    "                accelerator=accelerator,\n",
    "                args_predict=args_predict,\n",
    "                episode_counter=f\"data_{i}\",\n",
    "                base_path=base_path, \n",
    "                temperature=temperature\n",
    "            )\n",
    "\n",
    "        valid_rewards = [r for r in rewards if r is not None]\n",
    "        valid_outputs = [tokenized_outputs[j] for j in range(len(rewards)) if rewards[j] is not None]\n",
    "\n",
    "        if len(valid_rewards) >= 2:\n",
    "            max_reward_index = np.argmax(valid_rewards)\n",
    "            min_reward_index = np.argmin(valid_rewards)\n",
    "            average_reward = np.mean(valid_rewards)\n",
    "            chosen_output = valid_outputs[max_reward_index]\n",
    "            rejected_output = valid_outputs[min_reward_index]\n",
    "\n",
    "            obs_input = pack_inputs_v2(ar_tokenizer, selected_src_encodec[i], selected_instruction[i])\n",
    "            tokenize_input = ar_tokenizer.convert_ids_to_tokens(obs_input)\n",
    "            tokenize_input_str = ar_tokenizer.convert_tokens_to_string(tokenize_input)\n",
    "            prompts.append(tokenize_input_str)\n",
    "\n",
    "            chosen.append(chosen_output)\n",
    "            chosen_rewards.append(valid_rewards[max_reward_index])\n",
    "            rejected.append(rejected_output)\n",
    "            rejected_rewards.append(valid_rewards[min_reward_index])\n",
    "            average_rewards.append(average_reward)\n",
    "        else:\n",
    "            print(f\"Not enough valid rewards for data index {i}\")\n",
    "\n",
    "    # If there is only one data, we need to double the data because we need it for training set and validation set\n",
    "    if len(selected_src_encodec) == 1:\n",
    "        chosen *= 2\n",
    "        rejected *= 2\n",
    "        prompts *= 2\n",
    "        chosen_rewards *= 2\n",
    "        rejected_rewards *= 2\n",
    "        average_rewards *= 2    \n",
    "    \n",
    "    return chosen, rejected, prompts, chosen_rewards, rejected_rewards, average_rewards\n",
    "\n",
    "def generate_data(model,\n",
    "                  ar_model, \n",
    "                  ar_tokenizer, \n",
    "                  nar_model, \n",
    "                  nar_tokenizer, \n",
    "                  clap_model,\n",
    "                  accelerator,\n",
    "                  selected_src_encodec: List[list], \n",
    "                  selected_instruction: List[str],\n",
    "                  args_predict: SimpleNamespace, \n",
    "                  sample_size: int, \n",
    "                  iteration: int, \n",
    "                  agent_output_dir: str, \n",
    "                  base_path: str = \"/work/b0990106x/trl\", \n",
    "                  temperature: float = 1.0\n",
    ") -> Tuple[dict, List[float], List[float]]:\n",
    "    \"\"\"\n",
    "    Generates data for the dataset and saves info to a JSON file.\n",
    "    Returns:\n",
    "        tuple:\n",
    "            data_for_dataset (dict): A dictionary containing the data for the dataset.\n",
    "            chosen_rewards (List[float]): A list of rewards for the chosen outputs.\n",
    "            rejected_rewards (List[float]): A list of rewards for the rejected outputs.\n",
    "    \"\"\"\n",
    "    chosen, rejected, prompts, chosen_rewards, rejected_rewards, average_rewards = process_data_batch(\n",
    "        sample_size=sample_size,\n",
    "        model=model,\n",
    "        ar_model=ar_model,\n",
    "        nar_model=nar_model,\n",
    "        ar_tokenizer=ar_tokenizer,\n",
    "        nar_tokenizer=nar_tokenizer,\n",
    "        selected_src_encodec=selected_src_encodec,\n",
    "        selected_instruction=selected_instruction,\n",
    "        args_predict=args_predict,\n",
    "        base_path=base_path,\n",
    "        temperature=temperature,\n",
    "        iteration = iteration,\n",
    "        clap_model=clap_model,\n",
    "        accelerator=accelerator\n",
    "    )\n",
    "\n",
    "    data = {\n",
    "        \"prompt\": prompts,\n",
    "        \"chosen\": chosen,\n",
    "        \"rejected\": rejected,\n",
    "        \"chosen_rewards\": chosen_rewards,\n",
    "        \"rejected_rewards\": rejected_rewards,\n",
    "        \"average_rewards\": average_rewards\n",
    "    }\n",
    "\n",
    "    with open(f\"{agent_output_dir}/data_iter_{iteration}.json\", \"w\") as outfile:\n",
    "        json.dump(data, outfile, indent=4)\n",
    "\n",
    "    data_for_dataset = {key: data[key] for key in [\"prompt\", \"chosen\", \"rejected\"]}\n",
    "\n",
    "    return data_for_dataset, chosen_rewards, rejected_rewards\n",
    "\n",
    "def train_iteration(model, \n",
    "                    model_checkpoint,\n",
    "                    iteration, \n",
    "                    data_size, \n",
    "                    sample_size, \n",
    "                    ar_model, \n",
    "                    ar_tokenizer,\n",
    "                    nar_model, \n",
    "                    nar_tokenizer,\n",
    "                    all_src_encodec, \n",
    "                    all_instruction, \n",
    "                    args_predict, \n",
    "                    agent_output_dir,\n",
    "                    model_output_dir_base, \n",
    "                    clap_model,\n",
    "                    accelerator,\n",
    "                    beta = 0.1, \n",
    "                    temperature = 1.0,\n",
    "                    base_path=\"/work/b0990106x/trl\",\n",
    "                    resume_from_checkpoint = False,\n",
    "                    learning_rate = 5e-07,\n",
    "                    num_train_epochs = 100,\n",
    "                    max_length = 1024*9,\n",
    "                    max_prompt_length = 1024*9,\n",
    "                    max_target_length = 1024*9,\n",
    "                    per_device_train_batch_size = 1,\n",
    "                    gradient_accumulation_steps = 1,\n",
    "                    seed = 42,\n",
    "):\n",
    "    \"\"\"\n",
    "    Executes one training iteration: generates data, trains the model, and saves the output.\n",
    "    \"\"\"\n",
    "    # print(f\"Iteration {iteration}\")\n",
    "\n",
    "    # ar_model = BartForConditionalGeneration.from_pretrained(model_checkpoint)\n",
    "    # ar_tokenizer = AutoTokenizer.from_pretrained(ar_checkpoint)\n",
    "    # ar_tokenizer.pad_token = ar_tokenizer.eos_token\n",
    "    # nar_model = NARBartForConditionalGeneration.from_pretrained(nar_checkpoint)\n",
    "    # nar_tokenizer = AutoTokenizer.from_pretrained(nar_checkpoint)\n",
    "\n",
    "    selected_src_encodec = all_src_encodec[:data_size]\n",
    "    selected_instruction = all_instruction[:data_size]\n",
    "\n",
    "    data_for_dataset, chosen_rewards, rejected_rewards = generate_data(model=model, \n",
    "                                                                    ar_model=ar_model,\n",
    "                                                                    ar_tokenizer=ar_tokenizer,\n",
    "                                                                    nar_model=nar_model,\n",
    "                                                                    nar_tokenizer=nar_tokenizer,\n",
    "                                                                    selected_src_encodec=selected_src_encodec,\n",
    "                                                                    selected_instruction=selected_instruction,\n",
    "                                                                    args_predict=args_predict,\n",
    "                                                                    sample_size=sample_size,\n",
    "                                                                    iteration=iteration,\n",
    "                                                                    agent_output_dir=agent_output_dir,\n",
    "                                                                    base_path=base_path,\n",
    "                                                                    temperature=temperature,\n",
    "                                                                    clap_model=clap_model,\n",
    "                                                                    accelerator=accelerator)\n",
    "\n",
    "    dataset = Dataset.from_dict(data_for_dataset)\n",
    "    dataset_dict = dataset.train_test_split(test_size=0.1, shuffle=True, seed=seed)\n",
    "    train_dataset = dataset_dict[\"train\"]\n",
    "    val_dataset = dataset_dict[\"test\"]\n",
    "    \n",
    "    # print train_dataset and val_dataset\n",
    "    if iteration < 2:\n",
    "        print(\"train_dataset\", train_dataset.to_dict())\n",
    "        print(\"val_dataset\", val_dataset.to_dict())\n",
    "\n",
    "    model_output_dir = f\"{model_output_dir_base}/iter_{iteration}\"\n",
    "    os.makedirs(model_output_dir, exist_ok=True)\n",
    "\n",
    "    # model = AutoModelForSeq2SeqLMWithValueHead.from_pretrained(model_checkpoint, return_dict=True)\n",
    "    model_ref = create_reference_model(model)\n",
    "    \n",
    "    train_model(model=model,\n",
    "                model_ref=model_ref,\n",
    "                ar_tokenizer=ar_tokenizer,\n",
    "                train_dataset=train_dataset,\n",
    "                val_dataset=val_dataset,\n",
    "                model_output_dir=model_output_dir,\n",
    "                beta=beta,\n",
    "                resume_from_checkpoint=resume_from_checkpoint,\n",
    "                model_checkpoint=model_checkpoint,\n",
    "                learning_rate = learning_rate,\n",
    "                num_train_epochs = num_train_epochs,\n",
    "                max_length = max_length,\n",
    "                max_prompt_length = max_prompt_length,\n",
    "                max_target_length = max_target_length,\n",
    "                per_device_train_batch_size = per_device_train_batch_size,\n",
    "                gradient_accumulation_steps = gradient_accumulation_steps,\n",
    "                seed = seed)\n",
    "\n",
    "    return f\"{model_output_dir}/dpo_model\", chosen_rewards, rejected_rewards"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "# Load all data\n",
    "selected_src_encodec, selected_instruction = extract_data_from_json('dpo_data/src_encodec.json')\n",
    "\n",
    "# Define paths and device\n",
    "base_path = \"/work/b0990106x/trl\"\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# Define timestamp\n",
    "now = datetime.now()\n",
    "ts = now.strftime(\"%m%d-%H%M\")\n",
    "print(\"timestamp:\", ts)\n",
    "\n",
    "# Define paths\n",
    "model_output_dir = os.path.join(base_path, \"model_output\", ts) # Location where the model are saved\n",
    "agent_output_dir = os.path.join(base_path, \"output\", ts) # Path of saving the generated audio for reward model to evaluate\n",
    "os.makedirs(model_output_dir, exist_ok=True)\n",
    "os.makedirs(agent_output_dir, exist_ok=True)\n",
    "\n",
    "seed = 42 # Training: seed\n",
    "\n",
    "# Define arguments \n",
    "args_predict = SimpleNamespace(output_path=f\"{base_path}/output/{ts}/example.wav\", seed=seed, device=device)\n",
    "ar_checkpoint = \"lca0503/speech-chatgpt-base-ar-v2-epoch10-wotrans\"\n",
    "nar_checkpoint = \"lca0503/speech-chatgpt-base-nar-v2-epoch4-wotrans\"\n",
    "\n",
    "# Models and Iterations\n",
    "model_checkpoint = ar_checkpoint # Prepare: set the initial model checkpoint\n",
    "sample_size = 10 # Prepare Dataset: generate how many outputs to select max and min for chosen and rejected (original: 10)\n",
    "num_iterations = 1000  # Training: train how many iterations (original: 100)\n",
    "# train_selected_indices = [8, 16]\n",
    "train_selected_indices = [8]\n",
    "# train_selected_indices = [9]\n",
    "# train_selected_indices = random.sample(range(len(selected_src_encodec)), 5) # Training: train on selected data indicies from all_src_encodec\n",
    " # Training: train on selected data indicies from all_src_encodec\n",
    "data_size_per_iteration = len(train_selected_indices) # Training: each iteration will train how many data\n",
    "\n",
    "# Define Training Configuration\n",
    "beta = 0.1 # Training: beta value for DPO\n",
    "learning_rate = 5e-07 # Training: learning rate (original: 5e-07)\n",
    "num_train_epochs = 3 # Training: number of training epochs (original: 3)\n",
    "max_length = 1024*9 # Training: max length of the model\n",
    "max_prompt_length = 1024*9 # Training: max length of the prompt\n",
    "max_target_length = 1024*9 # Training: max length of the target\n",
    "per_device_train_batch_size = 8 # Training: batch size (original: 1)\n",
    "gradient_accumulation_steps = 1 # Training: gradient accumulation steps\n",
    "\n",
    "# Evaluation Configuration\n",
    "eval_train = True # Evaluation: evaluate on training data or not\n",
    "eval_test = False # Evaluation: evaluate on testing data or not\n",
    "eval_train_indices = train_selected_indices # Evaluation: evaluate on training data indicies from all_src_encodec\n",
    "eval_test_indices = random.sample(range(len(selected_src_encodec)), 5) # Evaluation: evaluate on testing data indicies from all_src_encodec\n",
    "eval_train_data_len = 1000 # Evaluation: evaluate how many training data\n",
    "eval_test_data_len = len(eval_test_indices) # Evaluation: evaluate how many testing data\n",
    "num_eval = 10 # Evaluation: evaluate how many times per data (original: 10)\n",
    "eval_frequency = 1 # Evaluation: evaluate every how many iterations\n",
    "# Define temperature\n",
    "# eval_selected_indices = random.sample(range(len(all_src_encodec)), eval_data_len) # Evaluation: select 10 data for evaluation\n",
    "print(f\"length of all_src_encodec: {len(selected_src_encodec)}\") # ~ 9000 data\n",
    "print(f\"length of all_instruction: {len(selected_instruction)}\") # ~ 9000 data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sr = 24000\n",
    "text_enc_name = \"google/flan-t5-large\"\n",
    "text_enc_dim = 1024\n",
    "text_blstm_dim = 256\n",
    "speech_enc_name = \"wavlm\"\n",
    "speech_enc_dim = 768\n",
    "speech_blstm_dim = 256\n",
    "rep_dim = 512\n",
    "sub_dim = 0\n",
    "n_sub = 1\n",
    "ckpt_pth=f'{base_path}/CLAPS/pretrained/7d/cp_claps_blstm_m_50k_v3/cp_0045000'\n",
    "project_dir = \"cp_claps\"\n",
    "\n",
    "a = argparse.Namespace(\n",
    "        sr=sr,\n",
    "        text_enc_name=text_enc_name,\n",
    "        text_enc_dim=text_enc_dim,\n",
    "        text_blstm_dim=text_blstm_dim,\n",
    "        speech_enc_name=speech_enc_name,\n",
    "        speech_enc_dim=speech_enc_dim,\n",
    "        speech_blstm_dim=speech_blstm_dim,\n",
    "        rep_dim=rep_dim,\n",
    "        sub_dim=sub_dim,\n",
    "        n_sub=n_sub,  # Number of subspaces, if any\n",
    "        ckpt_pth=ckpt_pth,  # Set your checkpoint path\n",
    "        project_dir=project_dir  # Example project directory\n",
    "    )\n",
    "\n",
    "clap_model, accelerator = load_model(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "print(f\"num_iterations: {num_iterations}\")\n",
    "print(f\"data_size_per_iteration: {data_size_per_iteration}\")\n",
    "print(f\"sample_size: {sample_size}\")\n",
    "print(f\"beta: {beta}\")\n",
    "print(f\"learning_rate: {learning_rate}\")\n",
    "print(f\"num_train_epochs: {num_train_epochs}\")\n",
    "print(f\"ar_checkpoint: {ar_checkpoint}\")\n",
    "print(f\"nar_checkpoint: {nar_checkpoint}\")\n",
    "print(f\"args_predict: {args_predict}\")\n",
    "print(f\"model_output_dir: {model_output_dir}\")\n",
    "print(f\"agent_output_dir: {agent_output_dir}\")\n",
    "print(f\"base_path: {base_path}\")\n",
    "print(f\"device: {device}\")\n",
    "print(f\"eval_train_data_len: {eval_train_data_len}\")\n",
    "print(f\"eval_test_data_len: {eval_test_data_len}\")\n",
    "print(f\"eval_train_indices: {eval_train_indices}\")\n",
    "print(f\"eval_test_indices: {eval_test_indices}\")\n",
    "print(f\"eval_train: {eval_train}\")\n",
    "print(f\"eval_test: {eval_test}\")\n",
    "print(f\"num_eval: {num_eval}\")\n",
    "\n",
    "# print training data\n",
    "for i in train_selected_indices:\n",
    "    print('training idx', i,':', selected_instruction[i])\n",
    "    \n",
    "# print evaluation data\n",
    "if eval_test:\n",
    "    for i in eval_test_indices:\n",
    "        print('evaluation idx', i,':', selected_instruction[i])\n",
    "\n",
    "if eval_train:\n",
    "    for i in eval_train_indices:\n",
    "        print('evaluation idx', i,':', selected_instruction[i])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoModelForSeq2SeqLMWithValueHead.from_pretrained(model_checkpoint, return_dict=True)\n",
    "ar_model = BartForConditionalGeneration.from_pretrained(ar_checkpoint)\n",
    "ar_tokenizer = AutoTokenizer.from_pretrained(ar_checkpoint)\n",
    "# ar_tokenizer.pad_token = ar_tokenizer.eos_token\n",
    "nar_model = NARBartForConditionalGeneration.from_pretrained(nar_checkpoint)\n",
    "nar_tokenizer = AutoTokenizer.from_pretrained(nar_checkpoint)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logging Start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "\n",
    "log_path = f'{model_output_dir}/log_training.log'\n",
    "print(f\"Logging to: {log_path}\")\n",
    "\n",
    "# Set up logging\n",
    "logging.basicConfig(\n",
    "    filename=log_path, \n",
    "    filemode='a', \n",
    "    format='%(asctime)s - %(levelname)s - %(message)s', \n",
    "    level=logging.INFO\n",
    ")\n",
    "\n",
    "logging.info(\n",
    "    f\"Parameters:\\n\"\n",
    "    f\"Prepare Data: sample_size: {sample_size}\\n\"\n",
    "    f\"Training: num_iterations: {num_iterations}\\n\"\n",
    "    f\"Training: data_size_per_iteration: {data_size_per_iteration}\\n\"\n",
    "    f\"Training: train_selected_indices: {train_selected_indices}\\n\"\n",
    "    f\"Training: beta: {beta}\\n\"\n",
    "    f\"Training: learning_rate: {learning_rate}\\n\"\n",
    "    f\"Training: num_train_epochs: {num_train_epochs}\\n\"\n",
    "    f\"Training: max_length: {max_length}\\n\"\n",
    "    f\"Training: max_prompt_length: {max_prompt_length}\\n\"\n",
    "    f\"Training: max_target_length: {max_target_length}\\n\"\n",
    "    f\"Training: per_device_train_batch_size: {per_device_train_batch_size}\\n\"\n",
    "    f\"Training: gradient_accumulation_steps: {gradient_accumulation_steps}\\n\"\n",
    "    f\"Training: seed: {seed}\\n\"\n",
    "    f\"Training: ar_checkpoint: {ar_checkpoint}\\n\"\n",
    "    f\"Training: nar_checkpoint: {nar_checkpoint}\\n\"\n",
    "    f\"Training: args_predict: {args_predict}\\n\"\n",
    "    f\"Training: model_output_dir: {model_output_dir}\\n\"\n",
    "    f\"Training: agent_output_dir: {agent_output_dir}\\n\"\n",
    "    f\"Training: base_path: {base_path}\\n\"\n",
    "    f\"Training: device: {device}\\n\"\n",
    "    f\"Evaluation: eval_train_data_len: {eval_train_data_len}\\n\"\n",
    "    f\"Evaluation: eval_test_data_len: {eval_test_data_len}\\n\"\n",
    "    f\"Evaluation: eval_train_indices: {eval_train_indices}\\n\"\n",
    "    f\"Evaluation: eval_test_indices: {eval_test_indices}\\n\"\n",
    "    f\"Evaluation: eval_train: {eval_train}\\n\"\n",
    "    f\"Evaluation: eval_test: {eval_test}\\n\"\n",
    "    f\"Evaluation: num_eval: {num_eval}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initial Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "# Start time\n",
    "total_start_time = time.time()\n",
    "if eval_train:\n",
    "    # eval dpo claps\n",
    "    # original_model_metrics, original_model_rewards = eval_dpo_claps_batch(nar_model=nar_model,\n",
    "    #                                                                 ar_tokenizer=ar_tokenizer,\n",
    "    #                                                                 nar_tokenizer=nar_tokenizer,\n",
    "    #                                                                 trained_model=model,\n",
    "    #                                                                 args_predict=args_predict,\n",
    "    #                                                                 all_src_encodec=selected_src_encodec,\n",
    "    #                                                                 all_instruction=selected_instruction,\n",
    "    #                                                                 iteration = -1,\n",
    "    #                                                                 num_evaluations = num_eval,\n",
    "    #                                                                 eval_data_len=eval_train_data_len,\n",
    "    #                                                                 selected_indices=eval_train_indices,\n",
    "    #                                                                 device=device,\n",
    "    #                                                                 clap_model=clap_model,\n",
    "    #                                                                 accelerator=accelerator\n",
    "    #                                                                 )\n",
    "    # logging.info(f\"Original Model Train Set Evaluation: \")\n",
    "    # logging.info(f\"Original model metrics on training set: {original_model_metrics}\")\n",
    "    # logging.info(f\"Original model rewards on training set: {original_model_rewards}\")\n",
    "    \n",
    "    # eval dpo mos\n",
    "    original_model_metrics_mos, original_model_rewards_mos = eval_dpo_mos(nar_model=nar_model,\n",
    "                                                                    ar_tokenizer=ar_tokenizer,\n",
    "                                                                    nar_tokenizer=nar_tokenizer,\n",
    "                                                                    trained_model=model,\n",
    "                                                                    args_predict=args_predict,\n",
    "                                                                    all_src_encodec=selected_src_encodec,\n",
    "                                                                    all_instruction=selected_instruction,\n",
    "                                                                    iteration = -1,\n",
    "                                                                    num_evaluations = num_eval,\n",
    "                                                                    eval_data_len=eval_train_data_len,\n",
    "                                                                    selected_indices=eval_train_indices,\n",
    "                                                                    device=device,\n",
    "                                                                    )\n",
    "    logging.info(f\"Original model metrics on training set: {original_model_metrics_mos}\")\n",
    "    logging.info(f\"Original model rewards on training set: {original_model_rewards_mos}\")\n",
    "    \n",
    "    # reward_list = []\n",
    "    # for rewards in original_model_rewards:\n",
    "    #     filter_rewards = [r for r in rewards if r is not None]\n",
    "    #     if len(filter_rewards) == 0:\n",
    "    #         reward_list.append(None)\n",
    "    #     else:\n",
    "    #         reward_list.append(np.mean(filter_rewards))\n",
    "    # logging.info(f\"Original model Cosine_Sim score list on training set: {reward_list}\")\n",
    "            \n",
    "    reward_list_mos = []\n",
    "    for rewards in original_model_rewards_mos:\n",
    "        filter_rewards = [r for r in rewards if r is not None]\n",
    "        if len(filter_rewards) == 0:\n",
    "            reward_list_mos.append(None)\n",
    "        else:\n",
    "            reward_list_mos.append(np.mean(filter_rewards))\n",
    "    logging.info(f\"Original model MOS score list on training set: {reward_list_mos}\")\n",
    "    \n",
    "    # filter_reward_list = [r for r in reward_list if r is not None]\n",
    "    # if len(filter_reward_list) != 0:\n",
    "    #     logging.info(f\"Original model average rewards on training set: {np.mean(filter_reward_list)}\")\n",
    "    # else: \n",
    "    #     logging.info(f\"Original model average rewards on training set: None\")\n",
    "        \n",
    "    filter_reward_list_mos = [r for r in reward_list_mos if r is not None]\n",
    "    if len(filter_reward_list_mos) != 0:\n",
    "        logging.info(f\"Original model average MOS on training set: {np.mean(filter_reward_list_mos)}\")\n",
    "    else:\n",
    "        logging.info(f\"Original model average MOS on training set: None\")\n",
    "        \n",
    "    # weighted_reward = 0.5 * np.mean(filter_reward_list) + 0.5 * np.mean(filter_reward_list_mos)/5\n",
    "    weighted_reward = np.mean(filter_reward_list_mos)/5\n",
    "    logging.info(f\"Original model weighted average rewards on training set: {weighted_reward}\")\n",
    "    \n",
    "if eval_test:\n",
    "    original_model_metrics, original_model_rewards = eval_dpo_claps_batch(nar_model=nar_model,\n",
    "                                                                    ar_tokenizer=ar_tokenizer,\n",
    "                                                                    nar_tokenizer=nar_tokenizer,\n",
    "                                                                    trained_model=model,\n",
    "                                                                    args_predict=args_predict,\n",
    "                                                                    all_src_encodec=selected_src_encodec,\n",
    "                                                                    all_instruction=selected_instruction,\n",
    "                                                                    iteration = -1,\n",
    "                                                                    num_evaluations = num_eval,\n",
    "                                                                    eval_data_len=eval_test_data_len,\n",
    "                                                                    selected_indices=eval_test_indices,\n",
    "                                                                    device=device,\n",
    "                                                                    clap_model=clap_model,\n",
    "                                                                    accelerator=accelerator\n",
    "                                                                    )\n",
    "    logging.info(f\"Original Model Test Set Evaluation: \")\n",
    "    logging.info(f\"Original model metrics on testing set: {original_model_metrics}\")\n",
    "    logging.info(f\"Original model rewards on testing set: {original_model_rewards}\")\n",
    "    reward_list = []\n",
    "    for rewards in original_model_rewards:\n",
    "        filter_rewards = [r for r in rewards if r is not None]\n",
    "        if len(filter_rewards) == 0:\n",
    "            reward_list.append(None)\n",
    "        else:\n",
    "            reward_list.append(np.mean(filter_rewards))\n",
    "    logging.info(f\"Original model reward list on testing set: {reward_list}\")\n",
    "    filter_reward_list = [r for r in reward_list if r is not None]\n",
    "    if len(filter_reward_list) != 0:\n",
    "        logging.info(f\"Original model average rewards on testing set: {np.mean(filter_reward_list)}\")\n",
    "    else: \n",
    "        logging.info(f\"Original model average rewards on testing set: None\")\n",
    "    \n",
    "# If train_selected_indices is not empty, we will use the selected indices for training\n",
    "if train_selected_indices:\n",
    "    batch_src_encodec = [selected_src_encodec[i] for i in train_selected_indices]\n",
    "    batch_instruction = [selected_instruction[i] for i in train_selected_indices]\n",
    "    logging.info(f\"Processing data from selected indices: {train_selected_indices}\")\n",
    "else:\n",
    "    start_idx = 0\n",
    "    end_idx = data_size_per_iteration\n",
    "    batch_src_encodec = selected_src_encodec[start_idx:end_idx] \n",
    "    batch_instruction = selected_instruction[start_idx:end_idx]\n",
    "    logging.info(f\"Processing data from index {start_idx} to {end_idx}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "\n",
    "# warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Start training iterations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "disable_tqdm = not os.isatty(1)\n",
    "for iteration in tqdm(range(num_iterations), desc=\"Training Iterations\", disable=disable_tqdm):\n",
    "    logging.info(f\"-----------Starting iteration {iteration}-----------\")\n",
    "    \n",
    "    # resume = iteration > 0 # resume from the previous checkpoint when iteration > 0\n",
    "    resume = False\n",
    "    \n",
    "    # model_checkpoint is the model checkpoint from the previous iteration\n",
    "    # chosen_rewards and rejected_rewards are the rewards of the data\n",
    "    model_checkpoint, chosen_rewards, rejected_rewards = train_iteration(model,\n",
    "                                model_checkpoint,\n",
    "                                iteration=iteration,\n",
    "                                data_size=data_size_per_iteration,\n",
    "                                sample_size=sample_size,\n",
    "                                ar_model=ar_model,\n",
    "                                ar_tokenizer=ar_tokenizer,\n",
    "                                nar_model=nar_model,\n",
    "                                nar_tokenizer=nar_tokenizer,\n",
    "                                all_src_encodec=batch_src_encodec,\n",
    "                                all_instruction=batch_instruction,\n",
    "                                args_predict=args_predict,\n",
    "                                agent_output_dir=agent_output_dir,\n",
    "                                model_output_dir_base=model_output_dir,\n",
    "                                temperature = 1.0,\n",
    "                                beta=beta,\n",
    "                                base_path=base_path,\n",
    "                                resume_from_checkpoint=resume, \n",
    "                                learning_rate=learning_rate,\n",
    "                                num_train_epochs=num_train_epochs,\n",
    "                                max_length=max_length,\n",
    "                                max_prompt_length=max_prompt_length,\n",
    "                                max_target_length=max_target_length,\n",
    "                                per_device_train_batch_size=per_device_train_batch_size,\n",
    "                                gradient_accumulation_steps=gradient_accumulation_steps,\n",
    "                                seed=seed,\n",
    "                                clap_model=clap_model,\n",
    "                                accelerator=accelerator\n",
    "                                )       \n",
    "\n",
    "    logging.info(f\"Chosen rewards for iteration {iteration}: {chosen_rewards}\")\n",
    "    logging.info(f\"Rejected rewards for iteration {iteration}: {rejected_rewards}\")\n",
    "    logging.info(f\"Finished training iteration {iteration}\")\n",
    "\n",
    "    if (iteration+1) % eval_frequency == 0:\n",
    "    # Evaluate the result of the current iteration\n",
    "        if eval_train:\n",
    "            # eval dpo claps\n",
    "            # trained_model_metrics, trained_model_rewards = eval_dpo_claps_batch(nar_model=nar_model,\n",
    "            #                                                             ar_tokenizer=ar_tokenizer,\n",
    "            #                                                             nar_tokenizer=nar_tokenizer,\n",
    "            #                                                             trained_model=model,\n",
    "            #                                                             args_predict=args_predict,\n",
    "            #                                                             all_src_encodec=selected_src_encodec,\n",
    "            #                                                             all_instruction=selected_instruction,\n",
    "            #                                                             iteration = iteration,\n",
    "            #                                                             num_evaluations = num_eval,\n",
    "            #                                                             eval_data_len=eval_train_data_len,\n",
    "            #                                                             selected_indices=eval_train_indices,\n",
    "            #                                                             device=device,\n",
    "            #                                                             clap_model=clap_model,\n",
    "            #                                                             accelerator=accelerator\n",
    "            #                                                             )\n",
    "            # logging.info(f\"Trained Model Iteration {iteration} Train Set Evaluation: \")\n",
    "            # logging.info(f\"EVAL: Cosine_Sim metrics Training Set for iteration {iteration}: {trained_model_metrics}\")\n",
    "            # logging.info(f\"EVAL: Cosine_Sim score Training Set for iteration {iteration}: {trained_model_rewards}\")\n",
    "            \n",
    "            # eval dpo mos\n",
    "            trained_model_metrics_mos, trained_model_rewards_mos = eval_dpo_mos(nar_model=nar_model,\n",
    "                                                                        ar_tokenizer=ar_tokenizer,\n",
    "                                                                        nar_tokenizer=nar_tokenizer,\n",
    "                                                                        trained_model=model,\n",
    "                                                                        args_predict=args_predict,\n",
    "                                                                        all_src_encodec=selected_src_encodec,\n",
    "                                                                        all_instruction=selected_instruction,\n",
    "                                                                        iteration = iteration,\n",
    "                                                                        num_evaluations = num_eval,\n",
    "                                                                        eval_data_len=eval_train_data_len,\n",
    "                                                                        selected_indices=eval_train_indices,\n",
    "                                                                        device=device\n",
    "                                                                        )\n",
    "            logging.info(f\"EVAL: MOS metrics Training Set for iteration {iteration}: {trained_model_metrics_mos}\")\n",
    "            logging.info(f\"EVAL: MOS score Training Set for iteration {iteration}: {trained_model_rewards_mos}\")\n",
    "            \n",
    "            # reward_list = []\n",
    "            # for rewards in trained_model_rewards:\n",
    "            #     filter_rewards = [r for r in rewards if r is not None]\n",
    "            #     if len(filter_rewards) == 0:\n",
    "            #         reward_list.append(None)\n",
    "            #     else:\n",
    "            #         reward_list.append(np.mean(filter_rewards))\n",
    "            # logging.info(f\"EVAL: Trained model Cosine_Sim score list on training set: {reward_list}\")\n",
    "            \n",
    "            reward_list_mos = []\n",
    "            for rewards in trained_model_rewards_mos:\n",
    "                filter_rewards = [r for r in rewards if r is not None]\n",
    "                if len(filter_rewards) == 0:\n",
    "                    reward_list_mos.append(None)\n",
    "                else:\n",
    "                    reward_list_mos.append(np.mean(filter_rewards))\n",
    "            logging.info(f\"EVAL: Trained model MOS score list on training set: {reward_list_mos}\")\n",
    "            \n",
    "            # filter_reward_list = [r for r in reward_list if r is not None]\n",
    "            # if len(filter_reward_list) != 0:\n",
    "            #     logging.info(f\"EVAL: Trained model average Cosine_Sim score on training set for iteration {iteration}: {np.mean(filter_reward_list)}\")\n",
    "            # else:\n",
    "            #     logging.info(f\"EVAL: Trained model average Cosine_Sim score on training set for iteration {iteration}: None\")\n",
    "            \n",
    "            filter_reward_list_mos = [r for r in reward_list_mos if r is not None]\n",
    "            if len(filter_reward_list_mos) != 0:\n",
    "                logging.info(f\"EVAL: Trained model average MOS score on training set for iteration {iteration}: {np.mean(filter_reward_list_mos)}\")\n",
    "            else:\n",
    "                logging.info(f\"EVAL: Trained model average MOS score on training set for iteration {iteration}: None\")\n",
    "                \n",
    "            # weighted_reward = 0.5 * np.mean(filter_reward_list) + 0.5 * np.mean(filter_reward_list_mos)/5\n",
    "            weighted_reward = np.mean(filter_reward_list_mos)/5\n",
    "            logging.info(f\"EVAL: Trained model weighted average score on training set for iteration {iteration}: {weighted_reward}\")\n",
    "\n",
    "        if eval_test:\n",
    "            trained_model_metrics, trained_model_rewards = eval_dpo_claps_batch(nar_model=nar_model,\n",
    "                                                                        ar_tokenizer=ar_tokenizer,\n",
    "                                                                        nar_tokenizer=nar_tokenizer,\n",
    "                                                                        trained_model=model,\n",
    "                                                                        args_predict=args_predict,\n",
    "                                                                        all_src_encodec=selected_src_encodec,\n",
    "                                                                        all_instruction=selected_instruction,\n",
    "                                                                        iteration = iteration,\n",
    "                                                                        num_evaluations = num_eval,\n",
    "                                                                        eval_data_len=eval_test_data_len,\n",
    "                                                                        selected_indices=eval_test_indices,\n",
    "                                                                        device=device,\n",
    "                                                                        clap_model=clap_model,\n",
    "                                                                        accelerator=accelerator\n",
    "                                                                        )\n",
    "            logging.info(f\"Trained Model Iteration {iteration} Test Set Evaluation: \")\n",
    "            logging.info(f\"EVAL: Cosine_Sim metrics Testing Set for iteration {iteration}: {trained_model_metrics}\")\n",
    "            logging.info(f\"EVAL: Cosine_Sim score Testing Set for iteration {iteration}: {trained_model_rewards}\")\n",
    "\n",
    "            reward_list = []\n",
    "            for rewards in trained_model_rewards:\n",
    "                filter_rewards = [r for r in rewards if r is not None]\n",
    "                if len(filter_rewards) == 0:\n",
    "                    reward_list.append(None)\n",
    "                else:\n",
    "                    reward_list.append(np.mean(filter_rewards))\n",
    "            logging.info(f\"EVAL: Trained model Cosine_Sim score list on testing set: {reward_list}\")\n",
    "            filter_reward_list = [r for r in reward_list if r is not None]\n",
    "            if len(filter_reward_list) != 0:\n",
    "                logging.info(f\"EVAL: Trained model average Cosine_Sim score on testing set: {np.mean(filter_reward_list)}\")\n",
    "            else:\n",
    "                logging.info(f\"EVAL: Trained model average Cosine_Sim score on testing set: None\")\n",
    "\n",
    "    logging.info(f\"-----------Finished iteration {iteration}-----------\")\n",
    "total_end_time = time.time()\n",
    "\n",
    "# Calculate total time taken\n",
    "total_time_taken = total_end_time - total_start_time\n",
    "logging.info(f\"Total time taken for the entire process: {total_time_taken:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "import ast\n",
    "import numpy as np\n",
    "\n",
    "# Function to parse the log file for both EVAL and Original model metrics\n",
    "def parse_log_file(log_path):\n",
    "    eval_pattern = re.compile(\n",
    "        r\"EVAL: Cosine_Sim metrics Training Set for iteration (\\d+): (.+)\"\n",
    "    )\n",
    "    original_model_pattern = re.compile(\n",
    "        r\"Original model metrics on training set: (.+)\"\n",
    "    )\n",
    "    \n",
    "    data = {\"EVAL\": {}, \"Original\": []}\n",
    "\n",
    "    # Read the log file line by line\n",
    "    with open(log_path, 'r') as log_file:\n",
    "        for line in log_file:\n",
    "            eval_match = eval_pattern.search(line)\n",
    "            original_match = original_model_pattern.search(line)\n",
    "\n",
    "            # If it's an EVAL line\n",
    "            if eval_match:\n",
    "                iteration = int(eval_match.group(1)) + 1  # Adding 1 to iteration as requested\n",
    "                metrics_list = eval_match.group(2).strip()\n",
    "\n",
    "                # Convert the metrics_list string to a Python object (list of dicts)\n",
    "                metrics_list = ast.literal_eval(metrics_list)\n",
    "\n",
    "                # Store means and std_devs for this iteration\n",
    "                means = []\n",
    "                std_devs = []\n",
    "                counts = []\n",
    "\n",
    "                for metrics in metrics_list:\n",
    "                    mean = metrics['metrics']['mean']\n",
    "                    std_dev = metrics['metrics']['std_dev']\n",
    "                    count = len(metrics['metrics']['rewards'])  # Number of rewards is the sample size\n",
    "\n",
    "                    means.append(mean)\n",
    "                    std_devs.append(std_dev)\n",
    "                    counts.append(count)\n",
    "\n",
    "                # Store mean, std_dev, and count for each iteration\n",
    "                data[\"EVAL\"][iteration] = {\n",
    "                    \"means\": means,\n",
    "                    \"std_devs\": std_devs,\n",
    "                    \"counts\": counts\n",
    "                }\n",
    "\n",
    "            # If it's an Original Model Metrics line\n",
    "            elif original_match:\n",
    "                metrics_list = original_match.group(1).strip()\n",
    "\n",
    "                # Convert the metrics_list string to a Python object (list of dicts)\n",
    "                metrics_list = ast.literal_eval(metrics_list)\n",
    "\n",
    "                for metrics in metrics_list:\n",
    "                    mean = metrics['metrics']['mean']\n",
    "                    std_dev = metrics['metrics']['std_dev']\n",
    "                    data[\"Original\"].append((mean, std_dev))\n",
    "\n",
    "    return data\n",
    "\n",
    "# Function to calculate the pooled standard deviation\n",
    "def pooled_std_dev(std_devs, counts):\n",
    "    # Pooled variance formula\n",
    "    numerator = sum((counts[i] - 1) * (std_devs[i] ** 2) for i in range(len(std_devs)))\n",
    "    denominator = sum(counts[i] - 1 for i in range(len(counts)))\n",
    "\n",
    "    if denominator > 0:\n",
    "        pooled_variance = numerator / denominator\n",
    "        return np.sqrt(pooled_variance)\n",
    "    else:\n",
    "        return 0  # In case of single value or no variance\n",
    "\n",
    "# Function to plot iteration vs the average mean and pooled std_dev\n",
    "def plot_metrics(data):\n",
    "    plt.figure(figsize=(10, 6))\n",
    "\n",
    "    # Plot EVAL data (average across all idx)\n",
    "    iterations = sorted(data[\"EVAL\"].keys())\n",
    "    avg_means = []\n",
    "    pooled_std_devs = []\n",
    "\n",
    "    for iteration in iterations:\n",
    "        means = data[\"EVAL\"][iteration][\"means\"]\n",
    "        std_devs = data[\"EVAL\"][iteration][\"std_devs\"]\n",
    "        counts = data[\"EVAL\"][iteration][\"counts\"]\n",
    "\n",
    "        # Calculate the average mean for the iteration\n",
    "        avg_mean = sum(means) / len(means)\n",
    "        avg_means.append(avg_mean)\n",
    "\n",
    "        # Calculate the pooled standard deviation for the iteration\n",
    "        pooled_std_dev_value = pooled_std_dev(std_devs, counts)\n",
    "        pooled_std_devs.append(pooled_std_dev_value)\n",
    "\n",
    "    # Plot the average means with pooled std_dev as error bars\n",
    "    plt.errorbar(iterations, avg_means, yerr=pooled_std_devs, fmt='o-', capsize=5, label='Average Mean with Pooled Std Dev')\n",
    "\n",
    "    # Plot Original Model data (if available)\n",
    "    if \"Original\" in data and len(data[\"Original\"]) > 0:\n",
    "        original_means = [item[0] for item in data[\"Original\"]]\n",
    "        original_std_devs = [item[1] for item in data[\"Original\"]]\n",
    "        avg_original_mean = np.mean(original_means)\n",
    "        pooled_original_std_dev = pooled_std_dev(original_std_devs, [10] * len(original_std_devs))  # Assuming 10 samples per idx\n",
    "\n",
    "        plt.errorbar([0], [avg_original_mean], yerr=[pooled_original_std_dev], fmt='x', color='r', label='Original Model Average Mean')\n",
    "\n",
    "    plt.xlabel('Iteration')\n",
    "    plt.ylabel('Mean')\n",
    "    plt.title('Iteration vs Average Mean with Pooled Std Dev')\n",
    "    plt.grid(True)\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# Parse the log file\n",
    "data = parse_log_file(log_path)\n",
    "\n",
    "# Plot the metrics\n",
    "plot_metrics(data)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "trl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
