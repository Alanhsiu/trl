{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Audio Metrics Test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Audio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from vc.trainer_encodec_vc_inference import get_ar_prediction_sampling_rate, pack_inputs_v2\n",
    "from types import SimpleNamespace\n",
    "from vc.encodec_model.nar_bart_model import NARBartForConditionalGeneration\n",
    "from transformers import BartForConditionalGeneration, AutoTokenizer\n",
    "from dpo_eval import extract_data_from_json\n",
    "import os\n",
    "\n",
    "base_path = \"/work/b0990106x/trl\"\n",
    "ts = \"test_sampling_rate\"\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "agent_output_dir = os.path.join(base_path, \"output\", ts) # Path of saving the generated audio for reward model to evaluate\n",
    "os.makedirs(agent_output_dir, exist_ok=True)\n",
    "\n",
    "args_predict = SimpleNamespace(output_path=f\"{base_path}/output/{ts}/example.wav\", seed=0, device=device)\n",
    "ar_checkpoint = \"lca0503/speech-chatgpt-base-ar-v2-epoch10-wotrans\"\n",
    "nar_checkpoint = \"lca0503/speech-chatgpt-base-nar-v2-epoch4-wotrans\"\n",
    "\n",
    "nar_model = NARBartForConditionalGeneration.from_pretrained(nar_checkpoint)\n",
    "ar_tokenizer = AutoTokenizer.from_pretrained(ar_checkpoint)\n",
    "nar_tokenizer = AutoTokenizer.from_pretrained(nar_checkpoint)\n",
    "ar_model = BartForConditionalGeneration.from_pretrained(ar_checkpoint, return_dict=True)\n",
    "\n",
    "all_src_encodec, all_instruction, all_tgt_encodec = extract_data_from_json('dpo_data/src_encodec.json')\n",
    "# single_src_encodec includes the first two examples in the dataset\n",
    "# single_instruction includes the first two examples in the dataset\n",
    "\n",
    "src_encodec = all_src_encodec[0:2]\n",
    "instruction = all_instruction[0:2]\n",
    "output_checkpoints = []\n",
    "for i in range(len(src_encodec)):\n",
    "    single_src_encodec = src_encodec[i]\n",
    "    single_instruction = instruction[i]\n",
    "    _, _, output_checkpoint = get_ar_prediction_sampling_rate(args_predict, ar_model, nar_model, ar_tokenizer, nar_tokenizer, single_src_encodec, single_instruction, episode_counter=i, temperature = 1.0)\n",
    "    output_checkpoints.append(output_checkpoint)   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PESQ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pesq import pesq\n",
    "from scipy.io import wavfile\n",
    "\n",
    "output_checkpoint_1 = \"/work/b0990106x/trl/output/test_sampling_rate/example_save_0.wav\"\n",
    "output_checkpoint_2 = \"/work/b0990106x/trl/output/test_sampling_rate/example_save_1.wav\"\n",
    "\n",
    "rate, ref = wavfile.read(output_checkpoint_1)\n",
    "print(rate)\n",
    "rate, deg = wavfile.read(output_checkpoint_2)\n",
    "print(rate)\n",
    "\n",
    "print(pesq(rate, ref, deg, 'wb'))\n",
    "print(pesq(rate, ref, deg, 'nb'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ASR Whisper\n",
    "\n",
    "### Requires:\n",
    "#### pip install --user transformers==4.36"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "from transformers import AutoModelForSpeechSeq2Seq, AutoProcessor, pipeline\n",
    "import torch\n",
    "import torchaudio\n",
    "\n",
    "from transformers import WhisperForConditionalGeneration, WhisperProcessor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'text': \" There's even a white row of beehives in the orchard under the walnut trees.\", 'chunks': [{'timestamp': (0.0, 12.96), 'text': \" There's even a white row of beehives in the orchard under the walnut trees.\"}]}\n"
     ]
    }
   ],
   "source": [
    "output_temp = \"/work/b0990106x/trl/output/0728-1417/example_save_eval_-1_data_0.wav\"\n",
    "output_checkpoint_1 = \"/work/b0990106x/trl/output/test_sampling_rate/example_save_0.wav\"\n",
    "output_checkpoint_2 = \"/work/b0990106x/trl/output/test_sampling_rate/example_save_1.wav\"\n",
    "\n",
    "def load_recorded_audio(path_audio,input_sample_rate=24000,output_sample_rate=24000):\n",
    "    # Dataset: convert recorded audio to vector\n",
    "    waveform, sample_rate = torchaudio.load(path_audio)\n",
    "    waveform_resampled = torchaudio.functional.resample(waveform, orig_freq=input_sample_rate, new_freq=output_sample_rate) #change sample rate to 16000 to match training. \n",
    "    sample = waveform_resampled.numpy()[0]\n",
    "    return sample\n",
    "\n",
    "device = torch.device('cpu')\n",
    "torch_dtype = torch.float32\n",
    "\n",
    "model = WhisperForConditionalGeneration.from_pretrained(\"openai/whisper-medium\")\n",
    "model.to(device)\n",
    "processor = WhisperProcessor.from_pretrained(\"openai/whisper-medium\")\n",
    "\n",
    "whisper = pipeline(\n",
    "    \"automatic-speech-recognition\",\n",
    "    model=model,\n",
    "    tokenizer=processor.tokenizer,\n",
    "    feature_extractor=processor.feature_extractor,\n",
    "    max_new_tokens=128,\n",
    "    chunk_length_s=30,\n",
    "    batch_size=16,\n",
    "    return_timestamps=True,\n",
    "    torch_dtype=torch_dtype,\n",
    "    device=device,\n",
    ")\n",
    "\n",
    "text = whisper(output_checkpoint_1)\n",
    "\n",
    "print(text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Word Error Rate\n",
    "#### Require pip install jiwer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " There's even a white row of beehives in the orchard under the walnut trees.\n"
     ]
    }
   ],
   "source": [
    "text = {'text': \" There's even a white row of beehives in the orchard under the walnut trees.\", 'chunks': [{'timestamp': (0.0, 12.96), 'text': \" There's even a white row of beehives in the orchard under the walnut trees.\"}]}\n",
    "# print the text dict\n",
    "print(text['text'])\n",
    "hypothesis = text['text']\n",
    "reference = \"There is even a white row of beehives in the orchard, under the walnut trees.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word Error Rate (WER) : 0.06666666666666667\n"
     ]
    }
   ],
   "source": [
    "import jiwer\n",
    "\n",
    "transforms = jiwer.Compose(\n",
    "    [\n",
    "        jiwer.ExpandCommonEnglishContractions(),\n",
    "        jiwer.RemoveEmptyStrings(),\n",
    "        jiwer.ToLowerCase(),\n",
    "        jiwer.RemoveMultipleSpaces(),\n",
    "        jiwer.Strip(),\n",
    "        jiwer.RemovePunctuation(),\n",
    "        jiwer.ReduceToListOfListOfWords(),\n",
    "    ]\n",
    ")\n",
    "\n",
    "wer = jiwer.wer(\n",
    "                reference,\n",
    "                hypothesis,\n",
    "                truth_transform=transforms,\n",
    "                hypothesis_transform=transforms,\n",
    "            )\n",
    "print(f\"Word Error Rate (WER) :\", wer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/b0990106x/miniconda3/envs/trl/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'DeepNoiseSuppressionMeanOpinionScore' from 'torchmetrics.audio' (/home/b0990106x/miniconda3/envs/trl/lib/python3.10/site-packages/torchmetrics/audio/__init__.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m randn\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorchmetrics\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01maudio\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m DeepNoiseSuppressionMeanOpinionScore\n\u001b[1;32m      3\u001b[0m g \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mmanual_seed(\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m      4\u001b[0m preds \u001b[38;5;241m=\u001b[39m randn(\u001b[38;5;241m8000\u001b[39m)\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'DeepNoiseSuppressionMeanOpinionScore' from 'torchmetrics.audio' (/home/b0990106x/miniconda3/envs/trl/lib/python3.10/site-packages/torchmetrics/audio/__init__.py)"
     ]
    }
   ],
   "source": [
    "from torch import randn\n",
    "from torchmetrics.audio import DeepNoiseSuppressionMeanOpinionScore\n",
    "g = torch.manual_seed(1)\n",
    "preds = randn(8000)\n",
    "dnsmos = DeepNoiseSuppressionMeanOpinionScore(8000, False)\n",
    "dnsmos(preds)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "trl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
