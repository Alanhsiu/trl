{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_output(\n",
    "        ar_model, \n",
    "        nar_model, \n",
    "        ar_tokenizer, \n",
    "        nar_tokenizer, \n",
    "        src_encodec: list, \n",
    "        instruction: list, \n",
    "        args_predict: SimpleNamespace, \n",
    "        episode_counter: int = 0, \n",
    "        base_path: str = \"/work/b0990106x/trl\", \n",
    "        temperature: float = 1.0\n",
    ") -> tuple[float, str]:\n",
    "    '''\n",
    "    Generates output from AR model, synthesize the audio, and evaluate the audio using NISQA.\n",
    "    Returns:\n",
    "        tuple:\n",
    "            reward(float): The reward of the audio.\n",
    "            tokenized_decode_ar(str): The tokenized output of the AR model - first layer.\n",
    "    '''\n",
    "    # Generate predictions using the AR model\n",
    "    decode_ar, wav = get_ar_prediction_get_audio(\n",
    "        args_predict, ar_model, nar_model, ar_tokenizer, nar_tokenizer, src_encodec, instruction, episode_counter, temperature=temperature\n",
    "    )\n",
    "    # extract the instruction from the list \n",
    "\n",
    "    tensor_wav = convert_array_to_tensor_format(wav)\n",
    "    if tensor_wav[0].shape[0]==1:\n",
    "        tensor_wav[0] = tensor_wav[0].squeeze(0)\n",
    "\n",
    "    reward = get_reward_claps(prompts = instruction, wavs = tensor_wav)\n",
    "    \n",
    "    list_decode_ar = decode_ar.flatten().tolist()   \n",
    "    filtered_decode_ar_list = list_decode_ar[2:-1]\n",
    "    decode_ar_tokens = ar_tokenizer.convert_ids_to_tokens(filtered_decode_ar_list)\n",
    "    tokenized_decode_ar = ar_tokenizer.convert_tokens_to_string(decode_ar_tokens)\n",
    "    print(f\"REWARD: {reward}\")\n",
    "\n",
    "    return reward, tokenized_decode_ar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_data(sample_size: int, \n",
    "                 ar_model, \n",
    "                 nar_model, \n",
    "                 ar_tokenizer, \n",
    "                 nar_tokenizer, \n",
    "                 all_src_encodec: List[list], \n",
    "                 all_instruction: List[str],\n",
    "                 args_predict: SimpleNamespace, \n",
    "                 base_path: str = \"/work/b0990106x/trl\", \n",
    "                 temperature: float = 1.0, \n",
    "                 iteration: int = 0\n",
    ") -> Tuple[List[str], List[str], List[str], List[float], List[float], List[float]]:\n",
    "    \"\"\"\n",
    "    Process data to generate outputs, calculate rewards, and organize chosen and rejected data.\n",
    "    Returns:\n",
    "        tuple:\n",
    "            chosen (List[str]): A list of chosen outputs.\n",
    "            rejected (List[str]): A list of rejected outputs.\n",
    "            prompts (List[str]): A list of prompts.\n",
    "            chosen_rewards (List[float]): A list of rewards for the chosen outputs.\n",
    "            rejected_rewards (List[float]): A list of rewards for the rejected outputs.\n",
    "            average_rewards (List[float]): A list of average rewards.\n",
    "    \"\"\"\n",
    "    # If sample size is 1, we cannot choose the best and worst outputs\n",
    "    if sample_size < 2:\n",
    "        raise ValueError(\"Parameter 'sample_size' must be greater than 1.\")\n",
    "\n",
    "    chosen, rejected, prompts, chosen_rewards, rejected_rewards, average_rewards = [], [], [], [], [], []\n",
    "\n",
    "    for i in tqdm(range(len(all_src_encodec)), desc=\"Processing Data\"):\n",
    "        rewards, tokenized_outputs = [], []\n",
    "\n",
    "        for j in tqdm(range(sample_size), desc=\"Processing Samples\"):\n",
    "            size_of_packed_input = (\n",
    "                len(all_src_encodec[i][0]) +\n",
    "                len(ar_tokenizer(all_instruction[i])[\"input_ids\"][1:-1]) +\n",
    "                3\n",
    "            )\n",
    "            if 4 < size_of_packed_input <= 1024:\n",
    "                set_seed(42+iteration+j)\n",
    "                reward, tokenized_decode_ar = generate_output(\n",
    "                    ar_model=ar_model, \n",
    "                    nar_model=nar_model, \n",
    "                    ar_tokenizer=ar_tokenizer, \n",
    "                    nar_tokenizer=nar_tokenizer,\n",
    "                    src_encodec = all_src_encodec[i],\n",
    "                    instruction=all_instruction[i], \n",
    "                    args_predict=args_predict,\n",
    "                    episode_counter=f\"data_{i}_episode_{j}\",\n",
    "                    base_path=base_path, \n",
    "                    temperature=temperature\n",
    "                )\n",
    "                rewards.append(reward)\n",
    "                tokenized_outputs.append(tokenized_decode_ar)\n",
    "\n",
    "\n",
    "        valid_rewards = [r for r in rewards if r is not None]\n",
    "        valid_outputs = [tokenized_outputs[j] for j in range(len(rewards)) if rewards[j] is not None]\n",
    "\n",
    "        if len(valid_rewards) >= 2:\n",
    "            max_reward_index = np.argmax(valid_rewards)\n",
    "            min_reward_index = np.argmin(valid_rewards)\n",
    "            average_reward = np.mean(valid_rewards)\n",
    "            chosen_output = valid_outputs[max_reward_index]\n",
    "            rejected_output = valid_outputs[min_reward_index]\n",
    "\n",
    "            obs_input = pack_inputs_v2(ar_tokenizer, all_src_encodec[i], all_instruction[i])\n",
    "            tokenize_input = ar_tokenizer.convert_ids_to_tokens(obs_input)\n",
    "            tokenize_input_str = ar_tokenizer.convert_tokens_to_string(tokenize_input)\n",
    "            prompts.append(tokenize_input_str)\n",
    "\n",
    "            chosen.append(chosen_output)\n",
    "            chosen_rewards.append(valid_rewards[max_reward_index])\n",
    "            rejected.append(rejected_output)\n",
    "            rejected_rewards.append(valid_rewards[min_reward_index])\n",
    "            average_rewards.append(average_reward)\n",
    "        else:\n",
    "            print(f\"Not enough valid rewards for data index {i}\")\n",
    "\n",
    "    # If there is only one data, we need to double the data because we need it for training set and validation set\n",
    "    if len(all_src_encodec) == 1:\n",
    "        chosen *= 2\n",
    "        rejected *= 2\n",
    "        prompts *= 2\n",
    "        chosen_rewards *= 2\n",
    "        rejected_rewards *= 2\n",
    "        average_rewards *= 2    \n",
    "    \n",
    "    return chosen, rejected, prompts, chosen_rewards, rejected_rewards, average_rewards\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
