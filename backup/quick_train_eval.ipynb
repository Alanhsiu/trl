{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quick Train - Continue from a checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'transformer'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformer\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m AutoTokenizer\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtrl\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m DPOTrainer, DPOConfig, AutoModelForSeq2SeqLMWithValueHead, create_reference_model\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'transformer'"
     ]
    }
   ],
   "source": [
    "from transformers import BartForConditionalGeneration, AutoModelForCausalLM, AutoTokenizer\n",
    "from trl import DPOTrainer, DPOConfig, AutoModelForSeq2SeqLMWithValueHead, create_reference_model\n",
    "from datasets import Dataset\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(\n",
    "        model,\n",
    "        model_ref,\n",
    "        ar_tokenizer,\n",
    "        train_dataset: Dataset,\n",
    "        val_dataset: Dataset,\n",
    "        model_output_dir: str,\n",
    "        beta: float,\n",
    "        resume_from_checkpoint: bool,\n",
    "        model_checkpoint: str,\n",
    "        learning_rate: float = 5e-07,\n",
    "        num_train_epochs: int = 100,\n",
    "        max_length: int = 1024*9,\n",
    "        max_prompt_length: int = 1024*9,\n",
    "        max_target_length: int = 1024*9,\n",
    "        per_device_train_batch_size: int = 1,\n",
    "        gradient_accumulation_steps: int = 1,\n",
    "        seed: int = 42\n",
    ") -> None:\n",
    "    '''\n",
    "    Train the DPO model and save the model.\n",
    "\n",
    "    Args:\n",
    "        model(AutoModelForSeq2SeqLMWithValueHead): The DPO model.\n",
    "        model_ref(AutoModelForCausalLM): The reference model.\n",
    "        ar_tokenizer(AutoTokenizer): The tokenizer.\n",
    "        train_dataset(Dataset): The training dataset.\n",
    "        val_dataset(Dataset): The validation dataset.\n",
    "        model_output_dir(str): The output directory for the model.\n",
    "        beta(float): The beta value.\n",
    "        resume_from_checkpoint(bool): Whether to resume from a checkpoint.\n",
    "        model_checkpoint(str): The path to the model\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    '''\n",
    "\n",
    "    training_args = DPOConfig(\n",
    "        beta = beta,\n",
    "        output_dir = model_output_dir,\n",
    "        generate_during_eval = True,\n",
    "        resume_from_checkpoint = model_checkpoint if resume_from_checkpoint else None,\n",
    "        seed = seed,\n",
    "        per_device_train_batch_size = per_device_train_batch_size,\n",
    "        num_train_epochs = num_train_epochs,\n",
    "        gradient_accumulation_steps = gradient_accumulation_steps,\n",
    "        learning_rate = learning_rate,\n",
    "        max_length = max_length,\n",
    "        max_prompt_length = max_prompt_length,\n",
    "        max_target_length = max_target_length\n",
    "    )\n",
    "    \n",
    "    trainer = DPOTrainer(\n",
    "        model=model,\n",
    "        ref_model=model_ref,\n",
    "        args=training_args,\n",
    "        tokenizer=ar_tokenizer,\n",
    "        train_dataset=train_dataset,\n",
    "        eval_dataset=val_dataset,\n",
    "    )\n",
    "    # Train the model\n",
    "    trainer.train()\n",
    "\n",
    "    # Save the model\n",
    "    trainer.save_model(f\"{model_output_dir}/dpo_model\")\n",
    "    model.config.to_json_file(f\"{model_output_dir}/dpo_model/config.json\")\n",
    "    ar_tokenizer.save_pretrained(f\"{model_output_dir}/dpo_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Base\n",
    "base_path = \"/work/b0990106x/trl\"\n",
    "ts = \"0718-1514\"\n",
    "iter_name = \"iter_0\"\n",
    "output_name = f\"output_trained_{ts}_{iter_name}\"\n",
    "\n",
    "# Model Input and Output Directories\n",
    "model_input_dir = f\"{base_path}/model_output/{ts}/{iter_name}\" # Location to load the model\n",
    "model_output_dir = f\"{base_path}/model_output/{ts}/{output_name}\" # Location to save the new trained model\n",
    "agent_output_dir = f\"{base_path}/output/{ts}\" # Path of saving the generated audio for reward model to evaluate\n",
    "ar_checkpoint = \"lca0503/speech-chatgpt-base-ar-v2-epoch10-wotrans\"\n",
    "model_checkpoint = f\"{model_input_dir}/dpo_model\"\n",
    "\n",
    "# Training Parameters\n",
    "beta = 0.1 # Training: beta value for DPO\n",
    "learning_rate = 5e-07 # Training: learning rate\n",
    "num_train_epochs = 100 # Training: number of training epochs\n",
    "max_length = 1024*9 # Training: max length of the model\n",
    "max_prompt_length = 1024*9 # Training: max length of the prompt\n",
    "max_target_length = 1024*9 # Training: max length of the target\n",
    "per_device_train_batch_size = 1 # Training: batch size\n",
    "gradient_accumulation_steps = 1 # Training: gradient accumulation steps\n",
    "seed = 42 # Training: seed\n",
    "\n",
    "with open(f\"{agent_output_dir}/{iter_name}.json\", \"r\") as f: # Change this file for different datasets\n",
    "    data = json.load(f)\n",
    "\n",
    "data_for_dataset = {key: data[key] for key in [\"prompt\", \"chosen\", \"rejected\"]}\n",
    "dataset = Dataset.from_dict(data_for_dataset)\n",
    "dataset_dict = dataset.train_test_split(test_size=0.1)\n",
    "train_dataset = dataset_dict[\"train\"]\n",
    "val_dataset = dataset_dict[\"test\"]\n",
    "\n",
    "ar_tokenizer = AutoTokenizer.from_pretrained(ar_checkpoint)\n",
    "ar_tokenizer.pad_token = ar_tokenizer.eos_token\n",
    "model = AutoModelForSeq2SeqLMWithValueHead.from_pretrained(model_checkpoint, return_dict=True)\n",
    "model_ref = create_reference_model(model)\n",
    "\n",
    "resume_from_checkpoint = True\n",
    "\n",
    "train_model(model=model,\n",
    "            model_ref=model_ref,\n",
    "            ar_tokenizer=ar_tokenizer,\n",
    "            train_dataset=train_dataset,\n",
    "            val_dataset=val_dataset,\n",
    "            model_output_dir=model_output_dir,\n",
    "            beta=beta,\n",
    "            resume_from_checkpoint=resume_from_checkpoint,\n",
    "            model_checkpoint=model_checkpoint,\n",
    "            learning_rate=learning_rate,\n",
    "            num_train_epochs=num_train_epochs,\n",
    "            max_length=max_length,\n",
    "            max_prompt_length=max_prompt_length,\n",
    "            max_target_length=max_target_length,\n",
    "            per_device_train_batch_size=per_device_train_batch_size,\n",
    "            gradient_accumulation_steps=gradient_accumulation_steps,\n",
    "            seed=seed\n",
    "            )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Quick Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import json\n",
    "from dpo_eval import eval_dpo_mos\n",
    "from typing import List, Tuple\n",
    "from types import SimpleNamespace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_data_from_json(file_path: str) -> Tuple[List[list], List[str], List[list]]:\n",
    "    \"\"\"\n",
    "    Loads data from a JSON file and extracts 'src_encodec', 'instruction', and 'tgt_encodec'.\n",
    "\n",
    "    Args:\n",
    "        file_path (str): The path to the JSON file.\n",
    "\n",
    "    Returns:\n",
    "        tuple:\n",
    "            all_src_encodec (List[list]): A list containing the 'src_encodec' data from each item in the JSON file.\n",
    "            all_instruction (List[str]): A list containing the 'instruction' data from each item in the JSON file.\n",
    "            all_tgt_encodec (List[list]): A list containing the 'tgt_encodec' data from each item in the JSON file.\n",
    "    \"\"\"\n",
    "    with open(file_path, 'r') as f:\n",
    "        data = json.load(f)\n",
    "\n",
    "    all_src_encodec = [item[\"src_encodec\"] for item in data]\n",
    "    all_instruction = [item[\"instruction\"] for item in data]\n",
    "    all_tgt_encodec = [item[\"tgt_encodec\"] for item in data]\n",
    "\n",
    "    return all_src_encodec, all_instruction, all_tgt_encodec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Set Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Base\n",
    "base_path = \"/work/b0990106x/trl\"\n",
    "ts = \"0718-1514\"\n",
    "ar_checkpoint = \"lca0503/speech-chatgpt-base-ar-v2-epoch10-wotrans\"\n",
    "nar_checkpoint = \"lca0503/speech-chatgpt-base-nar-v2-epoch4-wotrans\"\n",
    "\n",
    "# Model Configs\n",
    "iter_name = \"iter_0\"\n",
    "model_checkpoint = f\"{base_path}/model_output/{ts}/{iter_name}/dpo_model\"\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "all_src_encodec, all_instruction, all_tgt_encodec = extract_data_from_json('dpo_data/src_encodec.json')\n",
    "args_predict = SimpleNamespace(output_path=f\"{base_path}/output/{ts}/example.wav\", seed=0, device=device)\n",
    "\n",
    "# Evaluation Parameters\n",
    "eval_data_len = 10\n",
    "eval_indices = [0] # Evaluation: evaluate on data indicies from all_src_encodec\n",
    "eval_data_len = 1 # Evaluation: evaluate how many data\n",
    "num_evaluations = 1 # Evaluation: evaluate how many times per data\n",
    "iteration = \"eval_1\" # Evaluation: This is just for the name of the output audio file\n",
    "\n",
    "# Evaluation\n",
    "trained_model_metrics, trained_model_rewards = eval_dpo_mos(ar_checkpoint=ar_checkpoint,\n",
    "                                                            nar_checkpoint=nar_checkpoint,\n",
    "                                                            trained_model_checkpoint=model_checkpoint,\n",
    "                                                            all_src_encodec=all_src_encodec,\n",
    "                                                            all_instruction=all_instruction,\n",
    "                                                            eval_data_len=eval_data_len,\n",
    "                                                            selected_indices=eval_indices,\n",
    "                                                            device=device,\n",
    "                                                            iteration = iteration,\n",
    "                                                            args_predict=args_predict,\n",
    "                                                            num_evaluations=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Output metrics and rewards into a file\n",
    "output_path = f\"{base_path}/output/{ts}/eval_{iter_name}_{eval_data_len}.json\"\n",
    "with open(output_path, 'w') as f:\n",
    "    json.dump({\"metrics\": trained_model_metrics, \"rewards\": trained_model_rewards}, f)\n",
    "    print(f\"Metrics and rewards have been saved to {output_path}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "trl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
