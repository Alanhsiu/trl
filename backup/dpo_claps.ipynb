{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"/work/b0990106x/trl/vc\")\n",
    "sys.path.append('/work/b0990106x/trl/CLAPS')\n",
    "\n",
    "import importlib\n",
    "import torch\n",
    "import os\n",
    "import math\n",
    "import numpy as np\n",
    "import random\n",
    "import time\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "from types import SimpleNamespace\n",
    "from datetime import datetime\n",
    "from typing import List, Tuple\n",
    "\n",
    "import vc\n",
    "importlib.reload(vc)\n",
    "from vc.trainer_encodec_vc_inference import (\n",
    "    pack_inputs_v2,\n",
    "    get_ar_prediction_get_audio,\n",
    "    get_ar_prediction_audio_batch\n",
    ")\n",
    "from vc.encodec_model.nar_bart_model import NARBartForConditionalGeneration\n",
    "\n",
    "from transformers import BartForConditionalGeneration, AutoTokenizer\n",
    "from trl import (\n",
    "    DPOTrainer,\n",
    "    DPOConfig,\n",
    "    AutoModelForSeq2SeqLMWithValueHead,\n",
    "    create_reference_model\n",
    ")\n",
    "from datasets import Dataset\n",
    "\n",
    "from dpo_eval import (\n",
    "    get_reward_claps,\n",
    "    eval_dpo_claps_batch,\n",
    "    convert_array_to_tensor_format\n",
    ")\n",
    "from CLAPS.inference import load_model\n",
    "\n",
    "import argparse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Utility Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seed(seed):\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(seed)\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "def generate_output_batch(\n",
    "        ar_model, \n",
    "        nar_model, \n",
    "        ar_tokenizer, \n",
    "        nar_tokenizer, \n",
    "        clap_model,\n",
    "        accelerator,\n",
    "        src_encodec: list, \n",
    "        instruction: list, \n",
    "        args_predict: SimpleNamespace, \n",
    "        episode_counter: int = 0, \n",
    "        base_path: str = \"/work/b0990106x/trl\", \n",
    "        temperature: float = 1.0\n",
    ") -> tuple[float, str]:\n",
    "    audio_list, decode_ar_list = get_ar_prediction_audio_batch(\n",
    "        args_predict, ar_model, nar_model, ar_tokenizer, nar_tokenizer, src_encodec, instruction, episode_counter, temperature=temperature\n",
    "    )\n",
    "    reward_list, tokenized_decode_ar_list = [], []\n",
    "\n",
    "    for i, audio in enumerate(audio_list): \n",
    "        if audio is not None:\n",
    "            tensor_audio = convert_array_to_tensor_format(audio)\n",
    "            if tensor_audio[0].shape[0] == 1:\n",
    "                tensor_audio[0] = tensor_audio[0].squeeze(0)\n",
    "            reward = get_reward_claps(clap_model=clap_model, accelerator=accelerator, prompts=instruction[i], wavs=tensor_audio)\n",
    "        else: \n",
    "            reward = 0\n",
    "        reward_list.append(reward)\n",
    "    \n",
    "    for decode_ar in decode_ar_list:\n",
    "        list_decode_ar = decode_ar.flatten().tolist()   \n",
    "        filtered_decode_ar_list = list_decode_ar[2:-1]\n",
    "        decode_ar_tokens = ar_tokenizer.convert_ids_to_tokens(filtered_decode_ar_list)\n",
    "        tokenized_decode_ar = ar_tokenizer.convert_tokens_to_string(decode_ar_tokens)\n",
    "        tokenized_decode_ar_list.append(tokenized_decode_ar)\n",
    "        \n",
    "    return reward_list, tokenized_decode_ar_list\n",
    "\n",
    "def extract_data_from_json(file_path: str) -> Tuple[List[list], List[str], List[list]]:\n",
    "    with open(file_path, 'r') as f:\n",
    "        data = json.load(f)\n",
    "\n",
    "    all_src_encodec = [item[\"src_encodec\"] for item in data]\n",
    "    all_instruction = [item[\"instruction\"] for item in data]\n",
    "\n",
    "    return all_src_encodec, all_instruction\n",
    "\n",
    "def train_model(\n",
    "        model,\n",
    "        model_ref,\n",
    "        ar_tokenizer,\n",
    "        train_dataset: Dataset,\n",
    "        val_dataset: Dataset,\n",
    "        model_output_dir: str,\n",
    "        beta: float,\n",
    "        resume_from_checkpoint: bool,\n",
    "        model_checkpoint: str,\n",
    "        learning_rate: float = 5e-07,\n",
    "        num_train_epochs: int = 200,\n",
    "        max_length: int = 1024*9,\n",
    "        max_prompt_length: int = 1024*9,\n",
    "        max_target_length: int = 1024*9,\n",
    "        per_device_train_batch_size: int = 1,\n",
    "        gradient_accumulation_steps: int = 1,\n",
    "        seed: int = 42\n",
    ") -> None:\n",
    "    training_args = DPOConfig(\n",
    "        beta=beta,\n",
    "        output_dir=model_output_dir,\n",
    "        resume_from_checkpoint=model_checkpoint if resume_from_checkpoint else None,\n",
    "        seed=seed,\n",
    "        per_device_train_batch_size=per_device_train_batch_size,\n",
    "        num_train_epochs=num_train_epochs,\n",
    "        gradient_accumulation_steps=gradient_accumulation_steps,\n",
    "        learning_rate=learning_rate,\n",
    "        max_length=max_length,\n",
    "        max_prompt_length=max_prompt_length,\n",
    "        max_target_length=max_target_length,\n",
    "        evaluation_strategy=\"steps\",\n",
    "        save_steps=5000,\n",
    "        logging_dir=f\"{model_output_dir}/logs\"\n",
    "    )\n",
    "    \n",
    "    trainer = DPOTrainer(\n",
    "        model=model,\n",
    "        ref_model=model_ref,\n",
    "        args=training_args,\n",
    "        tokenizer=ar_tokenizer,\n",
    "        train_dataset=train_dataset,\n",
    "        eval_dataset=val_dataset,\n",
    "    )\n",
    "    trainer.train()\n",
    "\n",
    "    model.config.to_json_file(f\"{model_output_dir}/config.json\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "def process_data_batch(\n",
    "    sample_size: int, \n",
    "    ar_model, \n",
    "    nar_model, \n",
    "    ar_tokenizer, \n",
    "    nar_tokenizer, \n",
    "    clap_model,\n",
    "    accelerator,\n",
    "    selected_src_encodec: List[list], \n",
    "    selected_instruction: List[str],\n",
    "    args_predict, \n",
    "    base_path: str = \"/work/b0990106x/trl\", \n",
    "    temperature: float = 1.0, \n",
    "    iteration: int = 0,\n",
    "    prev_eval_avg: float = 0,\n",
    "    strategy: str = \"above_below_average\"  # Default to original strategy. Options: \"max_min\", \"top_bottom_percent\", \"above_below_average\", \"above_prev_eval\"\n",
    ") -> Tuple[List[str], List[str], List[str], List[float], List[float], List[float]]:\n",
    "    # Ensure sample size is valid\n",
    "    if sample_size < 2:\n",
    "        raise ValueError(\"Parameter 'sample_size' must be greater than 1.\")\n",
    "\n",
    "    chosen, rejected, prompts, chosen_rewards, rejected_rewards, average_rewards = [], [], [], [], [], []\n",
    "\n",
    "    disable_tqdm = not os.isatty(1)\n",
    "    for i in tqdm(range(len(selected_src_encodec)), desc=\"Processing Data\", disable=disable_tqdm):\n",
    "        rewards, tokenized_outputs = [], []\n",
    "        size_of_packed_input = (\n",
    "            len(selected_src_encodec[i][0]) +\n",
    "            len(ar_tokenizer(selected_instruction[i])[\"input_ids\"][1:-1]) +\n",
    "            3\n",
    "        )\n",
    "        if 4 < size_of_packed_input <= 1024:\n",
    "            selected_src_encodec_list = [selected_src_encodec[i]] * sample_size\n",
    "            selected_instruction_list = [selected_instruction[i]] * sample_size\n",
    "            rewards, tokenized_outputs = generate_output_batch(\n",
    "                ar_model=ar_model, \n",
    "                nar_model=nar_model, \n",
    "                ar_tokenizer=ar_tokenizer, \n",
    "                nar_tokenizer=nar_tokenizer,\n",
    "                src_encodec=selected_src_encodec_list,\n",
    "                instruction=selected_instruction_list, \n",
    "                clap_model=clap_model,\n",
    "                accelerator=accelerator,\n",
    "                args_predict=args_predict,\n",
    "                episode_counter=f\"data_{i}\",\n",
    "                base_path=base_path, \n",
    "                temperature=temperature\n",
    "            )\n",
    "\n",
    "        valid_rewards = [r for r in rewards if r is not None]\n",
    "        valid_outputs = [tokenized_outputs[j] for j in range(len(rewards)) if rewards[j] is not None]\n",
    "\n",
    "        if len(valid_rewards) >= 2:\n",
    "            average_reward = np.mean(valid_rewards)\n",
    "            print(f\"Average reward for data index {i}: {average_reward}\")\n",
    "\n",
    "            if strategy == \"max_min\":\n",
    "                # Original max-min strategy\n",
    "                max_reward_index = np.argmax(valid_rewards)\n",
    "                min_reward_index = np.argmin(valid_rewards)\n",
    "                chosen_outputs = [valid_outputs[max_reward_index]]\n",
    "                rejected_outputs = [valid_outputs[min_reward_index]]\n",
    "                chosen_rewards_part = [valid_rewards[max_reward_index]]\n",
    "                rejected_rewards_part = [valid_rewards[min_reward_index]]\n",
    "\n",
    "            elif strategy == \"top_bottom_percent\":\n",
    "                # Select top and bottom 20% of rewards\n",
    "                twenty_percent_num = max(1, math.ceil(len(valid_rewards) * 0.2))\n",
    "                max_indices = np.argsort(valid_rewards)[-twenty_percent_num:]\n",
    "                min_indices = np.argsort(valid_rewards)[:twenty_percent_num]\n",
    "\n",
    "                chosen_outputs = [valid_outputs[j] for j in max_indices]\n",
    "                rejected_outputs = [valid_outputs[j] for j in min_indices]\n",
    "                chosen_rewards_part = [valid_rewards[j] for j in max_indices]\n",
    "                rejected_rewards_part = [valid_rewards[j] for j in min_indices]\n",
    "\n",
    "            elif strategy == \"above_below_average\":\n",
    "                # Select rewards above and below the average\n",
    "                threshold = 0.05\n",
    "                chosen_outputs = [valid_outputs[j] for j in range(len(valid_rewards)) if valid_rewards[j] > average_reward + threshold]\n",
    "                rejected_outputs = [valid_outputs[j] for j in range(len(valid_rewards)) if valid_rewards[j] < average_reward - threshold]\n",
    "\n",
    "                # Sort and trim to ensure balanced chosen and rejected outputs\n",
    "                chosen_outputs = [x for _, x in sorted(zip(valid_rewards, chosen_outputs), reverse=True)]\n",
    "                rejected_outputs = [x for _, x in sorted(zip(valid_rewards, rejected_outputs))]\n",
    "\n",
    "                min_length = min(len(chosen_outputs), len(rejected_outputs))\n",
    "                chosen_outputs = chosen_outputs[:min_length]\n",
    "                rejected_outputs = rejected_outputs[:min_length]\n",
    "\n",
    "                chosen_rewards_part = [valid_rewards[j] for j in range(len(valid_rewards)) if valid_rewards[j] > average_reward][:min_length]\n",
    "                rejected_rewards_part = [valid_rewards[j] for j in range(len(valid_rewards)) if valid_rewards[j] < average_reward][:min_length]\n",
    "\n",
    "            elif strategy == \"above_prev_eval\":\n",
    "                # Select rewards above and below a previous evaluation average\n",
    "                chosen_outputs = [valid_outputs[j] for j in range(len(valid_rewards)) if valid_rewards[j] > prev_eval_avg]\n",
    "                rejected_outputs = [valid_outputs[j] for j in range(len(valid_rewards)) if valid_rewards[j] < prev_eval_avg]\n",
    "\n",
    "                # Sort and trim\n",
    "                chosen_outputs = [x for _, x in sorted(zip(valid_rewards, chosen_outputs), reverse=True)]\n",
    "                rejected_outputs = [x for _, x in sorted(zip(valid_rewards, rejected_outputs))]\n",
    "\n",
    "                min_length = min(len(chosen_outputs), len(rejected_outputs))\n",
    "                if min_length == 0:\n",
    "                    chosen_outputs = [valid_outputs[np.argmax(valid_rewards)]]\n",
    "                    rejected_outputs = [valid_outputs[np.argmin(valid_rewards)]]\n",
    "                    chosen_rewards_part = [valid_rewards[np.argmax(valid_rewards)]]\n",
    "                    rejected_rewards_part = [valid_rewards[np.argmin(valid_rewards)]]\n",
    "                else:\n",
    "                    chosen_outputs = chosen_outputs[:min_length]\n",
    "                    rejected_outputs = rejected_outputs[:min_length]\n",
    "                    chosen_rewards_part = [valid_rewards[j] for j in range(len(valid_rewards)) if valid_rewards[j] > prev_eval_avg][:min_length]\n",
    "                    rejected_rewards_part = [valid_rewards[j] for j in range(len(valid_rewards)) if valid_rewards[j] < prev_eval_avg][:min_length]\n",
    "\n",
    "            obs_input = pack_inputs_v2(ar_tokenizer, selected_src_encodec[i], selected_instruction[i])\n",
    "            tokenize_input = ar_tokenizer.convert_ids_to_tokens(obs_input)\n",
    "            tokenize_input_str = ar_tokenizer.convert_tokens_to_string(tokenize_input)\n",
    "            prompts.extend([tokenize_input_str] * len(chosen_outputs))\n",
    "            average_rewards.append(average_reward)\n",
    "\n",
    "            chosen.extend(chosen_outputs)\n",
    "            rejected.extend(rejected_outputs)\n",
    "            chosen_rewards.extend(chosen_rewards_part)\n",
    "            rejected_rewards.extend(rejected_rewards_part)\n",
    "        else:\n",
    "            print(f\"Not enough valid rewards for data index {i}\")\n",
    "\n",
    "    if len(selected_src_encodec) == 1:\n",
    "        chosen *= 2\n",
    "        rejected *= 2\n",
    "        prompts *= 2\n",
    "        chosen_rewards *= 2\n",
    "        rejected_rewards *= 2\n",
    "        average_rewards *= 2    \n",
    "\n",
    "    return chosen, rejected, prompts, chosen_rewards, rejected_rewards, average_rewards\n",
    "\n",
    "\n",
    "def generate_data(ar_model, \n",
    "                  ar_tokenizer, \n",
    "                  nar_model, \n",
    "                  nar_tokenizer, \n",
    "                  clap_model,\n",
    "                  accelerator,\n",
    "                  selected_src_encodec: List[list], \n",
    "                  selected_instruction: List[str],\n",
    "                  args_predict: SimpleNamespace, \n",
    "                  sample_size: int, \n",
    "                  iteration: int, \n",
    "                  agent_output_dir: str, \n",
    "                  base_path: str = \"/work/b0990106x/trl\", \n",
    "                  temperature: float = 1.0\n",
    ") -> Tuple[dict, List[float], List[float]]:\n",
    "    \"\"\"\n",
    "    Generates data for the dataset and saves info to a JSON file.\n",
    "    Returns:\n",
    "        tuple:\n",
    "            data_for_dataset (dict): A dictionary containing the data for the dataset.\n",
    "            chosen_rewards (List[float]): A list of rewards for the chosen outputs.\n",
    "            rejected_rewards (List[float]): A list of rewards for the rejected outputs.\n",
    "    \"\"\"\n",
    "    chosen, rejected, prompts, chosen_rewards, rejected_rewards, average_rewards = process_data_batch(\n",
    "        sample_size=sample_size,\n",
    "        ar_model=ar_model,\n",
    "        nar_model=nar_model,\n",
    "        ar_tokenizer=ar_tokenizer,\n",
    "        nar_tokenizer=nar_tokenizer,\n",
    "        selected_src_encodec=selected_src_encodec,\n",
    "        selected_instruction=selected_instruction,\n",
    "        args_predict=args_predict,\n",
    "        base_path=base_path,\n",
    "        temperature=temperature,\n",
    "        iteration = iteration,\n",
    "        clap_model=clap_model,\n",
    "        accelerator=accelerator\n",
    "    )\n",
    "\n",
    "    data = {\n",
    "        \"prompt\": prompts,\n",
    "        \"chosen\": chosen,\n",
    "        \"rejected\": rejected,\n",
    "        \"chosen_rewards\": chosen_rewards,\n",
    "        \"rejected_rewards\": rejected_rewards,\n",
    "        \"average_rewards\": average_rewards\n",
    "    }\n",
    "\n",
    "    with open(f\"{agent_output_dir}/data_iter_{iteration}.json\", \"w\") as outfile:\n",
    "        json.dump(data, outfile, indent=4)\n",
    "\n",
    "    data_for_dataset = {key: data[key] for key in [\"prompt\", \"chosen\", \"rejected\"]}\n",
    "\n",
    "    return data_for_dataset, chosen_rewards, rejected_rewards\n",
    "\n",
    "def train_iteration(model, \n",
    "                    model_checkpoint,\n",
    "                    iteration, \n",
    "                    data_size, \n",
    "                    sample_size, \n",
    "                    ar_model, \n",
    "                    ar_tokenizer,\n",
    "                    nar_model, \n",
    "                    nar_tokenizer,\n",
    "                    all_src_encodec, \n",
    "                    all_instruction, \n",
    "                    args_predict, \n",
    "                    agent_output_dir,\n",
    "                    model_output_dir_base, \n",
    "                    clap_model,\n",
    "                    accelerator,\n",
    "                    beta = 0.1, \n",
    "                    temperature = 1.0,\n",
    "                    base_path=\"/work/b0990106x/trl\",\n",
    "                    resume_from_checkpoint = False,\n",
    "                    learning_rate = 5e-07,\n",
    "                    num_train_epochs = 100,\n",
    "                    max_length = 1024*9,\n",
    "                    max_prompt_length = 1024*9,\n",
    "                    max_target_length = 1024*9,\n",
    "                    per_device_train_batch_size = 1,\n",
    "                    gradient_accumulation_steps = 1,\n",
    "                    seed = 42,\n",
    "):\n",
    "    \"\"\"\n",
    "    Executes one training iteration: generates data, trains the model, and saves the output.\n",
    "    \"\"\"\n",
    "    # print(f\"Iteration {iteration}\")\n",
    "\n",
    "    # ar_model = BartForConditionalGeneration.from_pretrained(model_checkpoint)\n",
    "    # ar_tokenizer = AutoTokenizer.from_pretrained(ar_checkpoint)\n",
    "    # ar_tokenizer.pad_token = ar_tokenizer.eos_token\n",
    "    # nar_model = NARBartForConditionalGeneration.from_pretrained(nar_checkpoint)\n",
    "    # nar_tokenizer = AutoTokenizer.from_pretrained(nar_checkpoint)\n",
    "\n",
    "    selected_src_encodec = all_src_encodec[:data_size]\n",
    "    selected_instruction = all_instruction[:data_size]\n",
    "\n",
    "    data_for_dataset, chosen_rewards, rejected_rewards = generate_data(ar_model=model,\n",
    "                                                                        ar_tokenizer=ar_tokenizer,\n",
    "                                                                        nar_model=nar_model,\n",
    "                                                                        nar_tokenizer=nar_tokenizer,\n",
    "                                                                        selected_src_encodec=selected_src_encodec,\n",
    "                                                                        selected_instruction=selected_instruction,\n",
    "                                                                        args_predict=args_predict,\n",
    "                                                                        sample_size=sample_size,\n",
    "                                                                        iteration=iteration,\n",
    "                                                                        agent_output_dir=agent_output_dir,\n",
    "                                                                        base_path=base_path,\n",
    "                                                                        temperature=temperature,\n",
    "                                                                        clap_model=clap_model,\n",
    "                                                                        accelerator=accelerator)\n",
    "\n",
    "    dataset = Dataset.from_dict(data_for_dataset)\n",
    "    dataset_dict = dataset.train_test_split(test_size=0.1, shuffle=True, seed=seed)\n",
    "    train_dataset = dataset_dict[\"train\"]\n",
    "    val_dataset = dataset_dict[\"test\"]\n",
    "\n",
    "    model_output_dir = f\"{model_output_dir_base}/iter_{iteration}\"\n",
    "    os.makedirs(model_output_dir, exist_ok=True)\n",
    "\n",
    "    # model = AutoModelForSeq2SeqLMWithValueHead.from_pretrained(model_checkpoint, return_dict=True)\n",
    "    model_ref = create_reference_model(model)\n",
    "    \n",
    "    train_model(model=model,\n",
    "                model_ref=model_ref,\n",
    "                ar_tokenizer=ar_tokenizer,\n",
    "                train_dataset=train_dataset,\n",
    "                val_dataset=val_dataset,\n",
    "                model_output_dir=model_output_dir,\n",
    "                beta=beta,\n",
    "                resume_from_checkpoint=resume_from_checkpoint,\n",
    "                model_checkpoint=model_checkpoint,\n",
    "                learning_rate = learning_rate,\n",
    "                num_train_epochs = num_train_epochs,\n",
    "                max_length = max_length,\n",
    "                max_prompt_length = max_prompt_length,\n",
    "                max_target_length = max_target_length,\n",
    "                per_device_train_batch_size = per_device_train_batch_size,\n",
    "                gradient_accumulation_steps = gradient_accumulation_steps,\n",
    "                seed = seed)\n",
    "\n",
    "    return f\"{model_output_dir}/dpo_model\", chosen_rewards, rejected_rewards"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "# Load data\n",
    "selected_src_encodec, selected_instruction = extract_data_from_json('dpo_data/src_encodec.json')\n",
    "\n",
    "# Define paths and device\n",
    "base_path = \"/work/b0990106x/trl\"\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# Timestamp\n",
    "now = datetime.now()\n",
    "ts = now.strftime(\"%m%d-%H%M\")\n",
    "print(\"timestamp:\", ts)\n",
    "\n",
    "# Output paths\n",
    "model_output_dir = os.path.join(base_path, \"model_output\", ts)\n",
    "agent_output_dir = os.path.join(base_path, \"output\", ts)\n",
    "os.makedirs(model_output_dir, exist_ok=True)\n",
    "os.makedirs(agent_output_dir, exist_ok=True)\n",
    "\n",
    "# Seed\n",
    "seed = 42\n",
    "\n",
    "# Arguments\n",
    "args_predict = SimpleNamespace(output_path=f\"{base_path}/output/{ts}/example.wav\", seed=seed, device=device)\n",
    "ar_checkpoint = \"lca0503/speech-chatgpt-base-ar-v2-epoch10-wotrans\"\n",
    "nar_checkpoint = \"lca0503/speech-chatgpt-base-nar-v2-epoch4-wotrans\"\n",
    "\n",
    "# Model and Iterations\n",
    "model_checkpoint = ar_checkpoint\n",
    "sample_size = 80\n",
    "num_iterations = 1000\n",
    "train_selected_indices = [8]\n",
    "data_size_per_iteration = len(train_selected_indices)\n",
    "\n",
    "# Training Configuration\n",
    "beta = 0.1\n",
    "learning_rate = 5e-07\n",
    "num_train_epochs = 3\n",
    "max_length = 1024 * 9\n",
    "max_prompt_length = 1024 * 9\n",
    "max_target_length = 1024 * 9\n",
    "per_device_train_batch_size = 8\n",
    "gradient_accumulation_steps = 1\n",
    "\n",
    "# Evaluation Configuration\n",
    "eval_train = True\n",
    "eval_test = False\n",
    "eval_train_indices = train_selected_indices\n",
    "eval_test_indices = random.sample(range(len(selected_src_encodec)), 5)\n",
    "eval_train_data_len = 1000\n",
    "eval_test_data_len = len(eval_test_indices)\n",
    "num_eval = 10\n",
    "eval_frequency = 1\n",
    "\n",
    "print(f\"length of all_src_encodec: {len(selected_src_encodec)}\")\n",
    "print(f\"length of all_instruction: {len(selected_instruction)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "sr = 24000\n",
    "text_enc_name = \"google/flan-t5-large\"\n",
    "text_enc_dim = 1024\n",
    "text_blstm_dim = 256\n",
    "speech_enc_name = \"wavlm\"\n",
    "speech_enc_dim = 768\n",
    "speech_blstm_dim = 256\n",
    "rep_dim = 512\n",
    "sub_dim = 0\n",
    "n_sub = 1\n",
    "ckpt_pth = f'{base_path}/CLAPS/pretrained/7d/cp_claps_blstm_m_50k_v3/cp_0045000'\n",
    "project_dir = \"cp_claps\"\n",
    "\n",
    "# Argument Namespace\n",
    "a = argparse.Namespace(\n",
    "    sr=sr,\n",
    "    text_enc_name=text_enc_name,\n",
    "    text_enc_dim=text_enc_dim,\n",
    "    text_blstm_dim=text_blstm_dim,\n",
    "    speech_enc_name=speech_enc_name,\n",
    "    speech_enc_dim=speech_enc_dim,\n",
    "    speech_blstm_dim=speech_blstm_dim,\n",
    "    rep_dim=rep_dim,\n",
    "    sub_dim=sub_dim,\n",
    "    n_sub=n_sub,\n",
    "    ckpt_pth=ckpt_pth,\n",
    "    project_dir=project_dir\n",
    ")\n",
    "\n",
    "# Load CLAP model\n",
    "clap_model, accelerator = load_model(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "# Print configurations\n",
    "print(f\"num_iterations: {num_iterations}\")\n",
    "print(f\"data_size_per_iteration: {data_size_per_iteration}\")\n",
    "print(f\"sample_size: {sample_size}\")\n",
    "print(f\"beta: {beta}\")\n",
    "print(f\"learning_rate: {learning_rate}\")\n",
    "print(f\"num_train_epochs: {num_train_epochs}\")\n",
    "print(f\"ar_checkpoint: {ar_checkpoint}\")\n",
    "print(f\"nar_checkpoint: {nar_checkpoint}\")\n",
    "print(f\"args_predict: {args_predict}\")\n",
    "print(f\"model_output_dir: {model_output_dir}\")\n",
    "print(f\"agent_output_dir: {agent_output_dir}\")\n",
    "print(f\"base_path: {base_path}\")\n",
    "print(f\"device: {device}\")\n",
    "print(f\"eval_train_data_len: {eval_train_data_len}\")\n",
    "print(f\"eval_test_data_len: {eval_test_data_len}\")\n",
    "print(f\"eval_train_indices: {eval_train_indices}\")\n",
    "print(f\"eval_test_indices: {eval_test_indices}\")\n",
    "print(f\"eval_train: {eval_train}\")\n",
    "print(f\"eval_test: {eval_test}\")\n",
    "print(f\"num_eval: {num_eval}\")\n",
    "\n",
    "# Print training data\n",
    "for i in train_selected_indices:\n",
    "    print(f'training idx {i}: {selected_instruction[i]}')\n",
    "\n",
    "# Print evaluation data\n",
    "if eval_test:\n",
    "    for i in eval_test_indices:\n",
    "        print(f'evaluation idx {i}: {selected_instruction[i]}')\n",
    "\n",
    "if eval_train:\n",
    "    for i in eval_train_indices:\n",
    "        print(f'evaluation idx {i}: {selected_instruction[i]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoModelForSeq2SeqLMWithValueHead.from_pretrained(model_checkpoint, return_dict=True)\n",
    "ar_model = BartForConditionalGeneration.from_pretrained(ar_checkpoint)\n",
    "ar_tokenizer = AutoTokenizer.from_pretrained(ar_checkpoint)\n",
    "# ar_tokenizer.pad_token = ar_tokenizer.eos_token\n",
    "nar_model = NARBartForConditionalGeneration.from_pretrained(nar_checkpoint)\n",
    "nar_tokenizer = AutoTokenizer.from_pretrained(nar_checkpoint)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logging Start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "\n",
    "log_path = f'{model_output_dir}/log_training.log'\n",
    "print(f\"Logging to: {log_path}\")\n",
    "\n",
    "# Set up logging\n",
    "logging.basicConfig(\n",
    "    filename=log_path,\n",
    "    filemode='a',\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s',\n",
    "    level=logging.INFO\n",
    ")\n",
    "\n",
    "logging.info(\n",
    "    f\"Parameters:\\n\"\n",
    "    f\"Prepare Data: sample_size: {sample_size}\\n\"\n",
    "    f\"Training: num_iterations: {num_iterations}\\n\"\n",
    "    f\"Training: data_size_per_iteration: {data_size_per_iteration}\\n\"\n",
    "    f\"Training: train_selected_indices: {train_selected_indices}\\n\"\n",
    "    f\"Training: beta: {beta}\\n\"\n",
    "    f\"Training: learning_rate: {learning_rate}\\n\"\n",
    "    f\"Training: num_train_epochs: {num_train_epochs}\\n\"\n",
    "    f\"Training: max_length: {max_length}\\n\"\n",
    "    f\"Training: max_prompt_length: {max_prompt_length}\\n\"\n",
    "    f\"Training: max_target_length: {max_target_length}\\n\"\n",
    "    f\"Training: per_device_train_batch_size: {per_device_train_batch_size}\\n\"\n",
    "    f\"Training: gradient_accumulation_steps: {gradient_accumulation_steps}\\n\"\n",
    "    f\"Training: seed: {seed}\\n\"\n",
    "    f\"Training: ar_checkpoint: {ar_checkpoint}\\n\"\n",
    "    f\"Training: nar_checkpoint: {nar_checkpoint}\\n\"\n",
    "    f\"Training: args_predict: {args_predict}\\n\"\n",
    "    f\"Training: model_output_dir: {model_output_dir}\\n\"\n",
    "    f\"Training: agent_output_dir: {agent_output_dir}\\n\"\n",
    "    f\"Training: base_path: {base_path}\\n\"\n",
    "    f\"Training: device: {device}\\n\"\n",
    "    f\"Evaluation: eval_train_data_len: {eval_train_data_len}\\n\"\n",
    "    f\"Evaluation: eval_test_data_len: {eval_test_data_len}\\n\"\n",
    "    f\"Evaluation: eval_train_indices: {eval_train_indices}\\n\"\n",
    "    f\"Evaluation: eval_test_indices: {eval_test_indices}\\n\"\n",
    "    f\"Evaluation: eval_train: {eval_train}\\n\"\n",
    "    f\"Evaluation: eval_test: {eval_test}\\n\"\n",
    "    f\"Evaluation: num_eval: {num_eval}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initial Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "# Start time\n",
    "total_start_time = time.time()\n",
    "\n",
    "def evaluate_model(eval_type, eval_indices, eval_data_len):\n",
    "    metrics, rewards = eval_dpo_claps_batch(\n",
    "        nar_model=nar_model,\n",
    "        ar_tokenizer=ar_tokenizer,\n",
    "        nar_tokenizer=nar_tokenizer,\n",
    "        trained_model=model,\n",
    "        args_predict=args_predict,\n",
    "        all_src_encodec=selected_src_encodec,\n",
    "        all_instruction=selected_instruction,\n",
    "        iteration=-1,\n",
    "        num_evaluations=num_eval,\n",
    "        eval_data_len=eval_data_len,\n",
    "        selected_indices=eval_indices,\n",
    "        device=device,\n",
    "        clap_model=clap_model,\n",
    "        accelerator=accelerator\n",
    "    )\n",
    "    logging.info(f\"Original Model {eval_type} Set Evaluation:\")\n",
    "    logging.info(f\"Original model metrics on {eval_type} set: {metrics}\")\n",
    "    logging.info(f\"Original model rewards on {eval_type} set: {rewards}\")\n",
    "\n",
    "    reward_list = []\n",
    "    for reward_group in rewards:\n",
    "        filtered_rewards = [r for r in reward_group if r is not None]\n",
    "        reward_list.append(None if not filtered_rewards else np.mean(filtered_rewards))\n",
    "    \n",
    "    logging.info(f\"Original model reward list on {eval_type} set: {reward_list}\")\n",
    "    filtered_reward_list = [r for r in reward_list if r is not None]\n",
    "    avg_reward = None if not filtered_reward_list else np.mean(filtered_reward_list)\n",
    "    logging.info(f\"Original model average rewards on {eval_type} set: {avg_reward}\")\n",
    "\n",
    "if eval_train:\n",
    "    evaluate_model(\"Train\", eval_train_indices, eval_train_data_len)\n",
    "\n",
    "if eval_test:\n",
    "    evaluate_model(\"Test\", eval_test_indices, eval_test_data_len)\n",
    "\n",
    "# Prepare data for training\n",
    "if train_selected_indices:\n",
    "    batch_src_encodec = [selected_src_encodec[i] for i in train_selected_indices]\n",
    "    batch_instruction = [selected_instruction[i] for i in train_selected_indices]\n",
    "    logging.info(f\"Processing data from selected indices: {train_selected_indices}\")\n",
    "else:\n",
    "    start_idx, end_idx = 0, data_size_per_iteration\n",
    "    batch_src_encodec = selected_src_encodec[start_idx:end_idx]\n",
    "    batch_instruction = selected_instruction[start_idx:end_idx]\n",
    "    logging.info(f\"Processing data from index {start_idx} to {end_idx}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "\n",
    "import os\n",
    "os.environ[\"WANDB_SILENT\"] = \"true\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Start training iterations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "disable_tqdm = not os.isatty(1)\n",
    "\n",
    "def evaluate_iteration(eval_type, iteration, eval_indices, eval_data_len):\n",
    "    metrics, rewards = eval_dpo_claps_batch(\n",
    "        nar_model=nar_model,\n",
    "        ar_tokenizer=ar_tokenizer,\n",
    "        nar_tokenizer=nar_tokenizer,\n",
    "        trained_model=model,\n",
    "        args_predict=args_predict,\n",
    "        all_src_encodec=selected_src_encodec,\n",
    "        all_instruction=selected_instruction,\n",
    "        iteration=iteration,\n",
    "        num_evaluations=num_eval,\n",
    "        eval_data_len=eval_data_len,\n",
    "        selected_indices=eval_indices,\n",
    "        device=device,\n",
    "        clap_model=clap_model,\n",
    "        accelerator=accelerator\n",
    "    )\n",
    "    logging.info(f\"Trained Model Iteration {iteration} {eval_type} Set Evaluation:\")\n",
    "    logging.info(f\"EVAL: Cosine_Sim metrics {eval_type} Set for iteration {iteration}: {metrics}\")\n",
    "    logging.info(f\"EVAL: Cosine_Sim score {eval_type} Set for iteration {iteration}: {rewards}\")\n",
    "\n",
    "    reward_list = [np.mean([r for r in reward_group if r is not None]) if reward_group else None for reward_group in rewards]\n",
    "    logging.info(f\"EVAL: Trained model Cosine_Sim score list on {eval_type} set: {reward_list}\")\n",
    "    filtered_reward_list = [r for r in reward_list if r is not None]\n",
    "    avg_reward = np.mean(filtered_reward_list) if filtered_reward_list else None\n",
    "    logging.info(f\"EVAL: Trained model average Cosine_Sim score on {eval_type} set: {avg_reward}\")\n",
    "\n",
    "for iteration in tqdm(range(num_iterations), desc=\"Training Iterations\", disable=disable_tqdm):\n",
    "    logging.info(f\"-----------Starting iteration {iteration}-----------\")\n",
    "\n",
    "    resume = False\n",
    "\n",
    "    model_checkpoint, chosen_rewards, rejected_rewards = train_iteration(\n",
    "        model=model,\n",
    "        model_checkpoint=model_checkpoint,\n",
    "        iteration=iteration,\n",
    "        data_size=data_size_per_iteration,\n",
    "        sample_size=sample_size,\n",
    "        ar_model=ar_model,\n",
    "        ar_tokenizer=ar_tokenizer,\n",
    "        nar_model=nar_model,\n",
    "        nar_tokenizer=nar_tokenizer,\n",
    "        all_src_encodec=batch_src_encodec,\n",
    "        all_instruction=batch_instruction,\n",
    "        args_predict=args_predict,\n",
    "        agent_output_dir=agent_output_dir,\n",
    "        model_output_dir_base=model_output_dir,\n",
    "        temperature=1.0,\n",
    "        beta=beta,\n",
    "        base_path=base_path,\n",
    "        resume_from_checkpoint=resume,\n",
    "        learning_rate=learning_rate,\n",
    "        num_train_epochs=num_train_epochs,\n",
    "        max_length=max_length,\n",
    "        max_prompt_length=max_prompt_length,\n",
    "        max_target_length=max_target_length,\n",
    "        per_device_train_batch_size=per_device_train_batch_size,\n",
    "        gradient_accumulation_steps=gradient_accumulation_steps,\n",
    "        seed=seed,\n",
    "        clap_model=clap_model,\n",
    "        accelerator=accelerator\n",
    "    )\n",
    "\n",
    "    logging.info(f\"Chosen rewards for iteration {iteration}: {chosen_rewards}\")\n",
    "    logging.info(f\"Rejected rewards for iteration {iteration}: {rejected_rewards}\")\n",
    "\n",
    "    if (iteration + 1) % eval_frequency == 0:\n",
    "        if eval_train:\n",
    "            evaluate_iteration(\"Train\", iteration, eval_train_indices, eval_train_data_len)\n",
    "        if eval_test:\n",
    "            evaluate_iteration(\"Test\", iteration, eval_test_indices, eval_test_data_len)\n",
    "\n",
    "    logging.info(f\"-----------Finished iteration {iteration}-----------\")\n",
    "\n",
    "total_end_time = time.time()\n",
    "total_time_taken = total_end_time - total_start_time\n",
    "logging.info(f\"Total time taken for the entire process: {total_time_taken:.2f} seconds\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dpo",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
