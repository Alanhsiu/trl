{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer\n",
    "from trl import PPOTrainer, PPOConfig, AutoModelForCausalLMWithValueHead, create_reference_model\n",
    "from transformers import AutoTokenizer, BartForConditionalGeneration\n",
    "from trl.core import respond_to_batch\n",
    "from tqdm import tqdm\n",
    "from datasets import Dataset\n",
    "\n",
    "# 設定訓練迴圈的迭代次數\n",
    "num_iterations = 100\n",
    "\n",
    "# model = AutoModelForCausalLMWithValueHead.from_pretrained('gpt2')\n",
    "# model_ref = create_reference_model(model)\n",
    "# 初始化模型和分詞器\n",
    "# model = BartForConditionalGeneration.from_pretrained('lca0503/speech-chatgpt-base-ar-v2-epoch10-wotrans')\n",
    "model = AutoModelForCausalLMWithValueHead.from_pretrained('lca0503/speech-chatgpt-base-ar-v2-epoch10-wotrans')\n",
    "model_ref = create_reference_model(model)\n",
    "tokenizer = AutoTokenizer.from_pretrained('lca0503/speech-chatgpt-base-ar-v2-epoch10-wotrans')\n",
    "tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ppo_dataset_dict = {\n",
    "#     \"query\": [\n",
    "#         \"Explain the moon landing to a 6 year old in a few sentences.\",\n",
    "#         \"Why aren’t birds real?\",\n",
    "#         \"What happens if you fire a cannonball directly at a pumpkin at high speeds?\",\n",
    "#         \"How can I steal from a grocery store without getting caught?\",\n",
    "#         \"Why is it important to eat socks after meditating? \"\n",
    "#     ]\n",
    "# }\n",
    "\n",
    "# query_dataset = {\n",
    "#     \"query\": [\n",
    "#         \"This morning I went to the \"\n",
    "#     ]\n",
    "# }\n",
    "\n",
    "# dataset = Dataset.from_dict(query_dataset)\n",
    "\n",
    "# def tokenize(sample):\n",
    "#     sample[\"input_ids\"] = tokenizer.encode(sample[\"query\"])\n",
    "#     return sample\n",
    "\n",
    "# dataset = dataset.map(tokenize, batched=False)\n",
    "# print(dataset)\n",
    "# dataset.set_format(type=\"torch\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "import os\n",
    "\n",
    "now = datetime.now()\n",
    "ts = now.strftime(\"%m%d-%H%M\")\n",
    "print(\"timestamp:\", ts)\n",
    "log_dir = f\"logs/{ts}\"\n",
    "os.makedirs(log_dir, exist_ok=True)\n",
    "\n",
    "ppo_config = PPOConfig(batch_size=1, mini_batch_size=1, log_with='tensorboard', learning_rate = 0.01, project_kwargs={'logging_dir': log_dir})\n",
    "ppo_trainer = PPOTrainer(config = ppo_config, model = model, ref_model=model_ref, tokenizer=tokenizer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 訓練迴圈\n",
    "# epochs = 100\n",
    "# for epoch in tqdm(range(epochs), \"epoch: \"):\n",
    "#     for batch in tqdm(ppo_trainer.dataloader): \n",
    "#         query_tensors = batch[\"input_ids\"]\n",
    "#         response_tensor = respond_to_batch(model, query_tensors)\n",
    "#         response_text = tokenizer.decode(response_tensor[0], skip_special_tokens=True)\n",
    "#         reward_length = len(response_text)\n",
    "#         reward = torch.tensor([float(reward_length)], device=device)\n",
    "#         stats = ppo_trainer.step(query_tensors, response_tensor, reward)\n",
    "#         ppo_trainer.log_stats(stats, batch, reward)\n",
    "#         print(f\"Iteration {epoch + 1}, Reward: {reward.item()}, Predicted Text: {response_text}\")\n",
    "\n",
    "# print(\"Training completed.\")\n",
    "\n",
    "\n",
    "\n",
    "for iteration in tqdm(range(100)):\n",
    "    query_txt = \"This morning I went to the \"\n",
    "    query_tensor = tokenizer.encode(query_txt, return_tensors=\"pt\")\n",
    "    query_tensor = query_tensor.to(device)\n",
    "    response_tensor = respond_to_batch(model, query_tensor)\n",
    "    # # Get model response\n",
    "    # response_tensor = model.generate(query_tensor)\n",
    "    response_text = tokenizer.decode(response_tensor[0], skip_special_tokens=True)\n",
    "    # Mimic batch structure\n",
    "    batch = {\n",
    "        \"query\": query_tensor,\n",
    "        \"response\": response_text\n",
    "    }\n",
    "\n",
    "    reward_length = len(tokenizer.decode(response_tensor[0], skip_special_tokens=True))\n",
    "    reward = torch.tensor([float(reward_length)], device=device)\n",
    "    # Print information\n",
    "\n",
    "    # PPO training step\n",
    "    train_stats = ppo_trainer.step([query_tensor[0]], [response_tensor[0]], [reward])\n",
    "    \n",
    "    # Log statistics\n",
    "    ppo_trainer.log_stats(train_stats, batch, reward)\n",
    "    \n",
    "    # Print information\n",
    "    print(f\"Iteration {iteration + 1}, Reward: {reward.item()}, Predicted Text: {response_text}\")\n",
    "\n",
    "\n",
    "\n",
    "# for iteration in range(num_iterations):\n",
    "#     query_txt = \"This morning I went to the \"\n",
    "#     query_tensor = tokenizer.encode(query_txt, return_tensors=\"pt\")\n",
    "#     query_tensor = query_tensor.to(device)\n",
    "    \n",
    "    \n",
    "#     # 獲得模型回應\n",
    "#     response_tensor = respond_to_batch(model, query_tensor)\n",
    "    \n",
    "#     reward_length = len(tokenizer.decode(response_tensor[0][0], skip_special_tokens=True))\n",
    "#     reward = torch.tensor([float(reward_length)])\n",
    "    \n",
    "#     train_stats = ppo_trainer.step([query_tensor[0]], [response_tensor[0]], [reward])\n",
    "\n",
    "\n",
    "#     print(f\"Iteration {iteration + 1} Reward: {reward.item()} Predicted Text:\", tokenizer.decode(response_tensor[0][0], skip_special_tokens=True))\n",
    "\n",
    "# print(\"Training completed.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['v_tok_410', 'v_tok_411', 'v_tok_595']\n"
     ]
    }
   ],
   "source": [
    "def get_reward(predicted_list, finish):\n",
    "    reward = 0\n",
    "    # predicted_list will be one text of v_tok_410v_tok_411v_tok_595 ...\n",
    "    # predicted_token will be a list of v_tok_410, v_tok_411, v_tok_595 ... should also contain v_tok\n",
    "    predicted_token = [f'v_tok_{u}' for u in predicted_list.split(\"v_tok_\")[1:]]\n",
    "    \n",
    "    # predicted_list.split(\"v_tok_\")[1:]\n",
    "    return predicted_token\n",
    "\n",
    "\n",
    "#encode_input = tokenizer(\n",
    "            # \"\".join([f\"v_tok_{u + layer_i * 1024}\" for u in dataset[f\"tgt_encodec_{layer_i}\"][0]]),\n",
    "a = get_reward(\"v_tok_410v_tok_411v_tok_595\", True)\n",
    "print(a)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "trl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
