{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch\n",
    "# torch.cuda.set_device(1)\n",
    "# print(f\"Using device: {torch.cuda.current_device()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "current_base_path = '/home/b09901066/trl'\n",
    "sys.path.append(f\"{current_base_path}/vc\")\n",
    "sys.path.append(f\"{current_base_path}/CLAPS\")\n",
    "\n",
    "import importlib\n",
    "import torch\n",
    "import os\n",
    "import math\n",
    "import numpy as np\n",
    "import random\n",
    "import time\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "from types import SimpleNamespace\n",
    "from datetime import datetime\n",
    "from typing import List, Tuple\n",
    "import string\n",
    "\n",
    "\n",
    "import vc\n",
    "importlib.reload(vc)\n",
    "from vc.trainer_encodec_vc_inference import (\n",
    "    pack_inputs_v2,\n",
    "    get_ar_prediction_audio_batch\n",
    ")\n",
    "from vc.encodec_model.nar_bart_model import NARBartForConditionalGeneration\n",
    "\n",
    "from transformers import BartForConditionalGeneration, AutoTokenizer\n",
    "from trl import (\n",
    "    DPOTrainer,\n",
    "    DPOConfig,\n",
    "    AutoModelForSeq2SeqLMWithValueHead,\n",
    "    create_reference_model\n",
    ")\n",
    "from datasets import Dataset\n",
    "\n",
    "from dpo_eval import (\n",
    "    get_reward_claps,\n",
    "    get_reward_asr,\n",
    "    eval_dpo_claps_asr_batch,\n",
    "    convert_array_to_tensor_format\n",
    ")\n",
    "from CLAPS.inference import load_model\n",
    "\n",
    "import argparse\n",
    "from faster_whisper import WhisperModel\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define paths and device\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(\"device:\", device)\n",
    "\n",
    "# Timestamp\n",
    "now = datetime.now()\n",
    "ts = now.strftime(\"%m%d-%H%M\")\n",
    "print(\"timestamp:\", ts)\n",
    "\n",
    "# Output paths\n",
    "model_output_dir = os.path.join(current_base_path, \"model_output\", ts)\n",
    "agent_output_dir = os.path.join(current_base_path, \"output\", ts)\n",
    "os.makedirs(model_output_dir, exist_ok=True)\n",
    "os.makedirs(agent_output_dir, exist_ok=True)\n",
    "\n",
    "# Initialize TensorBoard SummaryWriter\n",
    "log_dir = f\"{model_output_dir}/tensorboard_logs\"\n",
    "writer = SummaryWriter(log_dir=log_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Utility Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seed(seed):\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(seed)\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "import shutil\n",
    "from pathlib import Path\n",
    "import soundfile as sf\n",
    "\n",
    "def generate_output_batch(\n",
    "        ar_model, \n",
    "        nar_model, \n",
    "        ar_tokenizer, \n",
    "        nar_tokenizer, \n",
    "        clap_model,\n",
    "        asr_model,\n",
    "        accelerator,\n",
    "        src_encodec: list, \n",
    "        instruction: list, \n",
    "        transcription: list,\n",
    "        args_predict: SimpleNamespace, \n",
    "        episode_counter: int = 0, \n",
    "        base_path: str = current_base_path, \n",
    "        temperature: float = 1.0\n",
    ") -> tuple[list[float], list[str]]:\n",
    "    audio_list, decode_ar_list = get_ar_prediction_audio_batch(\n",
    "        args_predict, ar_model, nar_model, ar_tokenizer, nar_tokenizer, src_encodec, instruction, episode_counter, temperature=temperature\n",
    "    )\n",
    "    reward_list, tokenized_decode_ar_list = [], []\n",
    "    valid_audio_paths = []\n",
    "\n",
    "    # temp_dir_path = Path(\"/dev/shm/temp_audio_files\")\n",
    "    # temp_dir_path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    for i, audio in enumerate(audio_list):\n",
    "        if audio is not None:\n",
    "            tensor_audio = convert_array_to_tensor_format(audio)\n",
    "            if tensor_audio[0].shape[0] == 1:\n",
    "                tensor_audio[0] = tensor_audio[0].squeeze(0)\n",
    "            clap_reward = get_reward_claps(\n",
    "                clap_model=clap_model, accelerator=accelerator, prompts=instruction[i], wavs=tensor_audio\n",
    "            )\n",
    "            output_path_ckpt = args_predict.output_path.replace(\".wav\", f\"_generate_{episode_counter}_item_{i}.wav\")\n",
    "            sf.write(output_path_ckpt, np.ravel(audio), samplerate=24000)\n",
    "            asr_reward = get_reward_asr(file_path=output_path_ckpt, asr_model=asr_model, ground_truth=transcription[i])\n",
    "            \n",
    "            penalty = 0 if asr_reward > 0.5 else 1\n",
    "            final_reward = clap_reward*0.9 + asr_reward*0.1 - penalty\n",
    "\n",
    "            # final_reward = clap_reward * asr_reward\n",
    "            # final_reward = clap_reward*0.9 + asr_reward*0.1\n",
    "            # final_reward = clap_reward*0.6 + asr_reward*0.4\n",
    "            # final_reward = clap_reward*0.75 + asr_reward*0.25\n",
    "            reward_list.append(final_reward)\n",
    "            valid_audio_paths.append(output_path_ckpt)\n",
    "        else:\n",
    "            reward_list.append(0)\n",
    "\n",
    "    for decode_ar in decode_ar_list:\n",
    "        list_decode_ar = decode_ar.flatten().tolist()\n",
    "        filtered_decode_ar_list = list_decode_ar[2:-1]\n",
    "        decode_ar_tokens = ar_tokenizer.convert_ids_to_tokens(filtered_decode_ar_list)\n",
    "        tokenized_decode_ar = ar_tokenizer.convert_tokens_to_string(decode_ar_tokens)\n",
    "        tokenized_decode_ar_list.append(tokenized_decode_ar)\n",
    "\n",
    "    return reward_list, tokenized_decode_ar_list\n",
    "\n",
    "def extract_data_from_json(file_path: str, all_valid_data_idx) -> Tuple[List[list], List[str], List[list]]:\n",
    "    with open(file_path, 'r') as f:\n",
    "        data = json.load(f)\n",
    "        \n",
    "    with open(f\"{current_base_path}/dataset/emotional_instructions.json\", 'r') as f:\n",
    "        emotional_instructions = json.load(f)\n",
    "    \n",
    "    all_src_encodec = []\n",
    "    all_instruction = []\n",
    "    all_transcription = []\n",
    "    # all_index = []\n",
    "    # count = 0\n",
    "    \n",
    "    # for i in range(len(data)):\n",
    "    #     src_encodec = data[i][\"src_encodec\"]\n",
    "    #     instruction = emotional_instructions[count%len(emotional_instructions)]\n",
    "    #     transcription = ''.join([char for char in data[i][\"transcription\"] if char not in string.punctuation]).lower().strip()\n",
    "    #     if len(src_encodec[0]) <= 100:\n",
    "            \n",
    "    #         all_src_encodec.append(src_encodec)\n",
    "    #         all_instruction.append(instruction)\n",
    "    #         all_transcription.append(transcription)\n",
    "    #         all_index.append(i)\n",
    "        \n",
    "    #         count += 1\n",
    "    #         print(f\"Count: {count}, Transcription: {transcription}\")\n",
    "            \n",
    "    #     if count == extract_number:\n",
    "    #         break\n",
    "    \n",
    "    for i in all_valid_data_idx:\n",
    "        all_src_encodec.append(data[i][\"src_encodec\"])\n",
    "        all_instruction.append(emotional_instructions[i%len(emotional_instructions)])\n",
    "        all_transcription.append(''.join([char for char in data[i][\"transcription\"] if char not in string.punctuation]).lower().strip())\n",
    "        \n",
    "    return all_src_encodec, all_instruction, all_transcription\n",
    "\n",
    "def train_model(\n",
    "        model,\n",
    "        model_ref,\n",
    "        ar_tokenizer,\n",
    "        train_dataset: Dataset,\n",
    "        val_dataset: Dataset,\n",
    "        model_output_dir: str,\n",
    "        beta: float,\n",
    "        resume_from_checkpoint: bool,\n",
    "        model_checkpoint: str,\n",
    "        learning_rate: float = 5e-07,\n",
    "        num_train_epochs: int = 200,\n",
    "        max_length: int = 1024*9,\n",
    "        max_prompt_length: int = 1024*9,\n",
    "        max_target_length: int = 1024*9,\n",
    "        per_device_train_batch_size: int = 1,\n",
    "        gradient_accumulation_steps: int = 1,\n",
    "        seed: int = 42,\n",
    "        iteration: int = 0\n",
    ") -> None:\n",
    "    training_args = DPOConfig(\n",
    "        beta=beta,\n",
    "        output_dir=model_output_dir,\n",
    "        resume_from_checkpoint=model_checkpoint if resume_from_checkpoint else None,\n",
    "        seed=seed,\n",
    "        per_device_train_batch_size=per_device_train_batch_size,\n",
    "        num_train_epochs=num_train_epochs,\n",
    "        gradient_accumulation_steps=gradient_accumulation_steps,\n",
    "        learning_rate=learning_rate,\n",
    "        max_length=max_length,\n",
    "        max_prompt_length=max_prompt_length,\n",
    "        max_target_length=max_target_length,\n",
    "        evaluation_strategy=\"steps\",\n",
    "        save_steps=5000,\n",
    "        logging_dir=f\"{model_output_dir}/logs\"\n",
    "    )\n",
    "    \n",
    "    trainer = DPOTrainer(\n",
    "        model=model,\n",
    "        ref_model=model_ref,\n",
    "        args=training_args,\n",
    "        tokenizer=ar_tokenizer,\n",
    "        train_dataset=train_dataset,\n",
    "        eval_dataset=val_dataset,\n",
    "        loss_type=\"sigmoid\", # \"sigmoid\", \"hinge\", \"ipo\", \"kto_pair\", \"bco_pair\"\n",
    "        # log_fn=logging.info,  # Pass logging function\n",
    "        # writer=writer,        # Pass TensorBoard writer\n",
    "    )\n",
    "    trainer.train()\n",
    "\n",
    "    print(f\"Trainer State: {trainer.state}\")\n",
    "    print(f\"Trainer State Global Step: {trainer.state.global_step}, Trainer State Log History: {trainer.state.log_history}\")\n",
    "    \n",
    "    for log in trainer.state.log_history:\n",
    "        if \"train_loss\" in log:\n",
    "            iteration_loss = log[\"train_loss\"]\n",
    "            writer.add_scalar(\"Loss/Iteration\", iteration_loss, iteration)\n",
    "            logging.info(f\"Iteration: {iteration}, Loss: {iteration_loss}\")\n",
    "            print(f\"Iteration: {iteration}, Loss: {iteration_loss}\")\n",
    "        else:\n",
    "            print(f\"Log: {log}\")\n",
    "\n",
    "    model.config.to_json_file(f\"{model_output_dir}/config.json\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "def process_data_batch(\n",
    "    sample_size: int, \n",
    "    ar_model, \n",
    "    nar_model, \n",
    "    ar_tokenizer, \n",
    "    nar_tokenizer, \n",
    "    clap_model,\n",
    "    asr_model,\n",
    "    accelerator,\n",
    "    selected_src_encodec: List[list], \n",
    "    selected_instruction: List[str],\n",
    "    selected_transcription: List[str],\n",
    "    args_predict, \n",
    "    base_path: str = current_base_path, \n",
    "    temperature: float = 1.0, \n",
    "    iteration: int = 0,\n",
    "    prev_eval_avg: float = 0,\n",
    "    strategy: str = \"top_bottom_percent\"  # Default to original strategy. Options: \"max_min\", \"top_bottom_percent\", \"above_below_average\", \"above_prev_eval\"\n",
    ") -> Tuple[List[str], List[str], List[str], List[float], List[float], List[float]]:\n",
    "    # Ensure sample size is valid\n",
    "    if sample_size < 2:\n",
    "        raise ValueError(\"Parameter 'sample_size' must be greater than 1.\")\n",
    "\n",
    "    chosen, rejected, prompts, chosen_rewards, rejected_rewards, average_rewards = [], [], [], [], [], []\n",
    "\n",
    "    disable_tqdm = not os.isatty(1)\n",
    "    for i in tqdm(range(len(selected_src_encodec)), desc=\"Processing Data\", disable=disable_tqdm):\n",
    "        rewards, tokenized_outputs = [], []\n",
    "        size_of_packed_input = (\n",
    "            len(selected_src_encodec[i][0]) +\n",
    "            len(ar_tokenizer(selected_instruction[i])[\"input_ids\"][1:-1]) +\n",
    "            3\n",
    "        )\n",
    "        if 4 < size_of_packed_input <= 1024:\n",
    "            selected_src_encodec_list = [selected_src_encodec[i]] * sample_size\n",
    "            selected_instruction_list = [selected_instruction[i]] * sample_size\n",
    "            selected_transcription_list = [selected_transcription[i]] * sample_size\n",
    "            rewards, tokenized_outputs = generate_output_batch(\n",
    "                ar_model=ar_model, \n",
    "                nar_model=nar_model, \n",
    "                ar_tokenizer=ar_tokenizer, \n",
    "                nar_tokenizer=nar_tokenizer,\n",
    "                src_encodec=selected_src_encodec_list,\n",
    "                instruction=selected_instruction_list, \n",
    "                transcription=selected_transcription_list,\n",
    "                clap_model=clap_model,\n",
    "                asr_model=asr_model,\n",
    "                accelerator=accelerator,\n",
    "                args_predict=args_predict,\n",
    "                episode_counter=f\"data_{i}\",\n",
    "                base_path=base_path, \n",
    "                temperature=temperature\n",
    "            )\n",
    "\n",
    "        valid_rewards = [r for r in rewards if r is not None]\n",
    "        valid_outputs = [tokenized_outputs[j] for j in range(len(rewards)) if rewards[j] is not None]\n",
    "\n",
    "        if len(valid_rewards) >= 2:\n",
    "            average_reward = np.mean(valid_rewards)\n",
    "            print(f\"Average reward for data index {i}: {average_reward}\")\n",
    "\n",
    "            if strategy == \"max_min\":\n",
    "                # Original max-min strategy\n",
    "                max_reward_index = np.argmax(valid_rewards)\n",
    "                min_reward_index = np.argmin(valid_rewards)\n",
    "                chosen_outputs = [valid_outputs[max_reward_index]]\n",
    "                rejected_outputs = [valid_outputs[min_reward_index]]\n",
    "                chosen_rewards_part = [valid_rewards[max_reward_index]]\n",
    "                rejected_rewards_part = [valid_rewards[min_reward_index]]\n",
    "\n",
    "            elif strategy == \"top_bottom_percent\":\n",
    "                # Select top and bottom 20% of rewards\n",
    "                twenty_percent_num = max(1, math.ceil(len(valid_rewards) * 0.2))\n",
    "                max_indices = np.argsort(valid_rewards)[-twenty_percent_num:]\n",
    "                min_indices = np.argsort(valid_rewards)[:twenty_percent_num]\n",
    "\n",
    "                chosen_outputs = [valid_outputs[j] for j in max_indices]\n",
    "                rejected_outputs = [valid_outputs[j] for j in min_indices]\n",
    "                chosen_rewards_part = [valid_rewards[j] for j in max_indices]\n",
    "                rejected_rewards_part = [valid_rewards[j] for j in min_indices]\n",
    "\n",
    "            elif strategy == \"above_below_average\":\n",
    "                # Select rewards above and below the average\n",
    "                threshold = 0.05\n",
    "                chosen_outputs = [valid_outputs[j] for j in range(len(valid_rewards)) if valid_rewards[j] > average_reward + threshold]\n",
    "                rejected_outputs = [valid_outputs[j] for j in range(len(valid_rewards)) if valid_rewards[j] < average_reward - threshold]\n",
    "\n",
    "                # Sort and trim to ensure balanced chosen and rejected outputs\n",
    "                chosen_outputs = [x for _, x in sorted(zip(valid_rewards, chosen_outputs), reverse=True)]\n",
    "                rejected_outputs = [x for _, x in sorted(zip(valid_rewards, rejected_outputs))]\n",
    "\n",
    "                min_length = min(len(chosen_outputs), len(rejected_outputs))\n",
    "                chosen_outputs = chosen_outputs[:min_length]\n",
    "                rejected_outputs = rejected_outputs[:min_length]\n",
    "\n",
    "                chosen_rewards_part = [valid_rewards[j] for j in range(len(valid_rewards)) if valid_rewards[j] > average_reward][:min_length]\n",
    "                rejected_rewards_part = [valid_rewards[j] for j in range(len(valid_rewards)) if valid_rewards[j] < average_reward][:min_length]\n",
    "\n",
    "            elif strategy == \"above_prev_eval\":\n",
    "                # Select rewards above and below a previous evaluation average\n",
    "                chosen_outputs = [valid_outputs[j] for j in range(len(valid_rewards)) if valid_rewards[j] > prev_eval_avg]\n",
    "                rejected_outputs = [valid_outputs[j] for j in range(len(valid_rewards)) if valid_rewards[j] < prev_eval_avg]\n",
    "\n",
    "                # Sort and trim\n",
    "                chosen_outputs = [x for _, x in sorted(zip(valid_rewards, chosen_outputs), reverse=True)]\n",
    "                rejected_outputs = [x for _, x in sorted(zip(valid_rewards, rejected_outputs))]\n",
    "\n",
    "                min_length = min(len(chosen_outputs), len(rejected_outputs))\n",
    "                if min_length == 0:\n",
    "                    chosen_outputs = [valid_outputs[np.argmax(valid_rewards)]]\n",
    "                    rejected_outputs = [valid_outputs[np.argmin(valid_rewards)]]\n",
    "                    chosen_rewards_part = [valid_rewards[np.argmax(valid_rewards)]]\n",
    "                    rejected_rewards_part = [valid_rewards[np.argmin(valid_rewards)]]\n",
    "                else:\n",
    "                    chosen_outputs = chosen_outputs[:min_length]\n",
    "                    rejected_outputs = rejected_outputs[:min_length]\n",
    "                    chosen_rewards_part = [valid_rewards[j] for j in range(len(valid_rewards)) if valid_rewards[j] > prev_eval_avg][:min_length]\n",
    "                    rejected_rewards_part = [valid_rewards[j] for j in range(len(valid_rewards)) if valid_rewards[j] < prev_eval_avg][:min_length]\n",
    "\n",
    "            obs_input = pack_inputs_v2(ar_tokenizer, selected_src_encodec[i], selected_instruction[i])\n",
    "            tokenize_input = ar_tokenizer.convert_ids_to_tokens(obs_input)\n",
    "            tokenize_input_str = ar_tokenizer.convert_tokens_to_string(tokenize_input)\n",
    "            prompts.extend([tokenize_input_str] * len(chosen_outputs))\n",
    "            average_rewards.append(average_reward)\n",
    "\n",
    "            chosen.extend(chosen_outputs)\n",
    "            rejected.extend(rejected_outputs)\n",
    "            chosen_rewards.extend(chosen_rewards_part)\n",
    "            rejected_rewards.extend(rejected_rewards_part)\n",
    "        else:\n",
    "            print(f\"Not enough valid rewards for data index {i}\")\n",
    "\n",
    "    if len(selected_src_encodec) == 1:\n",
    "        chosen *= 2\n",
    "        rejected *= 2\n",
    "        prompts *= 2\n",
    "        chosen_rewards *= 2\n",
    "        rejected_rewards *= 2\n",
    "        average_rewards *= 2    \n",
    "\n",
    "    return chosen, rejected, prompts, chosen_rewards, rejected_rewards, average_rewards\n",
    "\n",
    "\n",
    "def generate_data(ar_model, \n",
    "                  ar_tokenizer, \n",
    "                  nar_model, \n",
    "                  nar_tokenizer, \n",
    "                  clap_model,\n",
    "                  asr_model,\n",
    "                  accelerator,\n",
    "                  selected_src_encodec: List[list], \n",
    "                  selected_instruction: List[str],\n",
    "                  selected_transcription: List[str],\n",
    "                  args_predict: SimpleNamespace, \n",
    "                  sample_size: int, \n",
    "                  iteration: int, \n",
    "                  agent_output_dir: str, \n",
    "                  base_path: str = current_base_path, \n",
    "                  temperature: float = 1.0,\n",
    "                  prev_eval_avg: float = 0\n",
    ") -> Tuple[dict, List[float], List[float]]:\n",
    "    \"\"\"\n",
    "    Generates data for the dataset and saves info to a JSON file.\n",
    "    Returns:\n",
    "        tuple:\n",
    "            data_for_dataset (dict): A dictionary containing the data for the dataset.\n",
    "            chosen_rewards (List[float]): A list of rewards for the chosen outputs.\n",
    "            rejected_rewards (List[float]): A list of rewards for the rejected outputs.\n",
    "    \"\"\"\n",
    "    chosen, rejected, prompts, chosen_rewards, rejected_rewards, average_rewards = process_data_batch(\n",
    "        sample_size=sample_size,\n",
    "        ar_model=ar_model,\n",
    "        nar_model=nar_model,\n",
    "        ar_tokenizer=ar_tokenizer,\n",
    "        nar_tokenizer=nar_tokenizer,\n",
    "        selected_src_encodec=selected_src_encodec,\n",
    "        selected_instruction=selected_instruction,\n",
    "        selected_transcription=selected_transcription,\n",
    "        args_predict=args_predict,\n",
    "        base_path=base_path,\n",
    "        temperature=temperature,\n",
    "        iteration = iteration,\n",
    "        clap_model=clap_model,\n",
    "        asr_model=asr_model,\n",
    "        accelerator=accelerator,\n",
    "        prev_eval_avg=prev_eval_avg\n",
    "    )\n",
    "\n",
    "    data = {\n",
    "        \"prompt\": prompts,\n",
    "        \"chosen\": chosen,\n",
    "        \"rejected\": rejected,\n",
    "        \"chosen_rewards\": chosen_rewards,\n",
    "        \"rejected_rewards\": rejected_rewards,\n",
    "        \"average_rewards\": average_rewards\n",
    "    }\n",
    "\n",
    "    with open(f\"{agent_output_dir}/data_iter_{iteration}.json\", \"w\") as outfile:\n",
    "        json.dump(data, outfile, indent=4)\n",
    "\n",
    "    data_for_dataset = {key: data[key] for key in [\"prompt\", \"chosen\", \"rejected\"]}\n",
    "\n",
    "    return data_for_dataset, chosen_rewards, rejected_rewards\n",
    "\n",
    "def train_iteration(model, \n",
    "                    model_checkpoint,\n",
    "                    iteration, \n",
    "                    data_size, \n",
    "                    sample_size, \n",
    "                    ar_model, \n",
    "                    ar_tokenizer,\n",
    "                    nar_model, \n",
    "                    nar_tokenizer,\n",
    "                    all_src_encodec, \n",
    "                    all_instruction, \n",
    "                    all_transcription,\n",
    "                    args_predict, \n",
    "                    agent_output_dir,\n",
    "                    model_output_dir_base, \n",
    "                    clap_model,\n",
    "                    asr_model,\n",
    "                    accelerator,\n",
    "                    beta = 0.1, \n",
    "                    temperature = 1.0,\n",
    "                    base_path=current_base_path,\n",
    "                    resume_from_checkpoint = False,\n",
    "                    learning_rate = 5e-07,\n",
    "                    num_train_epochs = 100,\n",
    "                    max_length = 1024*9,\n",
    "                    max_prompt_length = 1024*9,\n",
    "                    max_target_length = 1024*9,\n",
    "                    per_device_train_batch_size = 1,\n",
    "                    gradient_accumulation_steps = 1,\n",
    "                    seed = 42,\n",
    "                    prev_eval_avg = 0\n",
    "):\n",
    "    \"\"\"\n",
    "    Executes one training iteration: generates data, trains the model, and saves the output.\n",
    "    \"\"\"\n",
    "\n",
    "    selected_src_encodec = all_src_encodec[:data_size]\n",
    "    selected_instruction = all_instruction[:data_size]\n",
    "    selected_transcription = all_transcription[:data_size]\n",
    "\n",
    "    data_for_dataset, chosen_rewards, rejected_rewards = generate_data(ar_model=model,\n",
    "                                                                        ar_tokenizer=ar_tokenizer,\n",
    "                                                                        nar_model=nar_model,\n",
    "                                                                        nar_tokenizer=nar_tokenizer,\n",
    "                                                                        selected_src_encodec=selected_src_encodec,\n",
    "                                                                        selected_instruction=selected_instruction,\n",
    "                                                                        selected_transcription=selected_transcription,\n",
    "                                                                        args_predict=args_predict,\n",
    "                                                                        sample_size=sample_size,\n",
    "                                                                        iteration=iteration,\n",
    "                                                                        agent_output_dir=agent_output_dir,\n",
    "                                                                        base_path=base_path,\n",
    "                                                                        temperature=temperature,\n",
    "                                                                        clap_model=clap_model,\n",
    "                                                                        asr_model=asr_model,\n",
    "                                                                        accelerator=accelerator,\n",
    "                                                                        prev_eval_avg=prev_eval_avg\n",
    "    )\n",
    "\n",
    "    dataset = Dataset.from_dict(data_for_dataset)\n",
    "    dataset_dict = dataset.train_test_split(test_size=0.1, shuffle=True, seed=seed)\n",
    "    train_dataset = dataset_dict[\"train\"]\n",
    "    val_dataset = dataset_dict[\"test\"]\n",
    "\n",
    "    model_output_dir = f\"{model_output_dir_base}/iter_{iteration}\"\n",
    "    os.makedirs(model_output_dir, exist_ok=True)\n",
    "\n",
    "    model_ref = create_reference_model(model)\n",
    "    \n",
    "    train_model(\n",
    "        model=model,\n",
    "        model_ref=model_ref,\n",
    "        ar_tokenizer=ar_tokenizer,\n",
    "        train_dataset=train_dataset,\n",
    "        val_dataset=val_dataset,\n",
    "        model_output_dir=model_output_dir,\n",
    "        beta=beta,\n",
    "        resume_from_checkpoint=resume_from_checkpoint,\n",
    "        model_checkpoint=model_checkpoint,\n",
    "        learning_rate = learning_rate,\n",
    "        num_train_epochs = num_train_epochs,\n",
    "        max_length = max_length,\n",
    "        max_prompt_length = max_prompt_length,\n",
    "        max_target_length = max_target_length,\n",
    "        per_device_train_batch_size = per_device_train_batch_size,\n",
    "        gradient_accumulation_steps = gradient_accumulation_steps,\n",
    "        seed = seed,\n",
    "        iteration=iteration\n",
    "    )\n",
    "\n",
    "    return f\"{model_output_dir}/dpo_model\", chosen_rewards, rejected_rewards"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ASR Model Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "asr_model_size = \"large-v1\" ## tiny, medium, large-v1, large-v2\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    asr_model = WhisperModel(asr_model_size, device=\"cuda\", compute_type=\"float16\")\n",
    "    print(asr_model_size, \"cuda\", \"float16\")\n",
    "else:\n",
    "    asr_model = WhisperModel(asr_model_size, device=\"cpu\", compute_type=\"int8\")\n",
    "    print(asr_model_size, \"cpu\", \"int8\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "# Seed\n",
    "seed = 42\n",
    "\n",
    "# Arguments\n",
    "args_predict = SimpleNamespace(output_path=f\"{current_base_path}/output/{ts}/example.wav\", seed=seed, device=device)\n",
    "ar_checkpoint = \"lca0503/speech-chatgpt-base-ar-v2-epoch10-wotrans\"\n",
    "nar_checkpoint = \"lca0503/speech-chatgpt-base-nar-v2-epoch4-wotrans\"\n",
    "\n",
    "# Model and Iterations\n",
    "model_checkpoint = ar_checkpoint\n",
    "sample_size = 10\n",
    "# sample_size = 40\n",
    "num_iterations = 1000\n",
    "# all_data_size = 14\n",
    "all_valid_data_idx = [16, 111, 118, 137, 138, 151, 199, 283, 333, 361, 363, 368, 426, 428, 435, 513, 515, 534, 542, 688, 692, 733, 793, 798, 820, 826, 838, 887, 947, 976, 980, 1104, 1128, 1169, 1184, 1212, 1241, 1271, 1272]\n",
    "all_data_size = len(all_valid_data_idx)\n",
    "train_selected_indices = list(range(int(all_data_size * 0.8)))\n",
    "data_size_per_iteration = len(train_selected_indices)\n",
    "\n",
    "# Training Configuration\n",
    "beta = 0.1\n",
    "learning_rate = 5e-07\n",
    "num_train_epochs = 3\n",
    "max_length = 1024 * 9\n",
    "max_prompt_length = 1024 * 9\n",
    "max_target_length = 1024 * 9\n",
    "per_device_train_batch_size = 8\n",
    "gradient_accumulation_steps = 1\n",
    "\n",
    "# Evaluation Configuration\n",
    "eval_train = True\n",
    "eval_test = True\n",
    "eval_train_indices = train_selected_indices\n",
    "eval_test_indices = list(range(int(all_data_size * 0.8), all_data_size))\n",
    "eval_train_data_len = 1000\n",
    "eval_test_data_len = len(eval_test_indices)\n",
    "num_eval = 10\n",
    "eval_frequency = 1\n",
    "\n",
    "# Load data\n",
    "# original_src_encodec, original_instruction, original_transcription, original_index = extract_data_from_json('dataset/transformed_soxdata_encodec.json', all_valid_data_idx)\n",
    "original_src_encodec, original_instruction, original_transcription = extract_data_from_json('dataset/transformed_soxdata_encodec.json', all_valid_data_idx)\n",
    "\n",
    "# train_selected_indices = original_index[:int(all_data_size * 0.8)]\n",
    "# eval_train_indices = train_selected_indices\n",
    "# eval_test_indices = original_index[int(all_data_size * 0.8):]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CLAPS Model Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sr = 24000\n",
    "text_enc_name = \"google/flan-t5-large\"\n",
    "text_enc_dim = 1024\n",
    "text_blstm_dim = 256\n",
    "speech_enc_name = \"wavlm\"\n",
    "speech_enc_dim = 768\n",
    "speech_blstm_dim = 256\n",
    "rep_dim = 512\n",
    "sub_dim = 0\n",
    "n_sub = 1\n",
    "ckpt_pth = f'{current_base_path}/CLAPS/pretrained/7d/cp_claps_blstm_m_50k_v3/cp_0045000'\n",
    "project_dir = \"cp_claps\"\n",
    "\n",
    "a = argparse.Namespace(\n",
    "    sr=sr,\n",
    "    text_enc_name=text_enc_name,\n",
    "    text_enc_dim=text_enc_dim,\n",
    "    text_blstm_dim=text_blstm_dim,\n",
    "    speech_enc_name=speech_enc_name,\n",
    "    speech_enc_dim=speech_enc_dim,\n",
    "    speech_blstm_dim=speech_blstm_dim,\n",
    "    rep_dim=rep_dim,\n",
    "    sub_dim=sub_dim,\n",
    "    n_sub=n_sub,\n",
    "    ckpt_pth=ckpt_pth,\n",
    "    project_dir=project_dir\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoModelForSeq2SeqLMWithValueHead.from_pretrained(model_checkpoint, return_dict=True)\n",
    "ar_model = BartForConditionalGeneration.from_pretrained(ar_checkpoint)\n",
    "ar_tokenizer = AutoTokenizer.from_pretrained(ar_checkpoint)\n",
    "# ar_tokenizer.pad_token = ar_tokenizer.eos_token\n",
    "nar_model = NARBartForConditionalGeneration.from_pretrained(nar_checkpoint)\n",
    "nar_tokenizer = AutoTokenizer.from_pretrained(nar_checkpoint)\n",
    "clap_model, accelerator = load_model(a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logging Start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_path = f'{model_output_dir}/log_training.log'\n",
    "print(f\"Logging to: {log_path}\")\n",
    "\n",
    "# Set up logging\n",
    "logging.basicConfig(\n",
    "    filename=log_path,\n",
    "    filemode='a',\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s',\n",
    "    level=logging.INFO\n",
    ")\n",
    "\n",
    "# Log Configuration Parameters\n",
    "logging.info(\n",
    "    \"\\n========== Configuration Parameters ==========\\n\"\n",
    "    f\"Model Configuration:\\n\"\n",
    "    f\"  ASR Model Size           : {asr_model_size}\\n\"\n",
    "    f\"  AR Model Checkpoint      : {ar_checkpoint}\\n\"\n",
    "    f\"  NAR Model Checkpoint     : {nar_checkpoint}\\n\"\n",
    "    f\"  Device                   : {device}\\n\"\n",
    "    f\"\\nTraining Configuration:\\n\"\n",
    "    f\"  Number of Iterations     : {num_iterations}\\n\"\n",
    "    f\"  Data Size Per Iteration  : {data_size_per_iteration}\\n\"\n",
    "    f\"  Sample Size              : {sample_size}\\n\"\n",
    "    f\"  Learning Rate            : {learning_rate}\\n\"\n",
    "    f\"  Beta                     : {beta}\\n\"\n",
    "    f\"  Training Epochs          : {num_train_epochs}\\n\"\n",
    "    f\"  Max Sequence Length      : {max_length}\\n\"\n",
    "    f\"  Max Prompt Length        : {max_prompt_length}\\n\"\n",
    "    f\"  Max Target Length        : {max_target_length}\\n\"\n",
    "    f\"  Per Device Batch Size    : {per_device_train_batch_size}\\n\"\n",
    "    f\"  Gradient Accumulation    : {gradient_accumulation_steps}\\n\"\n",
    "    f\"  Random Seed              : {seed}\\n\"\n",
    "    # f\"\\nPaths:\\n\"\n",
    "    # f\"  Model Output Directory   : {model_output_dir}\\n\"\n",
    "    # f\"  Agent Output Directory   : {agent_output_dir}\\n\"\n",
    "    # f\"  Base Path                : {current_base_path}\\n\"\n",
    "    f\"\\nEvaluation Configuration:\\n\"\n",
    "    f\"  Train Data Length        : {eval_train_data_len}\\n\"\n",
    "    f\"  Test Data Length         : {eval_test_data_len}\\n\"\n",
    "    f\"  Train Indices            : {eval_train_indices}\\n\"\n",
    "    f\"  Test Indices             : {eval_test_indices}\\n\"\n",
    "    f\"  Train Evaluation Enabled : {eval_train}\\n\"\n",
    "    f\"  Test Evaluation Enabled  : {eval_test}\\n\"\n",
    "    f\"  Number of Evaluations    : {num_eval}\\n\"\n",
    "    f\"==============================================\"\n",
    ")\n",
    "# Log Evaluation Data Details\n",
    "if eval_train or eval_test:\n",
    "    output_lines = []\n",
    "\n",
    "    if eval_train:\n",
    "        output_lines.append(\"\\n========== Train Evaluation Data ==========\")\n",
    "        for i in eval_train_indices:\n",
    "            output_lines.append(f\"  Train Index {i:4d} -> Instruction: '{original_instruction[i]}' -> Transcription: '{original_transcription[i]}'\")\n",
    "        output_lines.append(\"===========================================\")\n",
    "\n",
    "    if eval_test:\n",
    "        output_lines.append(\"\\n========== Test Evaluation Data ==========\")\n",
    "        for i in eval_test_indices:\n",
    "            output_lines.append(f\"  Test Index {i:4d}  -> Instruction: '{original_instruction[i]}' -> Transcription: '{original_transcription[i]}'\")\n",
    "        output_lines.append(\"===========================================\")\n",
    "\n",
    "    logging.info(\"\\n\" + \"\\n\".join(output_lines))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initial Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "# Start time\n",
    "total_start_time = time.time()\n",
    "\n",
    "# Prepare data for training\n",
    "if train_selected_indices:\n",
    "    batch_src_encodec = [original_src_encodec[i] for i in train_selected_indices]\n",
    "    batch_instruction = [original_instruction[i] for i in train_selected_indices]\n",
    "    batch_transcription = [original_transcription[i] for i in train_selected_indices]\n",
    "    logging.info(f\"Processing data from selected indices: {train_selected_indices}\")\n",
    "else:\n",
    "    start_idx, end_idx = 0, data_size_per_iteration\n",
    "    batch_src_encodec = original_src_encodec[start_idx:end_idx]\n",
    "    batch_instruction = original_instruction[start_idx:end_idx]\n",
    "    batch_transcription = original_transcription[start_idx:end_idx]\n",
    "    logging.info(f\"Processing data from index {start_idx} to {end_idx}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "\n",
    "import os\n",
    "os.environ[\"WANDB__SERVICE\"] = \"disabled\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Start training iterations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "disable_tqdm = not os.isatty(1)\n",
    "\n",
    "def evaluate_iteration(eval_type, iteration, eval_indices, eval_data_len, is_baseline=False):\n",
    "    metrics, rewards, all_claps_rewards, all_asr_rewards = eval_dpo_claps_asr_batch(\n",
    "        nar_model=nar_model,\n",
    "        ar_tokenizer=ar_tokenizer,\n",
    "        nar_tokenizer=nar_tokenizer,\n",
    "        trained_model=model,\n",
    "        args_predict=args_predict,\n",
    "        all_src_encodec=original_src_encodec,\n",
    "        all_instruction=original_instruction,\n",
    "        all_ground_truth=original_transcription,\n",
    "        iteration=iteration,\n",
    "        num_evaluations=num_eval,\n",
    "        eval_data_len=eval_data_len,\n",
    "        selected_indices=eval_indices,\n",
    "        device=device,\n",
    "        clap_model=clap_model,\n",
    "        asr_model=asr_model,\n",
    "        accelerator=accelerator\n",
    "    )\n",
    "\n",
    "    prefix = \"Baseline/\" if is_baseline else f\"Eval/{eval_type}_\"\n",
    "    step = 0 if is_baseline else iteration\n",
    "\n",
    "    logging.info(f\"{'Baseline' if is_baseline else 'Trained Model'} Iteration {iteration} {eval_type} Set Evaluation:\")\n",
    "    logging.info(f\"EVAL: Cosine_Sim metrics {eval_type} Set for iteration {iteration}: {metrics}\")\n",
    "    logging.info(f\"EVAL: Cosine_Sim score {eval_type} Set for iteration {iteration}: {rewards}\")\n",
    "    \n",
    "    # Print and log ASR scores for each data point\n",
    "    # valid_idx = []\n",
    "    # for idx, asr_reward_group in enumerate(all_asr_rewards):\n",
    "    #     # Print those ASR average score > 0.5\n",
    "    #     if np.mean(asr_reward_group) > 0.5:\n",
    "    #         logging.info(f\"ASR score for data index {eval_indices[idx]}: {asr_reward_group}\")\n",
    "    #         valid_idx.append(original_index[eval_indices[idx]])\n",
    "            \n",
    "    # logging.info(f\"Valid Index: {valid_idx}\")\n",
    "\n",
    "    reward_list = [np.mean([r for r in reward_group if r is not None]) if reward_group else None for reward_group in rewards]\n",
    "    filtered_reward_list = [r for r in reward_list if r is not None]\n",
    "    avg_reward = np.mean(filtered_reward_list) if filtered_reward_list else None\n",
    "    avg_claps_rewards = np.mean(all_claps_rewards)\n",
    "    avg_asr_rewards = np.mean(all_asr_rewards)\n",
    "\n",
    "    logging.info(f\"EVAL: {'Baseline' if is_baseline else 'Trained model'} Cosine_Sim score list on {eval_type} set: {reward_list}\")\n",
    "    logging.info(f\"EVAL: {'Baseline' if is_baseline else 'Trained model'} average Cosine_Sim score on {eval_type} set: {avg_reward}\")\n",
    "    logging.info(f\"EVAL: {'Baseline' if is_baseline else 'Trained model'} average CLAPS rewards on {eval_type} set: {avg_claps_rewards}\")\n",
    "    logging.info(f\"EVAL: {'Baseline' if is_baseline else 'Trained model'} average ASR rewards on {eval_type} set: {avg_asr_rewards}\")\n",
    "\n",
    "    # Log rewards to TensorBoard\n",
    "    if avg_reward is not None:\n",
    "        writer.add_scalar(f\"{prefix}Avg_Cosine_Sim\", avg_reward, step)\n",
    "    if avg_claps_rewards is not None:\n",
    "        writer.add_scalar(f\"{prefix}Avg_CLAPS_Reward\", avg_claps_rewards, step)\n",
    "    if avg_asr_rewards is not None:\n",
    "        writer.add_scalar(f\"{prefix}Avg_ASR_Reward\", avg_asr_rewards, step)\n",
    "        \n",
    "    return avg_reward\n",
    "        \n",
    "logging.info(\"Evaluating initial model performance before training begins.\")\n",
    "\n",
    "if eval_train:\n",
    "    avg_reward = evaluate_iteration(\"Train\", iteration=-1, eval_indices=eval_train_indices, eval_data_len=eval_train_data_len, is_baseline=True)\n",
    "\n",
    "if eval_test:\n",
    "    evaluate_iteration(\"Test\", iteration=-1, eval_indices=eval_test_indices, eval_data_len=eval_test_data_len, is_baseline=True)\n",
    "\n",
    "prev_eval_avg = avg_reward\n",
    "for iteration in tqdm(range(num_iterations), desc=\"Training Iterations\", disable=disable_tqdm):\n",
    "    logging.info(f\"-----------Starting iteration {iteration}-----------\")\n",
    "\n",
    "    resume = False\n",
    "\n",
    "    model_checkpoint, chosen_rewards, rejected_rewards = train_iteration(\n",
    "        model=model,\n",
    "        model_checkpoint=model_checkpoint,\n",
    "        iteration=iteration,\n",
    "        data_size=data_size_per_iteration,\n",
    "        sample_size=sample_size,\n",
    "        ar_model=ar_model,\n",
    "        ar_tokenizer=ar_tokenizer,\n",
    "        nar_model=nar_model,\n",
    "        nar_tokenizer=nar_tokenizer,\n",
    "        all_src_encodec=batch_src_encodec,\n",
    "        all_instruction=batch_instruction,\n",
    "        all_transcription=batch_transcription,\n",
    "        args_predict=args_predict,\n",
    "        agent_output_dir=agent_output_dir,\n",
    "        model_output_dir_base=model_output_dir,\n",
    "        temperature=1.0,\n",
    "        beta=beta,\n",
    "        base_path=current_base_path,\n",
    "        resume_from_checkpoint=resume,\n",
    "        learning_rate=learning_rate,\n",
    "        num_train_epochs=num_train_epochs,\n",
    "        max_length=max_length,\n",
    "        max_prompt_length=max_prompt_length,\n",
    "        max_target_length=max_target_length,\n",
    "        per_device_train_batch_size=per_device_train_batch_size,\n",
    "        gradient_accumulation_steps=gradient_accumulation_steps,\n",
    "        seed=seed,\n",
    "        clap_model=clap_model,\n",
    "        asr_model=asr_model,\n",
    "        accelerator=accelerator,\n",
    "        prev_eval_avg=prev_eval_avg\n",
    "    )\n",
    "\n",
    "    logging.info(f\"Chosen rewards for iteration {iteration}: {chosen_rewards}\")\n",
    "    logging.info(f\"Rejected rewards for iteration {iteration}: {rejected_rewards}\")\n",
    "    \n",
    "    # Log chosen and rejected rewards to TensorBoard\n",
    "    if chosen_rewards:\n",
    "        writer.add_scalar(\"Train/Avg_Chosen_Reward\", np.mean(chosen_rewards), iteration)\n",
    "    if rejected_rewards:\n",
    "        writer.add_scalar(\"Train/Avg_Rejected_Reward\", np.mean(rejected_rewards), iteration)\n",
    "\n",
    "\n",
    "    if (iteration + 1) % eval_frequency == 0:\n",
    "        if eval_train:\n",
    "            prev_eval_avg = evaluate_iteration(\"Train\", iteration, eval_train_indices, eval_train_data_len)\n",
    "        if eval_test:\n",
    "            evaluate_iteration(\"Test\", iteration, eval_test_indices, eval_test_data_len)\n",
    "\n",
    "    logging.info(f\"-----------Finished iteration {iteration}-----------\")\n",
    "\n",
    "total_end_time = time.time()\n",
    "total_time_taken = total_end_time - total_start_time\n",
    "logging.info(f\"Total time taken for the entire process: {total_time_taken:.2f} seconds\")\n",
    "# Close TensorBoard SummaryWriter\n",
    "writer.close()\n",
    "\n",
    "# Transcriptions: 'neighboring fields', 'but i will be in a minute', 'will you', 'why is it', 'what more do you want', 'why not', 'the wind wakened me', 'yes', 'hush pearl hush', 'good night husband', 'what music', 'shut up will you', 'here he is', 'that was strange'\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dpo",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
