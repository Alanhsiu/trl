{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/b0990106x/miniconda3/envs/dpo/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/home/b0990106x/.local/lib/python3.10/site-packages/s3prl/upstream/byol_s/byol_a/common.py:20: UserWarning: torchaudio._backend.set_audio_backend has been deprecated. With dispatcher enabled, this function is no-op. You can remove the function call.\n",
      "  torchaudio.set_audio_backend(\"sox_io\")\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append(\"/work/b0990106x/trl/vc\")\n",
    "sys.path.append('/work/b0990106x/trl/CLAPS')\n",
    "\n",
    "import importlib\n",
    "import torch\n",
    "import os\n",
    "import math\n",
    "import numpy as np\n",
    "import random\n",
    "import time\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "from types import SimpleNamespace\n",
    "from datetime import datetime\n",
    "from typing import List, Tuple\n",
    "\n",
    "import vc\n",
    "importlib.reload(vc)\n",
    "from vc.trainer_encodec_vc_inference import (\n",
    "    pack_inputs_v2,\n",
    "    get_ar_prediction_audio_batch\n",
    ")\n",
    "from vc.encodec_model.nar_bart_model import NARBartForConditionalGeneration\n",
    "\n",
    "from transformers import BartForConditionalGeneration, AutoTokenizer\n",
    "from trl import (\n",
    "    DPOTrainer,\n",
    "    DPOConfig,\n",
    "    AutoModelForSeq2SeqLMWithValueHead,\n",
    "    create_reference_model\n",
    ")\n",
    "from datasets import Dataset\n",
    "\n",
    "from dpo_eval import (\n",
    "    get_reward_claps,\n",
    "    get_reward_asr,\n",
    "    eval_dpo_claps_asr_batch,\n",
    "    convert_array_to_tensor_format\n",
    ")\n",
    "from CLAPS.inference import load_model\n",
    "\n",
    "import argparse\n",
    "\n",
    "from faster_whisper import WhisperModel\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Utility Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seed(seed):\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(seed)\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "import shutil\n",
    "from pathlib import Path\n",
    "import soundfile as sf\n",
    "\n",
    "def generate_output_batch(\n",
    "        ar_model, \n",
    "        nar_model, \n",
    "        ar_tokenizer, \n",
    "        nar_tokenizer, \n",
    "        clap_model,\n",
    "        asr_model,\n",
    "        accelerator,\n",
    "        src_encodec: list, \n",
    "        instruction: list, \n",
    "        args_predict: SimpleNamespace, \n",
    "        episode_counter: int = 0, \n",
    "        base_path: str = \"/work/b0990106x/trl\", \n",
    "        temperature: float = 1.0\n",
    ") -> tuple[list[float], list[str]]:\n",
    "    audio_list, decode_ar_list = get_ar_prediction_audio_batch(\n",
    "        args_predict, ar_model, nar_model, ar_tokenizer, nar_tokenizer, src_encodec, instruction, episode_counter, temperature=temperature\n",
    "    )\n",
    "    reward_list, tokenized_decode_ar_list = [], []\n",
    "    valid_audio_paths = []\n",
    "\n",
    "    temp_dir_path = Path(\"/dev/shm/temp_audio_files\")\n",
    "    temp_dir_path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    for i, audio in enumerate(audio_list):\n",
    "        if audio is not None:\n",
    "            tensor_audio = convert_array_to_tensor_format(audio)\n",
    "            if tensor_audio[0].shape[0] == 1:\n",
    "                tensor_audio[0] = tensor_audio[0].squeeze(0)\n",
    "            clap_reward = get_reward_claps(\n",
    "                clap_model=clap_model, accelerator=accelerator, prompts=instruction[i], wavs=tensor_audio\n",
    "            )\n",
    "            \n",
    "            output_path_ckpt = temp_dir_path / f\"generate_{episode_counter}_item_{i}.wav\"\n",
    "            sf.write(output_path_ckpt, np.ravel(audio), samplerate=24000)\n",
    "            asr_reward = get_reward_asr(file_path=output_path_ckpt, asr_model=asr_model)\n",
    "\n",
    "            final_reward = clap_reward * asr_reward\n",
    "            reward_list.append(final_reward)\n",
    "            valid_audio_paths.append(output_path_ckpt)\n",
    "        else:\n",
    "            reward_list.append(0)\n",
    "\n",
    "    shutil.rmtree(temp_dir_path)\n",
    "\n",
    "    for decode_ar in decode_ar_list:\n",
    "        list_decode_ar = decode_ar.flatten().tolist()\n",
    "        filtered_decode_ar_list = list_decode_ar[2:-1]\n",
    "        decode_ar_tokens = ar_tokenizer.convert_ids_to_tokens(filtered_decode_ar_list)\n",
    "        tokenized_decode_ar = ar_tokenizer.convert_tokens_to_string(decode_ar_tokens)\n",
    "        tokenized_decode_ar_list.append(tokenized_decode_ar)\n",
    "\n",
    "    return reward_list, tokenized_decode_ar_list\n",
    "\n",
    "def extract_data_from_json(file_path: str) -> Tuple[List[list], List[str], List[list]]:\n",
    "    with open(file_path, 'r') as f:\n",
    "        data = json.load(f)\n",
    "\n",
    "    all_src_encodec = [item[\"src_encodec\"] for item in data]\n",
    "    all_instruction = [item[\"instruction\"] for item in data]\n",
    "\n",
    "    return all_src_encodec, all_instruction\n",
    "\n",
    "def train_model(\n",
    "        model,\n",
    "        model_ref,\n",
    "        ar_tokenizer,\n",
    "        train_dataset: Dataset,\n",
    "        val_dataset: Dataset,\n",
    "        model_output_dir: str,\n",
    "        beta: float,\n",
    "        resume_from_checkpoint: bool,\n",
    "        model_checkpoint: str,\n",
    "        learning_rate: float = 5e-07,\n",
    "        num_train_epochs: int = 200,\n",
    "        max_length: int = 1024*9,\n",
    "        max_prompt_length: int = 1024*9,\n",
    "        max_target_length: int = 1024*9,\n",
    "        per_device_train_batch_size: int = 1,\n",
    "        gradient_accumulation_steps: int = 1,\n",
    "        seed: int = 42\n",
    ") -> None:\n",
    "    training_args = DPOConfig(\n",
    "        beta=beta,\n",
    "        output_dir=model_output_dir,\n",
    "        resume_from_checkpoint=model_checkpoint if resume_from_checkpoint else None,\n",
    "        seed=seed,\n",
    "        per_device_train_batch_size=per_device_train_batch_size,\n",
    "        num_train_epochs=num_train_epochs,\n",
    "        gradient_accumulation_steps=gradient_accumulation_steps,\n",
    "        learning_rate=learning_rate,\n",
    "        max_length=max_length,\n",
    "        max_prompt_length=max_prompt_length,\n",
    "        max_target_length=max_target_length,\n",
    "        evaluation_strategy=\"steps\",\n",
    "        save_steps=5000,\n",
    "        logging_dir=f\"{model_output_dir}/logs\"\n",
    "    )\n",
    "    \n",
    "    trainer = DPOTrainer(\n",
    "        model=model,\n",
    "        ref_model=model_ref,\n",
    "        args=training_args,\n",
    "        tokenizer=ar_tokenizer,\n",
    "        train_dataset=train_dataset,\n",
    "        eval_dataset=val_dataset,\n",
    "    )\n",
    "    trainer.train()\n",
    "\n",
    "    model.config.to_json_file(f\"{model_output_dir}/config.json\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "def process_data_batch(\n",
    "    sample_size: int, \n",
    "    ar_model, \n",
    "    nar_model, \n",
    "    ar_tokenizer, \n",
    "    nar_tokenizer, \n",
    "    clap_model,\n",
    "    asr_model,\n",
    "    accelerator,\n",
    "    selected_src_encodec: List[list], \n",
    "    selected_instruction: List[str],\n",
    "    args_predict, \n",
    "    base_path: str = \"/work/b0990106x/trl\", \n",
    "    temperature: float = 1.0, \n",
    "    iteration: int = 0,\n",
    "    prev_eval_avg: float = 0,\n",
    "    strategy: str = \"top_bottom_percent\"  # Default to original strategy. Options: \"max_min\", \"top_bottom_percent\", \"above_below_average\", \"above_prev_eval\"\n",
    ") -> Tuple[List[str], List[str], List[str], List[float], List[float], List[float]]:\n",
    "    # Ensure sample size is valid\n",
    "    if sample_size < 2:\n",
    "        raise ValueError(\"Parameter 'sample_size' must be greater than 1.\")\n",
    "\n",
    "    chosen, rejected, prompts, chosen_rewards, rejected_rewards, average_rewards = [], [], [], [], [], []\n",
    "\n",
    "    disable_tqdm = not os.isatty(1)\n",
    "    for i in tqdm(range(len(selected_src_encodec)), desc=\"Processing Data\", disable=disable_tqdm):\n",
    "        rewards, tokenized_outputs = [], []\n",
    "        size_of_packed_input = (\n",
    "            len(selected_src_encodec[i][0]) +\n",
    "            len(ar_tokenizer(selected_instruction[i])[\"input_ids\"][1:-1]) +\n",
    "            3\n",
    "        )\n",
    "        if 4 < size_of_packed_input <= 1024:\n",
    "            selected_src_encodec_list = [selected_src_encodec[i]] * sample_size\n",
    "            selected_instruction_list = [selected_instruction[i]] * sample_size\n",
    "            rewards, tokenized_outputs = generate_output_batch(\n",
    "                ar_model=ar_model, \n",
    "                nar_model=nar_model, \n",
    "                ar_tokenizer=ar_tokenizer, \n",
    "                nar_tokenizer=nar_tokenizer,\n",
    "                src_encodec=selected_src_encodec_list,\n",
    "                instruction=selected_instruction_list, \n",
    "                clap_model=clap_model,\n",
    "                asr_model=asr_model,\n",
    "                accelerator=accelerator,\n",
    "                args_predict=args_predict,\n",
    "                episode_counter=f\"data_{i}\",\n",
    "                base_path=base_path, \n",
    "                temperature=temperature\n",
    "            )\n",
    "\n",
    "        valid_rewards = [r for r in rewards if r is not None]\n",
    "        valid_outputs = [tokenized_outputs[j] for j in range(len(rewards)) if rewards[j] is not None]\n",
    "\n",
    "        if len(valid_rewards) >= 2:\n",
    "            average_reward = np.mean(valid_rewards)\n",
    "            print(f\"Average reward for data index {i}: {average_reward}\")\n",
    "\n",
    "            if strategy == \"max_min\":\n",
    "                # Original max-min strategy\n",
    "                max_reward_index = np.argmax(valid_rewards)\n",
    "                min_reward_index = np.argmin(valid_rewards)\n",
    "                chosen_outputs = [valid_outputs[max_reward_index]]\n",
    "                rejected_outputs = [valid_outputs[min_reward_index]]\n",
    "                chosen_rewards_part = [valid_rewards[max_reward_index]]\n",
    "                rejected_rewards_part = [valid_rewards[min_reward_index]]\n",
    "\n",
    "            elif strategy == \"top_bottom_percent\":\n",
    "                # Select top and bottom 20% of rewards\n",
    "                twenty_percent_num = max(1, math.ceil(len(valid_rewards) * 0.2))\n",
    "                max_indices = np.argsort(valid_rewards)[-twenty_percent_num:]\n",
    "                min_indices = np.argsort(valid_rewards)[:twenty_percent_num]\n",
    "\n",
    "                chosen_outputs = [valid_outputs[j] for j in max_indices]\n",
    "                rejected_outputs = [valid_outputs[j] for j in min_indices]\n",
    "                chosen_rewards_part = [valid_rewards[j] for j in max_indices]\n",
    "                rejected_rewards_part = [valid_rewards[j] for j in min_indices]\n",
    "\n",
    "            elif strategy == \"above_below_average\":\n",
    "                # Select rewards above and below the average\n",
    "                threshold = 0.05\n",
    "                chosen_outputs = [valid_outputs[j] for j in range(len(valid_rewards)) if valid_rewards[j] > average_reward + threshold]\n",
    "                rejected_outputs = [valid_outputs[j] for j in range(len(valid_rewards)) if valid_rewards[j] < average_reward - threshold]\n",
    "\n",
    "                # Sort and trim to ensure balanced chosen and rejected outputs\n",
    "                chosen_outputs = [x for _, x in sorted(zip(valid_rewards, chosen_outputs), reverse=True)]\n",
    "                rejected_outputs = [x for _, x in sorted(zip(valid_rewards, rejected_outputs))]\n",
    "\n",
    "                min_length = min(len(chosen_outputs), len(rejected_outputs))\n",
    "                chosen_outputs = chosen_outputs[:min_length]\n",
    "                rejected_outputs = rejected_outputs[:min_length]\n",
    "\n",
    "                chosen_rewards_part = [valid_rewards[j] for j in range(len(valid_rewards)) if valid_rewards[j] > average_reward][:min_length]\n",
    "                rejected_rewards_part = [valid_rewards[j] for j in range(len(valid_rewards)) if valid_rewards[j] < average_reward][:min_length]\n",
    "\n",
    "            elif strategy == \"above_prev_eval\":\n",
    "                # Select rewards above and below a previous evaluation average\n",
    "                chosen_outputs = [valid_outputs[j] for j in range(len(valid_rewards)) if valid_rewards[j] > prev_eval_avg]\n",
    "                rejected_outputs = [valid_outputs[j] for j in range(len(valid_rewards)) if valid_rewards[j] < prev_eval_avg]\n",
    "\n",
    "                # Sort and trim\n",
    "                chosen_outputs = [x for _, x in sorted(zip(valid_rewards, chosen_outputs), reverse=True)]\n",
    "                rejected_outputs = [x for _, x in sorted(zip(valid_rewards, rejected_outputs))]\n",
    "\n",
    "                min_length = min(len(chosen_outputs), len(rejected_outputs))\n",
    "                if min_length == 0:\n",
    "                    chosen_outputs = [valid_outputs[np.argmax(valid_rewards)]]\n",
    "                    rejected_outputs = [valid_outputs[np.argmin(valid_rewards)]]\n",
    "                    chosen_rewards_part = [valid_rewards[np.argmax(valid_rewards)]]\n",
    "                    rejected_rewards_part = [valid_rewards[np.argmin(valid_rewards)]]\n",
    "                else:\n",
    "                    chosen_outputs = chosen_outputs[:min_length]\n",
    "                    rejected_outputs = rejected_outputs[:min_length]\n",
    "                    chosen_rewards_part = [valid_rewards[j] for j in range(len(valid_rewards)) if valid_rewards[j] > prev_eval_avg][:min_length]\n",
    "                    rejected_rewards_part = [valid_rewards[j] for j in range(len(valid_rewards)) if valid_rewards[j] < prev_eval_avg][:min_length]\n",
    "\n",
    "            obs_input = pack_inputs_v2(ar_tokenizer, selected_src_encodec[i], selected_instruction[i])\n",
    "            tokenize_input = ar_tokenizer.convert_ids_to_tokens(obs_input)\n",
    "            tokenize_input_str = ar_tokenizer.convert_tokens_to_string(tokenize_input)\n",
    "            prompts.extend([tokenize_input_str] * len(chosen_outputs))\n",
    "            average_rewards.append(average_reward)\n",
    "\n",
    "            chosen.extend(chosen_outputs)\n",
    "            rejected.extend(rejected_outputs)\n",
    "            chosen_rewards.extend(chosen_rewards_part)\n",
    "            rejected_rewards.extend(rejected_rewards_part)\n",
    "        else:\n",
    "            print(f\"Not enough valid rewards for data index {i}\")\n",
    "\n",
    "    if len(selected_src_encodec) == 1:\n",
    "        chosen *= 2\n",
    "        rejected *= 2\n",
    "        prompts *= 2\n",
    "        chosen_rewards *= 2\n",
    "        rejected_rewards *= 2\n",
    "        average_rewards *= 2    \n",
    "\n",
    "    return chosen, rejected, prompts, chosen_rewards, rejected_rewards, average_rewards\n",
    "\n",
    "\n",
    "def generate_data(ar_model, \n",
    "                  ar_tokenizer, \n",
    "                  nar_model, \n",
    "                  nar_tokenizer, \n",
    "                  clap_model,\n",
    "                  asr_model,\n",
    "                  accelerator,\n",
    "                  selected_src_encodec: List[list], \n",
    "                  selected_instruction: List[str],\n",
    "                  args_predict: SimpleNamespace, \n",
    "                  sample_size: int, \n",
    "                  iteration: int, \n",
    "                  agent_output_dir: str, \n",
    "                  base_path: str = \"/work/b0990106x/trl\", \n",
    "                  temperature: float = 1.0\n",
    ") -> Tuple[dict, List[float], List[float]]:\n",
    "    \"\"\"\n",
    "    Generates data for the dataset and saves info to a JSON file.\n",
    "    Returns:\n",
    "        tuple:\n",
    "            data_for_dataset (dict): A dictionary containing the data for the dataset.\n",
    "            chosen_rewards (List[float]): A list of rewards for the chosen outputs.\n",
    "            rejected_rewards (List[float]): A list of rewards for the rejected outputs.\n",
    "    \"\"\"\n",
    "    chosen, rejected, prompts, chosen_rewards, rejected_rewards, average_rewards = process_data_batch(\n",
    "        sample_size=sample_size,\n",
    "        ar_model=ar_model,\n",
    "        nar_model=nar_model,\n",
    "        ar_tokenizer=ar_tokenizer,\n",
    "        nar_tokenizer=nar_tokenizer,\n",
    "        selected_src_encodec=selected_src_encodec,\n",
    "        selected_instruction=selected_instruction,\n",
    "        args_predict=args_predict,\n",
    "        base_path=base_path,\n",
    "        temperature=temperature,\n",
    "        iteration = iteration,\n",
    "        clap_model=clap_model,\n",
    "        asr_model=asr_model,\n",
    "        accelerator=accelerator\n",
    "    )\n",
    "\n",
    "    data = {\n",
    "        \"prompt\": prompts,\n",
    "        \"chosen\": chosen,\n",
    "        \"rejected\": rejected,\n",
    "        \"chosen_rewards\": chosen_rewards,\n",
    "        \"rejected_rewards\": rejected_rewards,\n",
    "        \"average_rewards\": average_rewards\n",
    "    }\n",
    "\n",
    "    with open(f\"{agent_output_dir}/data_iter_{iteration}.json\", \"w\") as outfile:\n",
    "        json.dump(data, outfile, indent=4)\n",
    "\n",
    "    data_for_dataset = {key: data[key] for key in [\"prompt\", \"chosen\", \"rejected\"]}\n",
    "\n",
    "    return data_for_dataset, chosen_rewards, rejected_rewards\n",
    "\n",
    "def train_iteration(model, \n",
    "                    model_checkpoint,\n",
    "                    iteration, \n",
    "                    data_size, \n",
    "                    sample_size, \n",
    "                    ar_model, \n",
    "                    ar_tokenizer,\n",
    "                    nar_model, \n",
    "                    nar_tokenizer,\n",
    "                    all_src_encodec, \n",
    "                    all_instruction, \n",
    "                    args_predict, \n",
    "                    agent_output_dir,\n",
    "                    model_output_dir_base, \n",
    "                    clap_model,\n",
    "                    asr_model,\n",
    "                    accelerator,\n",
    "                    beta = 0.1, \n",
    "                    temperature = 1.0,\n",
    "                    base_path=\"/work/b0990106x/trl\",\n",
    "                    resume_from_checkpoint = False,\n",
    "                    learning_rate = 5e-07,\n",
    "                    num_train_epochs = 100,\n",
    "                    max_length = 1024*9,\n",
    "                    max_prompt_length = 1024*9,\n",
    "                    max_target_length = 1024*9,\n",
    "                    per_device_train_batch_size = 1,\n",
    "                    gradient_accumulation_steps = 1,\n",
    "                    seed = 42,\n",
    "):\n",
    "    \"\"\"\n",
    "    Executes one training iteration: generates data, trains the model, and saves the output.\n",
    "    \"\"\"\n",
    "\n",
    "    selected_src_encodec = all_src_encodec[:data_size]\n",
    "    selected_instruction = all_instruction[:data_size]\n",
    "\n",
    "    data_for_dataset, chosen_rewards, rejected_rewards = generate_data(ar_model=model,\n",
    "                                                                        ar_tokenizer=ar_tokenizer,\n",
    "                                                                        nar_model=nar_model,\n",
    "                                                                        nar_tokenizer=nar_tokenizer,\n",
    "                                                                        selected_src_encodec=selected_src_encodec,\n",
    "                                                                        selected_instruction=selected_instruction,\n",
    "                                                                        args_predict=args_predict,\n",
    "                                                                        sample_size=sample_size,\n",
    "                                                                        iteration=iteration,\n",
    "                                                                        agent_output_dir=agent_output_dir,\n",
    "                                                                        base_path=base_path,\n",
    "                                                                        temperature=temperature,\n",
    "                                                                        clap_model=clap_model,\n",
    "                                                                        asr_model=asr_model,\n",
    "                                                                        accelerator=accelerator)\n",
    "\n",
    "    dataset = Dataset.from_dict(data_for_dataset)\n",
    "    dataset_dict = dataset.train_test_split(test_size=0.1, shuffle=True, seed=seed)\n",
    "    train_dataset = dataset_dict[\"train\"]\n",
    "    val_dataset = dataset_dict[\"test\"]\n",
    "\n",
    "    model_output_dir = f\"{model_output_dir_base}/iter_{iteration}\"\n",
    "    os.makedirs(model_output_dir, exist_ok=True)\n",
    "\n",
    "    model_ref = create_reference_model(model)\n",
    "    \n",
    "    train_model(model=model,\n",
    "                model_ref=model_ref,\n",
    "                ar_tokenizer=ar_tokenizer,\n",
    "                train_dataset=train_dataset,\n",
    "                val_dataset=val_dataset,\n",
    "                model_output_dir=model_output_dir,\n",
    "                beta=beta,\n",
    "                resume_from_checkpoint=resume_from_checkpoint,\n",
    "                model_checkpoint=model_checkpoint,\n",
    "                learning_rate = learning_rate,\n",
    "                num_train_epochs = num_train_epochs,\n",
    "                max_length = max_length,\n",
    "                max_prompt_length = max_prompt_length,\n",
    "                max_target_length = max_target_length,\n",
    "                per_device_train_batch_size = per_device_train_batch_size,\n",
    "                gradient_accumulation_steps = gradient_accumulation_steps,\n",
    "                seed = seed)\n",
    "\n",
    "    return f\"{model_output_dir}/dpo_model\", chosen_rewards, rejected_rewards"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "timestamp: 1116-1954\n",
      "length of all_src_encodec: 9254\n",
      "length of all_instruction: 9254\n"
     ]
    }
   ],
   "source": [
    "# Load data\n",
    "selected_src_encodec, selected_instruction = extract_data_from_json('dpo_data/src_encodec.json')\n",
    "\n",
    "# Define paths and device\n",
    "base_path = \"/work/b0990106x/trl\"\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# Timestamp\n",
    "now = datetime.now()\n",
    "ts = now.strftime(\"%m%d-%H%M\")\n",
    "print(\"timestamp:\", ts)\n",
    "\n",
    "# Output paths\n",
    "model_output_dir = os.path.join(base_path, \"model_output\", ts)\n",
    "agent_output_dir = os.path.join(base_path, \"output\", ts)\n",
    "os.makedirs(model_output_dir, exist_ok=True)\n",
    "os.makedirs(agent_output_dir, exist_ok=True)\n",
    "\n",
    "# Seed\n",
    "seed = 42\n",
    "\n",
    "# Arguments\n",
    "args_predict = SimpleNamespace(output_path=f\"{base_path}/output/{ts}/example.wav\", seed=seed, device=device)\n",
    "ar_checkpoint = \"lca0503/speech-chatgpt-base-ar-v2-epoch10-wotrans\"\n",
    "nar_checkpoint = \"lca0503/speech-chatgpt-base-nar-v2-epoch4-wotrans\"\n",
    "\n",
    "# Model and Iterations\n",
    "model_checkpoint = ar_checkpoint\n",
    "sample_size = 80\n",
    "num_iterations = 1000\n",
    "train_selected_indices = [8]\n",
    "data_size_per_iteration = len(train_selected_indices)\n",
    "\n",
    "# Training Configuration\n",
    "beta = 0.1\n",
    "learning_rate = 5e-07\n",
    "num_train_epochs = 3\n",
    "max_length = 1024 * 9\n",
    "max_prompt_length = 1024 * 9\n",
    "max_target_length = 1024 * 9\n",
    "per_device_train_batch_size = 8\n",
    "gradient_accumulation_steps = 1\n",
    "\n",
    "# Evaluation Configuration\n",
    "eval_train = False\n",
    "eval_test = True\n",
    "eval_train_indices = train_selected_indices\n",
    "eval_test_indices = train_selected_indices\n",
    "# eval_test_indices = random.sample(range(len(selected_src_encodec)), 5)\n",
    "eval_train_data_len = 1000\n",
    "eval_test_data_len = len(eval_test_indices)\n",
    "num_eval = 10\n",
    "eval_frequency = 1\n",
    "\n",
    "print(f\"length of all_src_encodec: {len(selected_src_encodec)}\")\n",
    "print(f\"length of all_instruction: {len(selected_instruction)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Detected kernel version 3.10.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n",
      "/home/b0990106x/.local/lib/python3.10/site-packages/huggingface_hub/file_download.py:1150: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "/home/b0990106x/.local/lib/python3.10/site-packages/s3prl/upstream/wavlm/expert.py:37: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(ckpt)\n",
      "/home/b0990106x/miniconda3/envs/dpo/lib/python3.10/site-packages/torch/nn/utils/weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n",
      "/home/b0990106x/miniconda3/envs/dpo/lib/python3.10/site-packages/torch/nn/functional.py:5849: UserWarning: Support for mismatched key_padding_mask and attn_mask is deprecated. Use same type for both instead.\n",
      "  warnings.warn(\n",
      "/work/b0990106x/trl/CLAPS/inference.py:33: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(checkpoint_path, map_location=device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded checkpoint from /work/b0990106x/trl/CLAPS/pretrained/7d/cp_claps_blstm_m_50k_v3/cp_0045000/pytorch_model.bin\n"
     ]
    }
   ],
   "source": [
    "# Configuration\n",
    "sr = 24000\n",
    "text_enc_name = \"google/flan-t5-large\"\n",
    "text_enc_dim = 1024\n",
    "text_blstm_dim = 256\n",
    "speech_enc_name = \"wavlm\"\n",
    "speech_enc_dim = 768\n",
    "speech_blstm_dim = 256\n",
    "rep_dim = 512\n",
    "sub_dim = 0\n",
    "n_sub = 1\n",
    "ckpt_pth = f'{base_path}/CLAPS/pretrained/7d/cp_claps_blstm_m_50k_v3/cp_0045000'\n",
    "project_dir = \"cp_claps\"\n",
    "\n",
    "# Argument Namespace\n",
    "a = argparse.Namespace(\n",
    "    sr=sr,\n",
    "    text_enc_name=text_enc_name,\n",
    "    text_enc_dim=text_enc_dim,\n",
    "    text_blstm_dim=text_blstm_dim,\n",
    "    speech_enc_name=speech_enc_name,\n",
    "    speech_enc_dim=speech_enc_dim,\n",
    "    speech_blstm_dim=speech_blstm_dim,\n",
    "    rep_dim=rep_dim,\n",
    "    sub_dim=sub_dim,\n",
    "    n_sub=n_sub,\n",
    "    ckpt_pth=ckpt_pth,\n",
    "    project_dir=project_dir\n",
    ")\n",
    "\n",
    "# Load CLAP model\n",
    "clap_model, accelerator = load_model(a)\n",
    "\n",
    "asr_model_size = \"tiny\"\n",
    "asr_model = WhisperModel(asr_model_size, device=\"cpu\", compute_type=\"int8\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_iterations: 1000\n",
      "data_size_per_iteration: 1\n",
      "sample_size: 80\n",
      "beta: 0.1\n",
      "learning_rate: 5e-07\n",
      "num_train_epochs: 3\n",
      "ar_checkpoint: lca0503/speech-chatgpt-base-ar-v2-epoch10-wotrans\n",
      "nar_checkpoint: lca0503/speech-chatgpt-base-nar-v2-epoch4-wotrans\n",
      "args_predict: namespace(output_path='/work/b0990106x/trl/output/1116-1954/example.wav', seed=42, device='cuda')\n",
      "model_output_dir: /work/b0990106x/trl/model_output/1116-1954\n",
      "agent_output_dir: /work/b0990106x/trl/output/1116-1954\n",
      "base_path: /work/b0990106x/trl\n",
      "device: cuda\n",
      "eval_train_data_len: 1000\n",
      "eval_test_data_len: 1\n",
      "eval_train_indices: [8]\n",
      "eval_test_indices: [8]\n",
      "eval_train: False\n",
      "eval_test: True\n",
      "num_eval: 10\n",
      "training idx 8: Significantly dampen the vibrations of the high notes.\n",
      "evaluation idx 8: Significantly dampen the vibrations of the high notes.\n"
     ]
    }
   ],
   "source": [
    "# Print configurations\n",
    "print(f\"num_iterations: {num_iterations}\")\n",
    "print(f\"data_size_per_iteration: {data_size_per_iteration}\")\n",
    "print(f\"sample_size: {sample_size}\")\n",
    "print(f\"beta: {beta}\")\n",
    "print(f\"learning_rate: {learning_rate}\")\n",
    "print(f\"num_train_epochs: {num_train_epochs}\")\n",
    "print(f\"ar_checkpoint: {ar_checkpoint}\")\n",
    "print(f\"nar_checkpoint: {nar_checkpoint}\")\n",
    "print(f\"args_predict: {args_predict}\")\n",
    "print(f\"model_output_dir: {model_output_dir}\")\n",
    "print(f\"agent_output_dir: {agent_output_dir}\")\n",
    "print(f\"base_path: {base_path}\")\n",
    "print(f\"device: {device}\")\n",
    "print(f\"eval_train_data_len: {eval_train_data_len}\")\n",
    "print(f\"eval_test_data_len: {eval_test_data_len}\")\n",
    "print(f\"eval_train_indices: {eval_train_indices}\")\n",
    "print(f\"eval_test_indices: {eval_test_indices}\")\n",
    "print(f\"eval_train: {eval_train}\")\n",
    "print(f\"eval_test: {eval_test}\")\n",
    "print(f\"num_eval: {num_eval}\")\n",
    "\n",
    "# Print training data\n",
    "for i in train_selected_indices:\n",
    "    print(f'training idx {i}: {selected_instruction[i]}')\n",
    "\n",
    "# Print evaluation data\n",
    "if eval_test:\n",
    "    for i in eval_test_indices:\n",
    "        print(f'evaluation idx {i}: {selected_instruction[i]}')\n",
    "\n",
    "if eval_train:\n",
    "    for i in eval_train_indices:\n",
    "        print(f'evaluation idx {i}: {selected_instruction[i]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/b0990106x/miniconda3/envs/dpo/lib/python3.10/site-packages/transformers/modeling_utils.py:460: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  return torch.load(checkpoint_file, map_location=\"cpu\")\n",
      "/work/b0990106x/trl/trl/models/modeling_base.py:328: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  state_dict = loading_func(filename if not use_safe else safe_filename, **load_kwargs)\n"
     ]
    }
   ],
   "source": [
    "model = AutoModelForSeq2SeqLMWithValueHead.from_pretrained(model_checkpoint, return_dict=True)\n",
    "ar_model = BartForConditionalGeneration.from_pretrained(ar_checkpoint)\n",
    "ar_tokenizer = AutoTokenizer.from_pretrained(ar_checkpoint)\n",
    "# ar_tokenizer.pad_token = ar_tokenizer.eos_token\n",
    "nar_model = NARBartForConditionalGeneration.from_pretrained(nar_checkpoint)\n",
    "nar_tokenizer = AutoTokenizer.from_pretrained(nar_checkpoint)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logging Start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logging to: /work/b0990106x/trl/model_output/1116-1954/log_training.log\n"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "\n",
    "log_path = f'{model_output_dir}/log_training.log'\n",
    "print(f\"Logging to: {log_path}\")\n",
    "\n",
    "# Set up logging\n",
    "logging.basicConfig(\n",
    "    filename=log_path,\n",
    "    filemode='a',\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s',\n",
    "    level=logging.INFO\n",
    ")\n",
    "\n",
    "logging.info(\n",
    "    f\"Parameters:\\n\"\n",
    "    f\"Prepare Data: sample_size: {sample_size}\\n\"\n",
    "    f\"Training: num_iterations: {num_iterations}\\n\"\n",
    "    f\"Training: data_size_per_iteration: {data_size_per_iteration}\\n\"\n",
    "    f\"Training: train_selected_indices: {train_selected_indices}\\n\"\n",
    "    f\"Training: beta: {beta}\\n\"\n",
    "    f\"Training: learning_rate: {learning_rate}\\n\"\n",
    "    f\"Training: num_train_epochs: {num_train_epochs}\\n\"\n",
    "    f\"Training: max_length: {max_length}\\n\"\n",
    "    f\"Training: max_prompt_length: {max_prompt_length}\\n\"\n",
    "    f\"Training: max_target_length: {max_target_length}\\n\"\n",
    "    f\"Training: per_device_train_batch_size: {per_device_train_batch_size}\\n\"\n",
    "    f\"Training: gradient_accumulation_steps: {gradient_accumulation_steps}\\n\"\n",
    "    f\"Training: seed: {seed}\\n\"\n",
    "    f\"Training: ar_checkpoint: {ar_checkpoint}\\n\"\n",
    "    f\"Training: nar_checkpoint: {nar_checkpoint}\\n\"\n",
    "    f\"Training: args_predict: {args_predict}\\n\"\n",
    "    f\"Training: model_output_dir: {model_output_dir}\\n\"\n",
    "    f\"Training: agent_output_dir: {agent_output_dir}\\n\"\n",
    "    f\"Training: base_path: {base_path}\\n\"\n",
    "    f\"Training: device: {device}\\n\"\n",
    "    f\"Evaluation: eval_train_data_len: {eval_train_data_len}\\n\"\n",
    "    f\"Evaluation: eval_test_data_len: {eval_test_data_len}\\n\"\n",
    "    f\"Evaluation: eval_train_indices: {eval_train_indices}\\n\"\n",
    "    f\"Evaluation: eval_test_indices: {eval_test_indices}\\n\"\n",
    "    f\"Evaluation: eval_train: {eval_train}\\n\"\n",
    "    f\"Evaluation: eval_test: {eval_test}\\n\"\n",
    "    f\"Evaluation: num_eval: {num_eval}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initial Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken for batch inference: 3.879110097885132 seconds\n",
      "Length of layer_list_batch:  10\n",
      "Time taken for batch audio synthesis: 39.482266664505005 seconds\n",
      "modified_text: margarine fields, reward: 0.50\n",
      "Claps reward: 0.23, ASR reward: 0.50, Final reward: 0.12\n",
      "modified_text: make the ring feel, reward: 0.00\n",
      "Claps reward: 0.20, ASR reward: 0.00, Final reward: 0.00\n",
      "modified_text: thanks for watching, reward: 0.00\n",
      "Claps reward: 0.20, ASR reward: 0.00, Final reward: 0.00\n",
      "modified_text: neighboring fields, reward: 1.00\n",
      "Claps reward: 0.15, ASR reward: 1.00, Final reward: 0.15\n",
      "modified_text: neighboring fields, reward: 1.00\n",
      "Claps reward: 0.23, ASR reward: 1.00, Final reward: 0.23\n",
      "modified_text: thanks for watching, reward: 0.00\n",
      "Claps reward: 0.20, ASR reward: 0.00, Final reward: 0.00\n",
      "modified_text: neighboring fields, reward: 1.00\n",
      "Claps reward: 0.26, ASR reward: 1.00, Final reward: 0.26\n",
      "modified_text: neighboring fields, reward: 1.00\n",
      "Claps reward: 0.21, ASR reward: 1.00, Final reward: 0.21\n",
      "modified_text: thank you very much, reward: 0.00\n",
      "Claps reward: 0.15, ASR reward: 0.00, Final reward: 0.00\n",
      "modified_text: neighboring fields, reward: 1.00\n",
      "Claps reward: 0.17, ASR reward: 1.00, Final reward: 0.17\n"
     ]
    }
   ],
   "source": [
    "# Start time\n",
    "total_start_time = time.time()\n",
    "\n",
    "def evaluate_model(eval_type, eval_indices, eval_data_len):\n",
    "    metrics, rewards = eval_dpo_claps_asr_batch(\n",
    "        nar_model=nar_model,\n",
    "        ar_tokenizer=ar_tokenizer,\n",
    "        nar_tokenizer=nar_tokenizer,\n",
    "        trained_model=model,\n",
    "        args_predict=args_predict,\n",
    "        all_src_encodec=selected_src_encodec,\n",
    "        all_instruction=selected_instruction,\n",
    "        iteration=-1,\n",
    "        num_evaluations=num_eval,\n",
    "        eval_data_len=eval_data_len,\n",
    "        selected_indices=eval_indices,\n",
    "        device=device,\n",
    "        clap_model=clap_model,\n",
    "        asr_model=asr_model,\n",
    "        accelerator=accelerator,\n",
    "    )\n",
    "    logging.info(f\"Original Model {eval_type} Set Evaluation:\")\n",
    "    logging.info(f\"Original model metrics on {eval_type} set: {metrics}\")\n",
    "    logging.info(f\"Original model rewards on {eval_type} set: {rewards}\")\n",
    "\n",
    "    reward_list = []\n",
    "    for reward_group in rewards:\n",
    "        filtered_rewards = [r for r in reward_group if r is not None]\n",
    "        reward_list.append(None if not filtered_rewards else np.mean(filtered_rewards))\n",
    "    \n",
    "    logging.info(f\"Original model reward list on {eval_type} set: {reward_list}\")\n",
    "    filtered_reward_list = [r for r in reward_list if r is not None]\n",
    "    avg_reward = None if not filtered_reward_list else np.mean(filtered_reward_list)\n",
    "    logging.info(f\"Original model average rewards on {eval_type} set: {avg_reward}\")\n",
    "\n",
    "if eval_train:\n",
    "    evaluate_model(\"Train\", eval_train_indices, eval_train_data_len)\n",
    "\n",
    "if eval_test:\n",
    "    evaluate_model(\"Test\", eval_test_indices, eval_test_data_len)\n",
    "\n",
    "# Prepare data for training\n",
    "if train_selected_indices:\n",
    "    batch_src_encodec = [selected_src_encodec[i] for i in train_selected_indices]\n",
    "    batch_instruction = [selected_instruction[i] for i in train_selected_indices]\n",
    "    logging.info(f\"Processing data from selected indices: {train_selected_indices}\")\n",
    "else:\n",
    "    start_idx, end_idx = 0, data_size_per_iteration\n",
    "    batch_src_encodec = selected_src_encodec[start_idx:end_idx]\n",
    "    batch_instruction = selected_instruction[start_idx:end_idx]\n",
    "    logging.info(f\"Processing data from index {start_idx} to {end_idx}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "\n",
    "import os\n",
    "os.environ[\"WANDB_SILENT\"] = \"true\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Start training iterations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken for batch inference: 12.096856117248535 seconds\n",
      "Length of layer_list_batch:  80\n",
      "Time taken for batch audio synthesis: 20.46362328529358 seconds\n",
      "modified_text: thank you, reward: 0.00\n",
      "modified_text: neighboring fields, reward: 1.00\n",
      "modified_text: neighboring fields, reward: 1.00\n",
      "modified_text: neighboring fields, reward: 1.00\n",
      "modified_text: neighboring fields, reward: 1.00\n",
      "modified_text: neighboring fields, reward: 1.00\n",
      "modified_text: thank you very much, reward: 0.00\n",
      "modified_text: neighboring fields, reward: 1.00\n",
      "modified_text: theyre green fields, reward: 0.00\n",
      "modified_text: they drink fields, reward: 0.00\n",
      "modified_text: neighboring fields, reward: 1.00\n",
      "modified_text: thank you very much, reward: 0.00\n",
      "modified_text: thank you for your time, reward: 0.00\n",
      "modified_text: neighboring fields, reward: 1.00\n",
      "modified_text: agrain fields, reward: 0.50\n",
      "modified_text: neighboring fields, reward: 1.00\n",
      "modified_text: thank you very much, reward: 0.00\n",
      "modified_text: thank you, reward: 0.00\n",
      "modified_text: neighboring fields, reward: 1.00\n",
      "modified_text: neighboring fields, reward: 1.00\n",
      "modified_text: agrain field, reward: 0.00\n",
      "modified_text: neighboring fields, reward: 1.00\n",
      "modified_text: neighboring fields, reward: 1.00\n",
      "modified_text: neighboring fields, reward: 1.00\n",
      "modified_text: thank you very much, reward: 0.00\n",
      "modified_text: thank you, reward: 0.00\n",
      "modified_text: theyre green fields, reward: 0.00\n",
      "modified_text: neighboring fields, reward: 1.00\n",
      "modified_text: neighboring fields, reward: 1.00\n",
      "modified_text: thanks for watching, reward: 0.00\n",
      "modified_text: neighboring fields, reward: 1.00\n",
      "modified_text: thank you very much, reward: 0.00\n",
      "modified_text: neighboring fields, reward: 1.00\n",
      "modified_text: neighboring fields, reward: 1.00\n",
      "modified_text: neighboring fields, reward: 1.00\n",
      "modified_text: neighboring fields, reward: 1.00\n",
      "modified_text: neighboring fields, reward: 1.00\n",
      "modified_text: neighboring fields, reward: 1.00\n",
      "modified_text: neighboring fields, reward: 1.00\n",
      "modified_text: thank you, reward: 0.00\n",
      "modified_text: neighboring fields, reward: 1.00\n",
      "modified_text: neighboring fields, reward: 1.00\n",
      "modified_text: neighboring fields, reward: 1.00\n",
      "modified_text: thank you very much, reward: 0.00\n",
      "modified_text: neighboring fields, reward: 1.00\n",
      "modified_text: neighboring fields, reward: 1.00\n",
      "modified_text: theyre green fields, reward: 0.00\n",
      "modified_text: thank you, reward: 0.00\n",
      "modified_text: neighboring fields, reward: 1.00\n",
      "modified_text: neighboring fields, reward: 1.00\n",
      "modified_text: neighboring fields, reward: 1.00\n",
      "modified_text: neighboring fields, reward: 1.00\n",
      "modified_text: neighboring fields, reward: 1.00\n",
      "modified_text: neighboring fields, reward: 1.00\n",
      "modified_text: make a ring for you, reward: 0.00\n",
      "modified_text: thank you very much, reward: 0.00\n",
      "modified_text: neighboring fields, reward: 1.00\n",
      "modified_text: neighboring fields, reward: 1.00\n",
      "modified_text: neighboring fields, reward: 1.00\n",
      "modified_text: thanks for watching, reward: 0.00\n",
      "modified_text: neighboring fields, reward: 1.00\n",
      "modified_text: neighboring fields, reward: 1.00\n",
      "modified_text: neighboring fields, reward: 1.00\n",
      "modified_text: abringed fields, reward: 0.50\n",
      "modified_text: neighboring fields, reward: 1.00\n",
      "modified_text: neighboring fields, reward: 1.00\n",
      "modified_text: neighboring fields, reward: 1.00\n",
      "modified_text: neighboring fields, reward: 1.00\n",
      "modified_text: neighboring fields, reward: 1.00\n",
      "modified_text: neighboring fields, reward: 1.00\n",
      "modified_text: neighboring fields, reward: 1.00\n",
      "modified_text: neighboring fields, reward: 1.00\n",
      "modified_text: neighboring fields, reward: 1.00\n",
      "modified_text: thank you very much, reward: 0.00\n",
      "modified_text: neighboring fields, reward: 1.00\n",
      "modified_text: neighboring fields, reward: 1.00\n",
      "modified_text: neighboring fields, reward: 1.00\n",
      "modified_text: thank you very much, reward: 0.00\n",
      "modified_text: neighboring fields, reward: 1.00\n",
      "modified_text: neighboring fields, reward: 1.00\n",
      "Average reward for data index 0: 0.12729029739275574\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 28/28 [00:00<00:00, 1113.79 examples/s]\n",
      "Map: 100%|██████████| 4/4 [00:00<00:00, 522.26 examples/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='12' max='12' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [12/12 00:02, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken for batch inference: 2.580101728439331 seconds\n",
      "Length of layer_list_batch:  10\n",
      "Time taken for batch audio synthesis: 3.0354437828063965 seconds\n",
      "modified_text: neighboring fields, reward: 1.00\n",
      "Claps reward: 0.16, ASR reward: 1.00, Final reward: 0.16\n",
      "modified_text: neighboring fields, reward: 1.00\n",
      "Claps reward: 0.22, ASR reward: 1.00, Final reward: 0.22\n",
      "modified_text: neighboring fields, reward: 1.00\n",
      "Claps reward: 0.18, ASR reward: 1.00, Final reward: 0.18\n",
      "modified_text: thank you, reward: 0.00\n",
      "Claps reward: 0.23, ASR reward: 0.00, Final reward: 0.00\n",
      "modified_text: neighboring fields, reward: 1.00\n",
      "Claps reward: 0.14, ASR reward: 1.00, Final reward: 0.14\n",
      "modified_text: neighboring fields, reward: 1.00\n",
      "Claps reward: 0.26, ASR reward: 1.00, Final reward: 0.26\n",
      "modified_text: neighboring fields, reward: 1.00\n",
      "Claps reward: 0.04, ASR reward: 1.00, Final reward: 0.04\n",
      "modified_text: thank you, reward: 0.00\n",
      "Claps reward: 0.14, ASR reward: 0.00, Final reward: 0.00\n",
      "modified_text: neighboring fields, reward: 1.00\n",
      "Claps reward: 0.18, ASR reward: 1.00, Final reward: 0.18\n",
      "modified_text: neighboring fields, reward: 1.00\n",
      "Claps reward: 0.14, ASR reward: 1.00, Final reward: 0.14\n",
      "Time taken for batch inference: 12.072712898254395 seconds\n",
      "Length of layer_list_batch:  80\n",
      "Time taken for batch audio synthesis: 21.166449546813965 seconds\n",
      "modified_text: neighboring fields, reward: 1.00\n",
      "modified_text: neighboring fields, reward: 1.00\n",
      "modified_text: neighboring fields, reward: 1.00\n",
      "modified_text: neighboring fields, reward: 1.00\n",
      "modified_text: neighboring fields, reward: 1.00\n",
      "modified_text: neighboring fields, reward: 1.00\n",
      "modified_text: neighboring fields, reward: 1.00\n",
      "modified_text: neighboring fields, reward: 1.00\n",
      "modified_text: neighboring fields, reward: 1.00\n",
      "modified_text: neighboring fields, reward: 1.00\n",
      "modified_text: neighboring fields, reward: 1.00\n",
      "modified_text: thank you very much, reward: 0.00\n",
      "modified_text: neighboring fields, reward: 1.00\n",
      "modified_text: neighboring fields, reward: 1.00\n",
      "modified_text: neighboring fields, reward: 1.00\n",
      "modified_text: thank you very much, reward: 0.00\n",
      "modified_text: make her in fields, reward: 0.00\n",
      "modified_text: neighboring fields, reward: 1.00\n",
      "modified_text: theyre growing fields, reward: 0.00\n",
      "modified_text: neighboring fields, reward: 1.00\n",
      "modified_text: neighboring fields, reward: 1.00\n",
      "modified_text: neighboring fields, reward: 1.00\n",
      "modified_text: neighboring fields, reward: 1.00\n",
      "modified_text: neighboring fields, reward: 1.00\n",
      "modified_text: thank you, reward: 0.00\n",
      "modified_text: thank you, reward: 0.00\n",
      "modified_text: neighboring fields, reward: 1.00\n",
      "modified_text: neighboring fields, reward: 1.00\n",
      "modified_text: thank you, reward: 0.00\n",
      "modified_text: neighboring fields, reward: 1.00\n",
      "modified_text: neighboring fields, reward: 1.00\n",
      "modified_text: neighboring fields, reward: 1.00\n",
      "modified_text: thank you, reward: 0.00\n",
      "modified_text: neighboring fields, reward: 1.00\n",
      "modified_text: thank you, reward: 0.00\n",
      "modified_text: thank you, reward: 0.00\n",
      "modified_text: neighboring fields, reward: 1.00\n",
      "modified_text: make a ring for you, reward: 0.00\n",
      "modified_text: thank you, reward: 0.00\n",
      "modified_text: thanks for watching and ill see you in the next video, reward: 0.00\n",
      "modified_text: they bring fields, reward: 0.00\n",
      "modified_text: neighboring fields, reward: 1.00\n",
      "modified_text: thank you, reward: 0.00\n",
      "modified_text: neighboring fields, reward: 1.00\n",
      "modified_text: neighboring fields, reward: 1.00\n",
      "modified_text: thanks for watching and ill see you in the next video, reward: 0.00\n",
      "modified_text: neighboring fields, reward: 1.00\n",
      "modified_text: neighboring fields, reward: 1.00\n",
      "modified_text: thank you very much, reward: 0.00\n",
      "modified_text: neighboring fields, reward: 1.00\n",
      "modified_text: neighboring fields, reward: 1.00\n",
      "modified_text: theyre green fields, reward: 0.00\n",
      "modified_text: thank you very much, reward: 0.00\n",
      "modified_text: neighboring fields, reward: 1.00\n",
      "modified_text: they agree in fields, reward: 0.00\n",
      "modified_text: neighboring fields, reward: 1.00\n",
      "modified_text: thanks for watching and ill see you in the next video, reward: 0.00\n",
      "modified_text: neighboring fields, reward: 1.00\n",
      "modified_text: neighboring fields, reward: 1.00\n",
      "modified_text: neighboring fields, reward: 1.00\n",
      "modified_text: thank you very much, reward: 0.00\n",
      "modified_text: neighboring fields, reward: 1.00\n",
      "modified_text: thank you very much, reward: 0.00\n",
      "modified_text: neighboring fields, reward: 1.00\n",
      "modified_text: thank you, reward: 0.00\n",
      "modified_text: neighboring fields, reward: 1.00\n",
      "modified_text: neighboring fields, reward: 1.00\n",
      "modified_text: neighboring fields, reward: 1.00\n",
      "modified_text: thank you very much, reward: 0.00\n",
      "modified_text: neighboring fields, reward: 1.00\n",
      "modified_text: neighboring fields, reward: 1.00\n",
      "modified_text: neighboring fields, reward: 1.00\n",
      "modified_text: neighboring fields, reward: 1.00\n",
      "modified_text: neighboring fields, reward: 1.00\n",
      "modified_text: neighboring fields, reward: 1.00\n",
      "modified_text: thank you, reward: 0.00\n",
      "modified_text: neighboring fields, reward: 1.00\n",
      "modified_text: neighboring fields, reward: 1.00\n",
      "modified_text: bakering fields, reward: 0.50\n",
      "modified_text: thank you, reward: 0.00\n",
      "Average reward for data index 0: 0.11350361499935388\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 28/28 [00:00<00:00, 1013.79 examples/s]\n",
      "Map: 100%|██████████| 4/4 [00:00<00:00, 504.97 examples/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='12' max='12' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [12/12 00:02, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken for batch inference: 2.5118110179901123 seconds\n",
      "Length of layer_list_batch:  10\n",
      "Time taken for batch audio synthesis: 3.3522160053253174 seconds\n",
      "modified_text: neighboring fields, reward: 1.00\n",
      "Claps reward: 0.17, ASR reward: 1.00, Final reward: 0.17\n",
      "modified_text: neighboring fields, reward: 1.00\n",
      "Claps reward: 0.21, ASR reward: 1.00, Final reward: 0.21\n",
      "modified_text: neighboring fields, reward: 1.00\n",
      "Claps reward: 0.18, ASR reward: 1.00, Final reward: 0.18\n",
      "modified_text: thanks for watching and ill see you next time, reward: 0.00\n",
      "Claps reward: 0.23, ASR reward: 0.00, Final reward: 0.00\n",
      "modified_text: neighboring fields, reward: 1.00\n",
      "Claps reward: 0.10, ASR reward: 1.00, Final reward: 0.10\n",
      "modified_text: neighboring fields, reward: 1.00\n",
      "Claps reward: 0.23, ASR reward: 1.00, Final reward: 0.23\n",
      "modified_text: neighboring fields, reward: 1.00\n",
      "Claps reward: 0.04, ASR reward: 1.00, Final reward: 0.04\n",
      "modified_text: thank you, reward: 0.00\n",
      "Claps reward: 0.14, ASR reward: 0.00, Final reward: 0.00\n",
      "modified_text: neighboring fields, reward: 1.00\n",
      "Claps reward: 0.19, ASR reward: 1.00, Final reward: 0.19\n",
      "modified_text: neighboring fields, reward: 1.00\n",
      "Claps reward: 0.23, ASR reward: 1.00, Final reward: 0.23\n",
      "Time taken for batch inference: 10.37132453918457 seconds\n",
      "Length of layer_list_batch:  80\n",
      "Time taken for batch audio synthesis: 21.027660369873047 seconds\n",
      "modified_text: neighboring fields, reward: 1.00\n",
      "modified_text: neighboring fields, reward: 1.00\n",
      "modified_text: neighboring fields, reward: 1.00\n",
      "modified_text: thank you, reward: 0.00\n",
      "modified_text: thanks for watching, reward: 0.00\n",
      "modified_text: thank you, reward: 0.00\n",
      "modified_text: neighboring fields, reward: 1.00\n",
      "modified_text: neighboring fields, reward: 1.00\n",
      "modified_text: neighboring fields, reward: 1.00\n",
      "modified_text: neighboring fields, reward: 1.00\n",
      "modified_text: neighboring fields, reward: 1.00\n",
      "modified_text: neighboring fields, reward: 1.00\n",
      "modified_text: neighboring fields, reward: 1.00\n",
      "modified_text: neighboring fields, reward: 1.00\n",
      "modified_text: thank you, reward: 0.00\n",
      "modified_text: they bring fields, reward: 0.00\n",
      "modified_text: neighboring fields, reward: 1.00\n",
      "modified_text: theyre green fields, reward: 0.00\n",
      "modified_text: neighboring fields, reward: 1.00\n",
      "modified_text: thank you, reward: 0.00\n",
      "modified_text: neighboring fields, reward: 1.00\n",
      "modified_text: thank you very much, reward: 0.00\n",
      "modified_text: neighboring fields, reward: 1.00\n",
      "modified_text: thank you, reward: 0.00\n",
      "modified_text: neighboring fields, reward: 1.00\n",
      "modified_text: thank you very much, reward: 0.00\n",
      "modified_text: nightling fields, reward: 0.50\n",
      "modified_text: neighboring fields, reward: 1.00\n",
      "modified_text: make the ring feel, reward: 0.00\n",
      "modified_text: make a ring field, reward: 0.00\n",
      "modified_text: abringed fields, reward: 0.50\n",
      "modified_text: neighboring fields, reward: 1.00\n",
      "modified_text: thanks for watching, reward: 0.00\n",
      "modified_text: thank you very much, reward: 0.00\n",
      "modified_text: make the ring feel, reward: 0.00\n",
      "modified_text: neighboring fields, reward: 1.00\n",
      "modified_text: thanks for your time, reward: 0.00\n",
      "modified_text: neighboring fields, reward: 1.00\n",
      "modified_text: neighboring fields, reward: 1.00\n",
      "modified_text: that green fields, reward: 0.00\n",
      "modified_text: neighboring fields, reward: 1.00\n",
      "modified_text: neighboring fields, reward: 1.00\n",
      "modified_text: neighboring fields, reward: 1.00\n",
      "modified_text: neighboring fields, reward: 1.00\n",
      "modified_text: thank you, reward: 0.00\n",
      "modified_text: make the ring feel, reward: 0.00\n",
      "modified_text: neighboring fields, reward: 1.00\n",
      "modified_text: make a ring for you, reward: 0.00\n",
      "modified_text: neighboring fields, reward: 1.00\n",
      "modified_text: neighboring fields, reward: 1.00\n",
      "modified_text: neighboring fields, reward: 1.00\n",
      "modified_text: theyre growing fields, reward: 0.00\n",
      "modified_text: neighboring fields, reward: 1.00\n",
      "modified_text: neighboring fields, reward: 1.00\n",
      "modified_text: thank you very much, reward: 0.00\n",
      "modified_text: thank you, reward: 0.00\n",
      "modified_text: neighboring fields, reward: 1.00\n",
      "modified_text: thank you, reward: 0.00\n",
      "modified_text: neighboring fields, reward: 1.00\n",
      "modified_text: theyre growing fields, reward: 0.00\n",
      "modified_text: neighboring fields, reward: 1.00\n",
      "modified_text: neighboring fields, reward: 1.00\n",
      "modified_text: theyre green fields, reward: 0.00\n",
      "modified_text: thank you very much, reward: 0.00\n",
      "modified_text: neighboring fields, reward: 1.00\n",
      "modified_text: neighboring fields, reward: 1.00\n",
      "modified_text: neighboring fields, reward: 1.00\n",
      "modified_text: neighboring fields, reward: 1.00\n",
      "modified_text: neighboring fields, reward: 1.00\n",
      "modified_text: neighboring fields, reward: 1.00\n",
      "modified_text: neighboring fields, reward: 1.00\n",
      "modified_text: neighboring fields, reward: 1.00\n",
      "modified_text: neighboring fields, reward: 1.00\n",
      "modified_text: neighboring fields, reward: 1.00\n",
      "modified_text: neighboring fields, reward: 1.00\n",
      "modified_text: thank you, reward: 0.00\n",
      "modified_text: neighboring fields, reward: 1.00\n",
      "modified_text: neighboring fields, reward: 1.00\n",
      "modified_text: neighboring fields, reward: 1.00\n",
      "modified_text: they bring fields, reward: 0.00\n",
      "Average reward for data index 0: 0.11719234380871058\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 28/28 [00:00<00:00, 945.57 examples/s]\n",
      "Map: 100%|██████████| 4/4 [00:00<00:00, 544.24 examples/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='12' max='12' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [12/12 00:02, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken for batch inference: 2.5355796813964844 seconds\n",
      "Length of layer_list_batch:  10\n",
      "Time taken for batch audio synthesis: 2.6215949058532715 seconds\n",
      "modified_text: neighboring fields, reward: 1.00\n",
      "Claps reward: 0.17, ASR reward: 1.00, Final reward: 0.17\n",
      "modified_text: neighboring fields, reward: 1.00\n",
      "Claps reward: 0.22, ASR reward: 1.00, Final reward: 0.22\n",
      "modified_text: neighboring fields, reward: 1.00\n",
      "Claps reward: 0.17, ASR reward: 1.00, Final reward: 0.17\n",
      "modified_text: thank you, reward: 0.00\n",
      "Claps reward: 0.23, ASR reward: 0.00, Final reward: 0.00\n",
      "modified_text: neighboring fields, reward: 1.00\n",
      "Claps reward: 0.10, ASR reward: 1.00, Final reward: 0.10\n",
      "modified_text: neighboring fields, reward: 1.00\n",
      "Claps reward: 0.26, ASR reward: 1.00, Final reward: 0.26\n",
      "modified_text: neighboring fields, reward: 1.00\n",
      "Claps reward: 0.04, ASR reward: 1.00, Final reward: 0.04\n",
      "modified_text: thank you, reward: 0.00\n",
      "Claps reward: 0.14, ASR reward: 0.00, Final reward: 0.00\n",
      "modified_text: neighboring fields, reward: 1.00\n",
      "Claps reward: 0.19, ASR reward: 1.00, Final reward: 0.19\n",
      "modified_text: neighboring fields, reward: 1.00\n",
      "Claps reward: 0.21, ASR reward: 1.00, Final reward: 0.21\n",
      "Time taken for batch inference: 18.89831233024597 seconds\n",
      "Length of layer_list_batch:  80\n",
      "Time taken for batch audio synthesis: 20.09613537788391 seconds\n",
      "modified_text: neighboring fields, reward: 1.00\n",
      "modified_text: neighboring fields, reward: 1.00\n",
      "modified_text: neighboring fields, reward: 1.00\n",
      "modified_text: neighboring fields, reward: 1.00\n",
      "modified_text: neighboring fields, reward: 1.00\n",
      "modified_text: neighboring fields, reward: 1.00\n",
      "modified_text: neighboring fields, reward: 1.00\n",
      "modified_text: neighboring fields, reward: 1.00\n",
      "modified_text: neighboring fields, reward: 1.00\n",
      "modified_text: neighboring fields, reward: 1.00\n",
      "modified_text: neighboring fields, reward: 1.00\n",
      "modified_text: neighboring fields, reward: 1.00\n",
      "modified_text: neighboring fields, reward: 1.00\n",
      "modified_text: thank you, reward: 0.00\n",
      "modified_text: thank you, reward: 0.00\n",
      "modified_text: they bring fields, reward: 0.00\n",
      "modified_text: neighboring fields, reward: 1.00\n",
      "modified_text: theyre green fields, reward: 0.00\n",
      "modified_text: neighboring fields, reward: 1.00\n",
      "modified_text: thank you, reward: 0.00\n",
      "modified_text: neighboring fields, reward: 1.00\n",
      "modified_text: thank you very much, reward: 0.00\n",
      "modified_text: neighboring fields, reward: 1.00\n",
      "modified_text: thanks for watching make a ring of peace, reward: 0.00\n",
      "modified_text: neighboring fields, reward: 1.00\n",
      "modified_text: neighboring fields, reward: 1.00\n",
      "modified_text: neighboring fields, reward: 1.00\n",
      "modified_text: neighboring fields, reward: 1.00\n",
      "modified_text: make the ring feel, reward: 0.00\n",
      "modified_text: thank you, reward: 0.00\n",
      "modified_text: abringed fields, reward: 0.50\n",
      "modified_text: neighboring fields, reward: 1.00\n",
      "modified_text: neighboring fields, reward: 1.00\n",
      "modified_text: thank you very much, reward: 0.00\n",
      "modified_text: neighboring fields, reward: 1.00\n",
      "modified_text: neighboring fields, reward: 1.00\n",
      "modified_text: they bring fields, reward: 0.00\n",
      "modified_text: neighboring fields, reward: 1.00\n",
      "modified_text: neighboring fields, reward: 1.00\n",
      "modified_text: theyre green fields, reward: 0.00\n",
      "modified_text: neighboring fields, reward: 1.00\n",
      "modified_text: neighboring fields, reward: 1.00\n",
      "modified_text: neighboring fields, reward: 1.00\n",
      "modified_text: neighboring fields, reward: 1.00\n",
      "modified_text: thank you, reward: 0.00\n",
      "modified_text: neighboring fields, reward: 1.00\n",
      "modified_text: neighboring fields, reward: 1.00\n",
      "modified_text: make a ring for you, reward: 0.00\n",
      "modified_text: neighboring fields, reward: 1.00\n",
      "modified_text: neighboring fields, reward: 1.00\n",
      "modified_text: neighboring fields, reward: 1.00\n",
      "modified_text: theyre growing fields, reward: 0.00\n",
      "modified_text: thank you, reward: 0.00\n",
      "modified_text: neighboring fields, reward: 1.00\n",
      "modified_text: thank you very much, reward: 0.00\n",
      "modified_text: neighboring fields, reward: 1.00\n",
      "modified_text: neighboring fields, reward: 1.00\n",
      "modified_text: thank you, reward: 0.00\n",
      "modified_text: neighboring fields, reward: 1.00\n",
      "modified_text: theyre growing fields, reward: 0.00\n",
      "modified_text: neighboring fields, reward: 1.00\n",
      "modified_text: neighboring fields, reward: 1.00\n",
      "modified_text: theyre green fields, reward: 0.00\n",
      "modified_text: thank you, reward: 0.00\n",
      "modified_text: neighboring fields, reward: 1.00\n",
      "modified_text: neighboring fields, reward: 1.00\n",
      "modified_text: neighboring fields, reward: 1.00\n",
      "modified_text: neighboring fields, reward: 1.00\n",
      "modified_text: neighboring fields, reward: 1.00\n",
      "modified_text: neighboring fields, reward: 1.00\n",
      "modified_text: neighboring fields, reward: 1.00\n",
      "modified_text: neighboring fields, reward: 1.00\n",
      "modified_text: neighboring fields, reward: 1.00\n",
      "modified_text: neighboring fields, reward: 1.00\n",
      "modified_text: neighboring fields, reward: 1.00\n",
      "modified_text: neighboring fields, reward: 1.00\n",
      "modified_text: neighboring fields, reward: 1.00\n",
      "modified_text: neighboring fields, reward: 1.00\n",
      "modified_text: neighboring fields, reward: 1.00\n",
      "modified_text: theyre growing fields, reward: 0.00\n",
      "Average reward for data index 0: 0.13985764160752295\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 28/28 [00:00<00:00, 1066.51 examples/s]\n",
      "Map: 100%|██████████| 4/4 [00:00<00:00, 540.73 examples/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='12' max='12' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [12/12 00:02, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken for batch inference: 2.494483470916748 seconds\n",
      "Length of layer_list_batch:  10\n",
      "Time taken for batch audio synthesis: 2.545757293701172 seconds\n",
      "modified_text: neighboring fields, reward: 1.00\n",
      "Claps reward: 0.17, ASR reward: 1.00, Final reward: 0.17\n",
      "modified_text: neighboring fields, reward: 1.00\n",
      "Claps reward: 0.21, ASR reward: 1.00, Final reward: 0.21\n",
      "modified_text: neighboring fields, reward: 1.00\n",
      "Claps reward: 0.17, ASR reward: 1.00, Final reward: 0.17\n",
      "modified_text: thank you very much, reward: 0.00\n",
      "Claps reward: 0.25, ASR reward: 0.00, Final reward: 0.00\n",
      "modified_text: neighboring fields, reward: 1.00\n",
      "Claps reward: 0.12, ASR reward: 1.00, Final reward: 0.12\n",
      "modified_text: thank you very much, reward: 0.00\n",
      "Claps reward: 0.25, ASR reward: 0.00, Final reward: 0.00\n",
      "modified_text: neighboring fields, reward: 1.00\n",
      "Claps reward: 0.04, ASR reward: 1.00, Final reward: 0.04\n",
      "modified_text: neighboring fields, reward: 1.00\n",
      "Claps reward: 0.16, ASR reward: 1.00, Final reward: 0.16\n",
      "modified_text: neighboring fields, reward: 1.00\n",
      "Claps reward: 0.23, ASR reward: 1.00, Final reward: 0.23\n",
      "modified_text: neighboring fields, reward: 1.00\n",
      "Claps reward: 0.20, ASR reward: 1.00, Final reward: 0.20\n",
      "Time taken for batch inference: 9.995656490325928 seconds\n",
      "Length of layer_list_batch:  80\n",
      "Time taken for batch audio synthesis: 20.909382104873657 seconds\n",
      "modified_text: thank you, reward: 0.00\n",
      "modified_text: neighboring fields, reward: 1.00\n",
      "modified_text: neighboring fields, reward: 1.00\n",
      "modified_text: neighboring fields, reward: 1.00\n",
      "modified_text: neighboring fields, reward: 1.00\n",
      "modified_text: thank you, reward: 0.00\n",
      "modified_text: neighboring fields, reward: 1.00\n",
      "modified_text: neighboring fields, reward: 1.00\n",
      "modified_text: neighboring fields, reward: 1.00\n",
      "modified_text: neighboring fields, reward: 1.00\n",
      "modified_text: neighboring fields, reward: 1.00\n",
      "modified_text: neighboring fields, reward: 1.00\n",
      "modified_text: neighboring fields, reward: 1.00\n",
      "modified_text: thank you, reward: 0.00\n",
      "modified_text: thank you, reward: 0.00\n",
      "modified_text: neighboring fields, reward: 1.00\n",
      "modified_text: neighboring fields, reward: 1.00\n",
      "modified_text: neighboring fields, reward: 1.00\n",
      "modified_text: neighboring fields, reward: 1.00\n",
      "modified_text: thank you, reward: 0.00\n",
      "modified_text: neighboring fields, reward: 1.00\n",
      "modified_text: thank you very much, reward: 0.00\n",
      "modified_text: neighboring fields, reward: 1.00\n",
      "modified_text: thank you very much, reward: 0.00\n",
      "modified_text: neighboring fields, reward: 1.00\n",
      "modified_text: neighboring fields, reward: 1.00\n",
      "modified_text: neighboring fields, reward: 1.00\n",
      "modified_text: neighboring fields, reward: 1.00\n",
      "modified_text: make the ring feel, reward: 0.00\n",
      "modified_text: thank you, reward: 0.00\n",
      "modified_text: neighboring fields, reward: 1.00\n",
      "modified_text: neighboring fields, reward: 1.00\n",
      "modified_text: neighboring fields, reward: 1.00\n",
      "modified_text: neighboring fields, reward: 1.00\n",
      "modified_text: neighboring fields, reward: 1.00\n",
      "modified_text: neighboring fields, reward: 1.00\n",
      "modified_text: they bring fields, reward: 0.00\n",
      "modified_text: neighboring fields, reward: 1.00\n",
      "modified_text: neighboring fields, reward: 1.00\n",
      "modified_text: theyre green fields, reward: 0.00\n",
      "modified_text: neighboring fields, reward: 1.00\n",
      "modified_text: neighboring fields, reward: 1.00\n",
      "modified_text: neighboring fields, reward: 1.00\n",
      "modified_text: neighboring fields, reward: 1.00\n",
      "modified_text: thank you, reward: 0.00\n",
      "modified_text: make the ring fields, reward: 0.00\n",
      "modified_text: neighboring fields, reward: 1.00\n",
      "modified_text: make a ring for you, reward: 0.00\n",
      "modified_text: neighboring fields, reward: 1.00\n",
      "modified_text: neighboring fields, reward: 1.00\n",
      "modified_text: neighboring fields, reward: 1.00\n",
      "modified_text: theyre growing fields, reward: 0.00\n",
      "modified_text: neighboring fields, reward: 1.00\n",
      "modified_text: neighboring fields, reward: 1.00\n",
      "modified_text: thank you very much, reward: 0.00\n",
      "modified_text: neighboring fields, reward: 1.00\n",
      "modified_text: neighboring fields, reward: 1.00\n",
      "modified_text: thank you, reward: 0.00\n",
      "modified_text: neighboring fields, reward: 1.00\n",
      "modified_text: theyre growing fields, reward: 0.00\n",
      "modified_text: neighboring fields, reward: 1.00\n",
      "modified_text: neighboring fields, reward: 1.00\n",
      "modified_text: theyre green fields, reward: 0.00\n",
      "modified_text: thank you very much, reward: 0.00\n",
      "modified_text: neighboring fields, reward: 1.00\n",
      "modified_text: neighboring fields, reward: 1.00\n",
      "modified_text: neighboring fields, reward: 1.00\n",
      "modified_text: neighboring fields, reward: 1.00\n",
      "modified_text: neighboring fields, reward: 1.00\n",
      "modified_text: neighboring fields, reward: 1.00\n",
      "modified_text: neighboring fields, reward: 1.00\n",
      "modified_text: neighboring fields, reward: 1.00\n",
      "modified_text: neighboring fields, reward: 1.00\n",
      "modified_text: neighboring fields, reward: 1.00\n",
      "modified_text: neighboring fields, reward: 1.00\n",
      "modified_text: thank you, reward: 0.00\n",
      "modified_text: neighboring fields, reward: 1.00\n",
      "modified_text: neighboring fields, reward: 1.00\n",
      "modified_text: neighboring fields, reward: 1.00\n",
      "modified_text: theyre going to be a lot of fun, reward: 0.00\n",
      "Average reward for data index 0: 0.13699163012206556\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 28/28 [00:00<00:00, 1059.36 examples/s]\n",
      "Map: 100%|██████████| 4/4 [00:00<00:00, 563.96 examples/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='12' max='12' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [12/12 00:02, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken for batch inference: 2.5644805431365967 seconds\n",
      "Length of layer_list_batch:  10\n"
     ]
    }
   ],
   "source": [
    "disable_tqdm = not os.isatty(1)\n",
    "\n",
    "def evaluate_iteration(eval_type, iteration, eval_indices, eval_data_len):\n",
    "    metrics, rewards = eval_dpo_claps_asr_batch(\n",
    "        nar_model=nar_model,\n",
    "        ar_tokenizer=ar_tokenizer,\n",
    "        nar_tokenizer=nar_tokenizer,\n",
    "        trained_model=model,\n",
    "        args_predict=args_predict,\n",
    "        all_src_encodec=selected_src_encodec,\n",
    "        all_instruction=selected_instruction,\n",
    "        iteration=iteration,\n",
    "        num_evaluations=num_eval,\n",
    "        eval_data_len=eval_data_len,\n",
    "        selected_indices=eval_indices,\n",
    "        device=device,\n",
    "        clap_model=clap_model,\n",
    "        asr_model=asr_model,\n",
    "        accelerator=accelerator\n",
    "    )\n",
    "    logging.info(f\"Trained Model Iteration {iteration} {eval_type} Set Evaluation:\")\n",
    "    logging.info(f\"EVAL: Cosine_Sim metrics {eval_type} Set for iteration {iteration}: {metrics}\")\n",
    "    logging.info(f\"EVAL: Cosine_Sim score {eval_type} Set for iteration {iteration}: {rewards}\")\n",
    "\n",
    "    reward_list = [np.mean([r for r in reward_group if r is not None]) if reward_group else None for reward_group in rewards]\n",
    "    logging.info(f\"EVAL: Trained model Cosine_Sim score list on {eval_type} set: {reward_list}\")\n",
    "    filtered_reward_list = [r for r in reward_list if r is not None]\n",
    "    avg_reward = np.mean(filtered_reward_list) if filtered_reward_list else None\n",
    "    logging.info(f\"EVAL: Trained model average Cosine_Sim score on {eval_type} set: {avg_reward}\")\n",
    "\n",
    "for iteration in tqdm(range(num_iterations), desc=\"Training Iterations\", disable=disable_tqdm):\n",
    "    logging.info(f\"-----------Starting iteration {iteration}-----------\")\n",
    "\n",
    "    resume = False\n",
    "\n",
    "    model_checkpoint, chosen_rewards, rejected_rewards = train_iteration(\n",
    "        model=model,\n",
    "        model_checkpoint=model_checkpoint,\n",
    "        iteration=iteration,\n",
    "        data_size=data_size_per_iteration,\n",
    "        sample_size=sample_size,\n",
    "        ar_model=ar_model,\n",
    "        ar_tokenizer=ar_tokenizer,\n",
    "        nar_model=nar_model,\n",
    "        nar_tokenizer=nar_tokenizer,\n",
    "        all_src_encodec=batch_src_encodec,\n",
    "        all_instruction=batch_instruction,\n",
    "        args_predict=args_predict,\n",
    "        agent_output_dir=agent_output_dir,\n",
    "        model_output_dir_base=model_output_dir,\n",
    "        temperature=1.0,\n",
    "        beta=beta,\n",
    "        base_path=base_path,\n",
    "        resume_from_checkpoint=resume,\n",
    "        learning_rate=learning_rate,\n",
    "        num_train_epochs=num_train_epochs,\n",
    "        max_length=max_length,\n",
    "        max_prompt_length=max_prompt_length,\n",
    "        max_target_length=max_target_length,\n",
    "        per_device_train_batch_size=per_device_train_batch_size,\n",
    "        gradient_accumulation_steps=gradient_accumulation_steps,\n",
    "        seed=seed,\n",
    "        clap_model=clap_model,\n",
    "        asr_model=asr_model,\n",
    "        accelerator=accelerator\n",
    "    )\n",
    "\n",
    "    logging.info(f\"Chosen rewards for iteration {iteration}: {chosen_rewards}\")\n",
    "    logging.info(f\"Rejected rewards for iteration {iteration}: {rejected_rewards}\")\n",
    "\n",
    "    if (iteration + 1) % eval_frequency == 0:\n",
    "        if eval_train:\n",
    "            evaluate_iteration(\"Train\", iteration, eval_train_indices, eval_train_data_len)\n",
    "        if eval_test:\n",
    "            evaluate_iteration(\"Test\", iteration, eval_test_indices, eval_test_data_len)\n",
    "\n",
    "    logging.info(f\"-----------Finished iteration {iteration}-----------\")\n",
    "\n",
    "total_end_time = time.time()\n",
    "total_time_taken = total_end_time - total_start_time\n",
    "logging.info(f\"Total time taken for the entire process: {total_time_taken:.2f} seconds\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dpo",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
