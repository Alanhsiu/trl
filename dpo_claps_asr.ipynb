{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/b0990106x/miniconda3/envs/dpo/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/home/b0990106x/.local/lib/python3.10/site-packages/s3prl/upstream/byol_s/byol_a/common.py:20: UserWarning: torchaudio._backend.set_audio_backend has been deprecated. With dispatcher enabled, this function is no-op. You can remove the function call.\n",
      "  torchaudio.set_audio_backend(\"sox_io\")\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append(\"/work/b0990106x/trl/vc\")\n",
    "sys.path.append('/work/b0990106x/trl/CLAPS')\n",
    "\n",
    "import importlib\n",
    "import torch\n",
    "import os\n",
    "import math\n",
    "import numpy as np\n",
    "import random\n",
    "import time\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "from types import SimpleNamespace\n",
    "from datetime import datetime\n",
    "from typing import List, Tuple\n",
    "\n",
    "import vc\n",
    "importlib.reload(vc)\n",
    "from vc.trainer_encodec_vc_inference import (\n",
    "    pack_inputs_v2,\n",
    "    get_ar_prediction_audio_batch\n",
    ")\n",
    "from vc.encodec_model.nar_bart_model import NARBartForConditionalGeneration\n",
    "\n",
    "from transformers import BartForConditionalGeneration, AutoTokenizer\n",
    "from trl import (\n",
    "    DPOTrainer,\n",
    "    DPOConfig,\n",
    "    AutoModelForSeq2SeqLMWithValueHead,\n",
    "    create_reference_model\n",
    ")\n",
    "from datasets import Dataset\n",
    "\n",
    "from dpo_eval import (\n",
    "    get_reward_claps,\n",
    "    get_reward_asr,\n",
    "    eval_dpo_claps_asr_batch,\n",
    "    convert_array_to_tensor_format\n",
    ")\n",
    "from CLAPS.inference import load_model\n",
    "\n",
    "import argparse\n",
    "from faster_whisper import WhisperModel\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Utility Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seed(seed):\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(seed)\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "import shutil\n",
    "from pathlib import Path\n",
    "import soundfile as sf\n",
    "\n",
    "def generate_output_batch(\n",
    "        ar_model, \n",
    "        nar_model, \n",
    "        ar_tokenizer, \n",
    "        nar_tokenizer, \n",
    "        clap_model,\n",
    "        asr_model,\n",
    "        accelerator,\n",
    "        src_encodec: list, \n",
    "        instruction: list, \n",
    "        transcription: list,\n",
    "        args_predict: SimpleNamespace, \n",
    "        episode_counter: int = 0, \n",
    "        base_path: str = \"/work/b0990106x/trl\", \n",
    "        temperature: float = 1.0\n",
    ") -> tuple[list[float], list[str]]:\n",
    "    audio_list, decode_ar_list = get_ar_prediction_audio_batch(\n",
    "        args_predict, ar_model, nar_model, ar_tokenizer, nar_tokenizer, src_encodec, instruction, episode_counter, temperature=temperature\n",
    "    )\n",
    "    reward_list, tokenized_decode_ar_list = [], []\n",
    "    valid_audio_paths = []\n",
    "\n",
    "    # temp_dir_path = Path(\"/dev/shm/temp_audio_files\")\n",
    "    # temp_dir_path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    for i, audio in enumerate(audio_list):\n",
    "        if audio is not None:\n",
    "            tensor_audio = convert_array_to_tensor_format(audio)\n",
    "            if tensor_audio[0].shape[0] == 1:\n",
    "                tensor_audio[0] = tensor_audio[0].squeeze(0)\n",
    "            clap_reward = get_reward_claps(\n",
    "                clap_model=clap_model, accelerator=accelerator, prompts=instruction[i], wavs=tensor_audio\n",
    "            )\n",
    "            output_path_ckpt = args_predict.output_path.replace(\".wav\", f\"_generate_{episode_counter}_item_{i}.wav\")\n",
    "            # output_path_ckpt = temp_dir_path / f\"generate_{episode_counter}_item_{i}.wav\"\n",
    "            sf.write(output_path_ckpt, np.ravel(audio), samplerate=24000)\n",
    "            asr_reward = get_reward_asr(file_path=output_path_ckpt, asr_model=asr_model, ground_truth=transcription[i])\n",
    "            \n",
    "            # only keep the file when i is 0\n",
    "            # if i != 0:\n",
    "            #     os.remove(output_path_ckpt)\n",
    "\n",
    "            final_reward = clap_reward * asr_reward\n",
    "            reward_list.append(final_reward)\n",
    "            valid_audio_paths.append(output_path_ckpt)\n",
    "        else:\n",
    "            reward_list.append(0)\n",
    "\n",
    "    # if episode_counter == \"data_0\":\n",
    "    #     base_path = Path(base_path)\n",
    "    #     target_dir = base_path / \"temp_audio\"\n",
    "    #     if target_dir.exists():\n",
    "    #         shutil.rmtree(target_dir)\n",
    "    #     shutil.copytree(temp_dir_path, target_dir)\n",
    "    #     print(f\"copied temp dir to {target_dir}\")\n",
    "        \n",
    "    # shutil.rmtree(temp_dir_path)\n",
    "\n",
    "    for decode_ar in decode_ar_list:\n",
    "        list_decode_ar = decode_ar.flatten().tolist()\n",
    "        filtered_decode_ar_list = list_decode_ar[2:-1]\n",
    "        decode_ar_tokens = ar_tokenizer.convert_ids_to_tokens(filtered_decode_ar_list)\n",
    "        tokenized_decode_ar = ar_tokenizer.convert_tokens_to_string(decode_ar_tokens)\n",
    "        tokenized_decode_ar_list.append(tokenized_decode_ar)\n",
    "\n",
    "    return reward_list, tokenized_decode_ar_list\n",
    "\n",
    "def extract_data_from_json(file_path: str) -> Tuple[List[list], List[str], List[list]]:\n",
    "    with open(file_path, 'r') as f:\n",
    "        data = json.load(f)\n",
    "\n",
    "    all_src_encodec = [item[\"src_encodec\"] for item in data]\n",
    "    all_instruction = [item[\"instruction\"] for item in data]\n",
    "\n",
    "    return all_src_encodec, all_instruction\n",
    "\n",
    "def train_model(\n",
    "        model,\n",
    "        model_ref,\n",
    "        ar_tokenizer,\n",
    "        train_dataset: Dataset,\n",
    "        val_dataset: Dataset,\n",
    "        model_output_dir: str,\n",
    "        beta: float,\n",
    "        resume_from_checkpoint: bool,\n",
    "        model_checkpoint: str,\n",
    "        learning_rate: float = 5e-07,\n",
    "        num_train_epochs: int = 200,\n",
    "        max_length: int = 1024*9,\n",
    "        max_prompt_length: int = 1024*9,\n",
    "        max_target_length: int = 1024*9,\n",
    "        per_device_train_batch_size: int = 1,\n",
    "        gradient_accumulation_steps: int = 1,\n",
    "        seed: int = 42\n",
    ") -> None:\n",
    "    training_args = DPOConfig(\n",
    "        beta=beta,\n",
    "        output_dir=model_output_dir,\n",
    "        resume_from_checkpoint=model_checkpoint if resume_from_checkpoint else None,\n",
    "        seed=seed,\n",
    "        per_device_train_batch_size=per_device_train_batch_size,\n",
    "        num_train_epochs=num_train_epochs,\n",
    "        gradient_accumulation_steps=gradient_accumulation_steps,\n",
    "        learning_rate=learning_rate,\n",
    "        max_length=max_length,\n",
    "        max_prompt_length=max_prompt_length,\n",
    "        max_target_length=max_target_length,\n",
    "        evaluation_strategy=\"steps\",\n",
    "        save_steps=5000,\n",
    "        logging_dir=f\"{model_output_dir}/logs\"\n",
    "    )\n",
    "    \n",
    "    trainer = DPOTrainer(\n",
    "        model=model,\n",
    "        ref_model=model_ref,\n",
    "        args=training_args,\n",
    "        tokenizer=ar_tokenizer,\n",
    "        train_dataset=train_dataset,\n",
    "        eval_dataset=val_dataset,\n",
    "        loss_type=\"sigmoid\" # \"sigmoid\", \"hinge\", \"ipo\", \"kto_pair\", \"bco_pair\"\n",
    "    )\n",
    "    trainer.train()\n",
    "\n",
    "    model.config.to_json_file(f\"{model_output_dir}/config.json\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "def process_data_batch(\n",
    "    sample_size: int, \n",
    "    ar_model, \n",
    "    nar_model, \n",
    "    ar_tokenizer, \n",
    "    nar_tokenizer, \n",
    "    clap_model,\n",
    "    asr_model,\n",
    "    accelerator,\n",
    "    selected_src_encodec: List[list], \n",
    "    selected_instruction: List[str],\n",
    "    selected_transcription: List[str],\n",
    "    args_predict, \n",
    "    base_path: str = \"/work/b0990106x/trl\", \n",
    "    temperature: float = 1.0, \n",
    "    iteration: int = 0,\n",
    "    prev_eval_avg: float = 0,\n",
    "    strategy: str = \"top_bottom_percent\"  # Default to original strategy. Options: \"max_min\", \"top_bottom_percent\", \"above_below_average\", \"above_prev_eval\"\n",
    ") -> Tuple[List[str], List[str], List[str], List[float], List[float], List[float]]:\n",
    "    # Ensure sample size is valid\n",
    "    if sample_size < 2:\n",
    "        raise ValueError(\"Parameter 'sample_size' must be greater than 1.\")\n",
    "\n",
    "    chosen, rejected, prompts, chosen_rewards, rejected_rewards, average_rewards = [], [], [], [], [], []\n",
    "\n",
    "    disable_tqdm = not os.isatty(1)\n",
    "    for i in tqdm(range(len(selected_src_encodec)), desc=\"Processing Data\", disable=disable_tqdm):\n",
    "        rewards, tokenized_outputs = [], []\n",
    "        size_of_packed_input = (\n",
    "            len(selected_src_encodec[i][0]) +\n",
    "            len(ar_tokenizer(selected_instruction[i])[\"input_ids\"][1:-1]) +\n",
    "            3\n",
    "        )\n",
    "        if 4 < size_of_packed_input <= 1024:\n",
    "            selected_src_encodec_list = [selected_src_encodec[i]] * sample_size\n",
    "            selected_instruction_list = [selected_instruction[i]] * sample_size\n",
    "            selected_transcription_list = [selected_transcription[i]] * sample_size\n",
    "            rewards, tokenized_outputs = generate_output_batch(\n",
    "                ar_model=ar_model, \n",
    "                nar_model=nar_model, \n",
    "                ar_tokenizer=ar_tokenizer, \n",
    "                nar_tokenizer=nar_tokenizer,\n",
    "                src_encodec=selected_src_encodec_list,\n",
    "                instruction=selected_instruction_list, \n",
    "                transcription=selected_transcription_list,\n",
    "                clap_model=clap_model,\n",
    "                asr_model=asr_model,\n",
    "                accelerator=accelerator,\n",
    "                args_predict=args_predict,\n",
    "                episode_counter=f\"data_{i}\",\n",
    "                base_path=base_path, \n",
    "                temperature=temperature\n",
    "            )\n",
    "\n",
    "        valid_rewards = [r for r in rewards if r is not None]\n",
    "        valid_outputs = [tokenized_outputs[j] for j in range(len(rewards)) if rewards[j] is not None]\n",
    "\n",
    "        if len(valid_rewards) >= 2:\n",
    "            average_reward = np.mean(valid_rewards)\n",
    "            print(f\"Average reward for data index {i}: {average_reward}\")\n",
    "\n",
    "            if strategy == \"max_min\":\n",
    "                # Original max-min strategy\n",
    "                max_reward_index = np.argmax(valid_rewards)\n",
    "                min_reward_index = np.argmin(valid_rewards)\n",
    "                chosen_outputs = [valid_outputs[max_reward_index]]\n",
    "                rejected_outputs = [valid_outputs[min_reward_index]]\n",
    "                chosen_rewards_part = [valid_rewards[max_reward_index]]\n",
    "                rejected_rewards_part = [valid_rewards[min_reward_index]]\n",
    "\n",
    "            elif strategy == \"top_bottom_percent\":\n",
    "                # Select top and bottom 20% of rewards\n",
    "                twenty_percent_num = max(1, math.ceil(len(valid_rewards) * 0.2))\n",
    "                max_indices = np.argsort(valid_rewards)[-twenty_percent_num:]\n",
    "                min_indices = np.argsort(valid_rewards)[:twenty_percent_num]\n",
    "\n",
    "                chosen_outputs = [valid_outputs[j] for j in max_indices]\n",
    "                rejected_outputs = [valid_outputs[j] for j in min_indices]\n",
    "                chosen_rewards_part = [valid_rewards[j] for j in max_indices]\n",
    "                rejected_rewards_part = [valid_rewards[j] for j in min_indices]\n",
    "\n",
    "            elif strategy == \"above_below_average\":\n",
    "                # Select rewards above and below the average\n",
    "                threshold = 0.05\n",
    "                chosen_outputs = [valid_outputs[j] for j in range(len(valid_rewards)) if valid_rewards[j] > average_reward + threshold]\n",
    "                rejected_outputs = [valid_outputs[j] for j in range(len(valid_rewards)) if valid_rewards[j] < average_reward - threshold]\n",
    "\n",
    "                # Sort and trim to ensure balanced chosen and rejected outputs\n",
    "                chosen_outputs = [x for _, x in sorted(zip(valid_rewards, chosen_outputs), reverse=True)]\n",
    "                rejected_outputs = [x for _, x in sorted(zip(valid_rewards, rejected_outputs))]\n",
    "\n",
    "                min_length = min(len(chosen_outputs), len(rejected_outputs))\n",
    "                chosen_outputs = chosen_outputs[:min_length]\n",
    "                rejected_outputs = rejected_outputs[:min_length]\n",
    "\n",
    "                chosen_rewards_part = [valid_rewards[j] for j in range(len(valid_rewards)) if valid_rewards[j] > average_reward][:min_length]\n",
    "                rejected_rewards_part = [valid_rewards[j] for j in range(len(valid_rewards)) if valid_rewards[j] < average_reward][:min_length]\n",
    "\n",
    "            elif strategy == \"above_prev_eval\":\n",
    "                # Select rewards above and below a previous evaluation average\n",
    "                chosen_outputs = [valid_outputs[j] for j in range(len(valid_rewards)) if valid_rewards[j] > prev_eval_avg]\n",
    "                rejected_outputs = [valid_outputs[j] for j in range(len(valid_rewards)) if valid_rewards[j] < prev_eval_avg]\n",
    "\n",
    "                # Sort and trim\n",
    "                chosen_outputs = [x for _, x in sorted(zip(valid_rewards, chosen_outputs), reverse=True)]\n",
    "                rejected_outputs = [x for _, x in sorted(zip(valid_rewards, rejected_outputs))]\n",
    "\n",
    "                min_length = min(len(chosen_outputs), len(rejected_outputs))\n",
    "                if min_length == 0:\n",
    "                    chosen_outputs = [valid_outputs[np.argmax(valid_rewards)]]\n",
    "                    rejected_outputs = [valid_outputs[np.argmin(valid_rewards)]]\n",
    "                    chosen_rewards_part = [valid_rewards[np.argmax(valid_rewards)]]\n",
    "                    rejected_rewards_part = [valid_rewards[np.argmin(valid_rewards)]]\n",
    "                else:\n",
    "                    chosen_outputs = chosen_outputs[:min_length]\n",
    "                    rejected_outputs = rejected_outputs[:min_length]\n",
    "                    chosen_rewards_part = [valid_rewards[j] for j in range(len(valid_rewards)) if valid_rewards[j] > prev_eval_avg][:min_length]\n",
    "                    rejected_rewards_part = [valid_rewards[j] for j in range(len(valid_rewards)) if valid_rewards[j] < prev_eval_avg][:min_length]\n",
    "\n",
    "            obs_input = pack_inputs_v2(ar_tokenizer, selected_src_encodec[i], selected_instruction[i])\n",
    "            tokenize_input = ar_tokenizer.convert_ids_to_tokens(obs_input)\n",
    "            tokenize_input_str = ar_tokenizer.convert_tokens_to_string(tokenize_input)\n",
    "            prompts.extend([tokenize_input_str] * len(chosen_outputs))\n",
    "            average_rewards.append(average_reward)\n",
    "\n",
    "            chosen.extend(chosen_outputs)\n",
    "            rejected.extend(rejected_outputs)\n",
    "            chosen_rewards.extend(chosen_rewards_part)\n",
    "            rejected_rewards.extend(rejected_rewards_part)\n",
    "        else:\n",
    "            print(f\"Not enough valid rewards for data index {i}\")\n",
    "\n",
    "    if len(selected_src_encodec) == 1:\n",
    "        chosen *= 2\n",
    "        rejected *= 2\n",
    "        prompts *= 2\n",
    "        chosen_rewards *= 2\n",
    "        rejected_rewards *= 2\n",
    "        average_rewards *= 2    \n",
    "\n",
    "    return chosen, rejected, prompts, chosen_rewards, rejected_rewards, average_rewards\n",
    "\n",
    "\n",
    "def generate_data(ar_model, \n",
    "                  ar_tokenizer, \n",
    "                  nar_model, \n",
    "                  nar_tokenizer, \n",
    "                  clap_model,\n",
    "                  asr_model,\n",
    "                  accelerator,\n",
    "                  selected_src_encodec: List[list], \n",
    "                  selected_instruction: List[str],\n",
    "                  selected_transcription: List[str],\n",
    "                  args_predict: SimpleNamespace, \n",
    "                  sample_size: int, \n",
    "                  iteration: int, \n",
    "                  agent_output_dir: str, \n",
    "                  base_path: str = \"/work/b0990106x/trl\", \n",
    "                  temperature: float = 1.0\n",
    ") -> Tuple[dict, List[float], List[float]]:\n",
    "    \"\"\"\n",
    "    Generates data for the dataset and saves info to a JSON file.\n",
    "    Returns:\n",
    "        tuple:\n",
    "            data_for_dataset (dict): A dictionary containing the data for the dataset.\n",
    "            chosen_rewards (List[float]): A list of rewards for the chosen outputs.\n",
    "            rejected_rewards (List[float]): A list of rewards for the rejected outputs.\n",
    "    \"\"\"\n",
    "    chosen, rejected, prompts, chosen_rewards, rejected_rewards, average_rewards = process_data_batch(\n",
    "        sample_size=sample_size,\n",
    "        ar_model=ar_model,\n",
    "        nar_model=nar_model,\n",
    "        ar_tokenizer=ar_tokenizer,\n",
    "        nar_tokenizer=nar_tokenizer,\n",
    "        selected_src_encodec=selected_src_encodec,\n",
    "        selected_instruction=selected_instruction,\n",
    "        selected_transcription=selected_transcription,\n",
    "        args_predict=args_predict,\n",
    "        base_path=base_path,\n",
    "        temperature=temperature,\n",
    "        iteration = iteration,\n",
    "        clap_model=clap_model,\n",
    "        asr_model=asr_model,\n",
    "        accelerator=accelerator\n",
    "    )\n",
    "\n",
    "    data = {\n",
    "        \"prompt\": prompts,\n",
    "        \"chosen\": chosen,\n",
    "        \"rejected\": rejected,\n",
    "        \"chosen_rewards\": chosen_rewards,\n",
    "        \"rejected_rewards\": rejected_rewards,\n",
    "        \"average_rewards\": average_rewards\n",
    "    }\n",
    "\n",
    "    with open(f\"{agent_output_dir}/data_iter_{iteration}.json\", \"w\") as outfile:\n",
    "        json.dump(data, outfile, indent=4)\n",
    "\n",
    "    data_for_dataset = {key: data[key] for key in [\"prompt\", \"chosen\", \"rejected\"]}\n",
    "\n",
    "    return data_for_dataset, chosen_rewards, rejected_rewards\n",
    "\n",
    "def train_iteration(model, \n",
    "                    model_checkpoint,\n",
    "                    iteration, \n",
    "                    data_size, \n",
    "                    sample_size, \n",
    "                    ar_model, \n",
    "                    ar_tokenizer,\n",
    "                    nar_model, \n",
    "                    nar_tokenizer,\n",
    "                    all_src_encodec, \n",
    "                    all_instruction, \n",
    "                    all_transcription,\n",
    "                    args_predict, \n",
    "                    agent_output_dir,\n",
    "                    model_output_dir_base, \n",
    "                    clap_model,\n",
    "                    asr_model,\n",
    "                    accelerator,\n",
    "                    beta = 0.1, \n",
    "                    temperature = 1.0,\n",
    "                    base_path=\"/work/b0990106x/trl\",\n",
    "                    resume_from_checkpoint = False,\n",
    "                    learning_rate = 5e-07,\n",
    "                    num_train_epochs = 100,\n",
    "                    max_length = 1024*9,\n",
    "                    max_prompt_length = 1024*9,\n",
    "                    max_target_length = 1024*9,\n",
    "                    per_device_train_batch_size = 1,\n",
    "                    gradient_accumulation_steps = 1,\n",
    "                    seed = 42,\n",
    "):\n",
    "    \"\"\"\n",
    "    Executes one training iteration: generates data, trains the model, and saves the output.\n",
    "    \"\"\"\n",
    "\n",
    "    selected_src_encodec = all_src_encodec[:data_size]\n",
    "    selected_instruction = all_instruction[:data_size]\n",
    "    selected_transcription = all_transcription[:data_size]\n",
    "\n",
    "    data_for_dataset, chosen_rewards, rejected_rewards = generate_data(ar_model=model,\n",
    "                                                                        ar_tokenizer=ar_tokenizer,\n",
    "                                                                        nar_model=nar_model,\n",
    "                                                                        nar_tokenizer=nar_tokenizer,\n",
    "                                                                        selected_src_encodec=selected_src_encodec,\n",
    "                                                                        selected_instruction=selected_instruction,\n",
    "                                                                        selected_transcription=selected_transcription,\n",
    "                                                                        args_predict=args_predict,\n",
    "                                                                        sample_size=sample_size,\n",
    "                                                                        iteration=iteration,\n",
    "                                                                        agent_output_dir=agent_output_dir,\n",
    "                                                                        base_path=base_path,\n",
    "                                                                        temperature=temperature,\n",
    "                                                                        clap_model=clap_model,\n",
    "                                                                        asr_model=asr_model,\n",
    "                                                                        accelerator=accelerator)\n",
    "\n",
    "    dataset = Dataset.from_dict(data_for_dataset)\n",
    "    dataset_dict = dataset.train_test_split(test_size=0.1, shuffle=True, seed=seed)\n",
    "    train_dataset = dataset_dict[\"train\"]\n",
    "    val_dataset = dataset_dict[\"test\"]\n",
    "\n",
    "    model_output_dir = f\"{model_output_dir_base}/iter_{iteration}\"\n",
    "    os.makedirs(model_output_dir, exist_ok=True)\n",
    "\n",
    "    model_ref = create_reference_model(model)\n",
    "    \n",
    "    train_model(model=model,\n",
    "                model_ref=model_ref,\n",
    "                ar_tokenizer=ar_tokenizer,\n",
    "                train_dataset=train_dataset,\n",
    "                val_dataset=val_dataset,\n",
    "                model_output_dir=model_output_dir,\n",
    "                beta=beta,\n",
    "                resume_from_checkpoint=resume_from_checkpoint,\n",
    "                model_checkpoint=model_checkpoint,\n",
    "                learning_rate = learning_rate,\n",
    "                num_train_epochs = num_train_epochs,\n",
    "                max_length = max_length,\n",
    "                max_prompt_length = max_prompt_length,\n",
    "                max_target_length = max_target_length,\n",
    "                per_device_train_batch_size = per_device_train_batch_size,\n",
    "                gradient_accumulation_steps = gradient_accumulation_steps,\n",
    "                seed = seed)\n",
    "\n",
    "    return f\"{model_output_dir}/dpo_model\", chosen_rewards, rejected_rewards"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device: cuda\n",
      "timestamp: 1123-1620\n"
     ]
    }
   ],
   "source": [
    "# Define paths and device\n",
    "base_path = \"/work/b0990106x/trl\"\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(\"device:\", device)\n",
    "\n",
    "# Timestamp\n",
    "now = datetime.now()\n",
    "ts = now.strftime(\"%m%d-%H%M\")\n",
    "print(\"timestamp:\", ts)\n",
    "\n",
    "# Output paths\n",
    "model_output_dir = os.path.join(base_path, \"model_output\", ts)\n",
    "agent_output_dir = os.path.join(base_path, \"output\", ts)\n",
    "os.makedirs(model_output_dir, exist_ok=True)\n",
    "os.makedirs(agent_output_dir, exist_ok=True)\n",
    "\n",
    "# Seed\n",
    "seed = 42\n",
    "\n",
    "# Arguments\n",
    "args_predict = SimpleNamespace(output_path=f\"{base_path}/output/{ts}/example.wav\", seed=seed, device=device)\n",
    "ar_checkpoint = \"lca0503/speech-chatgpt-base-ar-v2-epoch10-wotrans\"\n",
    "nar_checkpoint = \"lca0503/speech-chatgpt-base-nar-v2-epoch4-wotrans\"\n",
    "\n",
    "# Model and Iterations\n",
    "model_checkpoint = ar_checkpoint\n",
    "sample_size = 40\n",
    "num_iterations = 1000\n",
    "# train_selected_indices = [8, 16, 105]\n",
    "# train_selected_indices = [8, 16, 105, 132, 140]\n",
    "# train_selected_indices = [16, 105, 132, 140, 270]\n",
    "train_selected_indices = [132]\n",
    "data_size_per_iteration = len(train_selected_indices)\n",
    "\n",
    "# Training Configuration\n",
    "beta = 0.1\n",
    "learning_rate = 5e-07\n",
    "num_train_epochs = 3\n",
    "max_length = 1024 * 9\n",
    "max_prompt_length = 1024 * 9\n",
    "max_target_length = 1024 * 9\n",
    "per_device_train_batch_size = 8\n",
    "gradient_accumulation_steps = 1\n",
    "\n",
    "# Evaluation Configuration\n",
    "eval_train = True\n",
    "eval_test = True\n",
    "# eval_train = True\n",
    "# eval_test = False\n",
    "eval_train_indices = train_selected_indices\n",
    "# eval_test_indices = [386, 474, 477, 693, 1294]\n",
    "eval_test_indices = [693]\n",
    "# eval_test_indices = random.sample(range(len(selected_src_encodec)), 5)\n",
    "eval_train_data_len = 1000\n",
    "eval_test_data_len = len(eval_test_indices)\n",
    "num_eval = 10\n",
    "eval_frequency = 1\n",
    "\n",
    "# Load data\n",
    "original_src_encodec, original_instruction = extract_data_from_json('dpo_data/src_encodec.json')\n",
    "total_data_len = len(original_src_encodec)\n",
    "original_transcription = ['' for _ in range(total_data_len)]\n",
    "# 8 neighboring fields\n",
    "# 16 but i will be in a minute\n",
    "# 65 will you\n",
    "# 100 do they\n",
    "# 102 dont you\n",
    "# 105 i dont know\n",
    "# 112 why is it\n",
    "# 132 why not\n",
    "# 140 goodbye\n",
    "# 184 the idea\n",
    "# original_transcription[8] = \"neighboring fields\" # avg reward = 0.6 -> 0.8\n",
    "original_transcription[16] = \"but i will be in a minute\" # avg reward = 0.93 -> 0.94\n",
    "# original_transcription[65] = \"will you\" # avg reward = 0.2\n",
    "# original_transcription[100] = \"do they\" # avg reward = 0.15\n",
    "# original_transcription[102] = \"dont you\" # avg reward = 0.15\n",
    "original_transcription[105] = \"i dont know\" # avg reward = 0.90 -> 1.0\n",
    "# original_transcription[112] = \"why is it\" # avg reward = 0.0\n",
    "original_transcription[132] = \"why not\" # avg reward = 1.0 -> 1.0\n",
    "original_transcription[140] = \"goodbye\" # avg reward = 1.0 -> 1.0\n",
    "# original_transcription[184] = \"the idea\" # avg reward = 0.0\n",
    "original_transcription[270] = \"look\"\n",
    "\n",
    "original_transcription[386] = \"words\"\n",
    "original_transcription[474] = \"milk\"\n",
    "original_transcription[477] = \"well\"\n",
    "original_transcription[693] = \"yes\"\n",
    "original_transcription[1294] = \"i can do that\"\n",
    "# original_transcription[6159] = \"all in good time\"\n",
    "# original_transcription[8231] = \"he must be awake\"\n",
    "# original_transcription[8417] = \"who are you\"\n",
    "\n",
    "original_instruction[16] = \"Speak in an angry tone.\"\n",
    "original_instruction[105] = \"Speak in an angry tone.\"\n",
    "original_instruction[132] = \"Speak in an angry tone.\"\n",
    "original_instruction[140] = \"Speak in an angry tone.\"\n",
    "original_instruction[270] = \"Speak in an angry tone.\"\n",
    "\n",
    "original_instruction[386] = \"Speak in an angry tone.\"\n",
    "original_instruction[474] = \"Speak in an angry tone.\"\n",
    "original_instruction[477] = \"Speak in an angry tone.\"\n",
    "original_instruction[693] = \"Speak in an angry tone.\"\n",
    "original_instruction[1294] = \"Speak in an angry tone.\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Play the audio twice.\n",
      "Mildly decrease the emphasis on the higher frequencies.\n",
      "Considerably abate the bass frequencies.\n",
      "Heighten the chorus effect in the audio by a small amount.\n",
      "Hold off on playing the audio for 1 second.\n",
      "Intensify the sound of the higher frequencies.\n",
      "Give the audio a gradual increase in volume for 5 seconds from the onset.\n",
      "Add a conspicuous chorus effect to the audio.\n",
      "Significantly dampen the vibrations of the high notes.\n",
      "Decrease the pitch of the audio by a moderate amount.\n",
      "Introduce a minor adjustment to the pitch of the audio to make it lower.\n",
      "Enlarge the scope and widen the reach of the sound quality.\n",
      "Amplifying the sound to deliver a clearer and brighter rendition.\n",
      "Backtrack the sound.\n",
      "Enlarge the depth of the lower frequencies significantly.\n",
      "Refining the sound to make it more discernible and lively.\n",
      "Speak in an angry tone.\n",
      "Strengthen the chorus effect on the audio.\n",
      "During the introduction of the audio, steadily increase the volume by 5 second.\n",
      "Apply a chorus effect that's easily noticeable to the audio.\n",
      "Increase volume and clarity to brighten the overall sound output.\n",
      "Loop the audio twice.\n",
      "Over the course of 2 seconds, gradually elevate the sound level at the start of the audio.\n",
      "Implement a recognizable chorus effect to the audio for a more dynamic tone.\n",
      "Raise the audio's volume gently and gradually, extending over a duration of 5 seconds at the beginning.\n",
      "Carefully raise the volume level of the audio.\n",
      "Apply an effect that produces an amplified audio and a quick echo that is almost instantly heard.\n",
      "Aggressively pump up the lower frequencies.\n",
      "Speed up the audio to a significant degree.\n",
      "Evoke a sense of openness and depth in the audio.\n",
      "Change the direction of the audio.\n",
      "Prolong the audio by 2 seconds.\n",
      "Loop the audio twice.\n",
      "Duplicate the audio playback.\n",
      "Tarry the audio by 2 seconds.\n",
      "Boost the audio's spaciousness quotient by adding depth to the sound.\n",
      "Boost the volume of the audio for maximum audibility.\n",
      "Halt the audio's playback for 4 second.\n",
      "Significantly augment the high notes.\n",
      "Multiply the width and enrich the depth of the sound.\n",
      "Dampen the intensity of the audio sound to a great degree.\n",
      "Slowly raise the volume of the audio.\n",
      "Softly boost the treble tones.\n",
      "Slightly enhance the pitch of the audio.\n",
      "Improving the sound quality to produce a more vibrant and lively audio output. \n",
      "The bass frequencies will be noticeably lowered.\n",
      "Play the audio backwards.\n",
      "Give the audio a more distinct chorus effect by slightly increasing its intensity.\n",
      "Start the audio with a gentle lift in volume, taking 4 seconds to reach the required level.\n",
      "Run the audio twice.\n",
      "Rehear the audio two times.\n",
      "Convey a feeling of expansiveness and depth in the audio.\n",
      "Include a subtle chorus enhancement to the sound.\n",
      "Flip the audio.\n",
      "Ascertain that the volume elevates by 1 second during the audio's commencing stages.\n",
      "During the introduction of the audio, steadily increase the volume by 1 second.\n",
      "Make the audio noticeably quieter.\n",
      "Subtly decrease the audio volume.\n",
      "Reduce the audio volume to a significant extent.\n",
      "Run the audio twice.\n",
      "Strengthen the chorus effect in the audio file.\n",
      "Slightly augment the pitch of the audio.\n",
      "Start the audio with a gentle lift in volume, taking 3 seconds to reach the required level.\n",
      "Intensify the chorus effect of the audio a little more.\n",
      "Give the audio a wider sound profile by adding depth and spaciousness.\n",
      "Drastically increase the volume of the audio.\n",
      "Significantly lower the audio's volume.\n",
      "Rearrange the audio.\n",
      "Accelerate the speed of the audio substantially.\n",
      "Softly lower the audio's loudness.\n",
      "During the audio's initial stages, gradually increase the volume over 4 seconds.\n",
      "Enhance the audio's spatial dimension and depth.\n",
      "Hand over the audio file in reverse.\n",
      "Add a clear chorus effect to the audio.\n",
      "Slowly lower the loudness of the audio.\n",
      "Generate a prominent chorus effect in the audio.\n",
      "Increase the volume of the audio and enhance its impact.\n",
      "Lightly reduce the emphasis on the uppermost sound frequencies.\n",
      "Listen to the audio track two times over.\n",
      "Intensify the bottom range.\n",
      "Intensify the expanse and deepen the complexity of the audio.\n",
      "Bring down the tone of the audio considerably. \n",
      "Incorporate an echo with a prolonged delay and low intensity into the audio.\n",
      "Improve the audio's spatial quality by adding depth to the sound.\n",
      "Refining the sound by magnifying its clarity and brightness.\n",
      "Increasing the sound's clarity to make it crisper and brighter.\n",
      "Run the audio twice for clarity.\n",
      "Intensifying the sound to create a more lucid and lustrous audio.\n",
      "Substantially reduce the pitch of the audio to a very low tone.\n",
      "Replay the audio once more.\n",
      "Gradually reduce the volume of the bass frequencies.\n",
      "Increase volume and clarity to brighten the overall sound output.\n",
      "Incorporate a discreet chorus effect in the audio.\n",
      "Slightly elevate the pitch of the audio.\n",
      "Imbue the audio with a spacious and deep quality.\n",
      "Realize a conspicuous decrease in the bass spectrum.\n",
      "Increase the volume gradually over 5 seconds during the audio's opening.\n",
      "Put a 5-second pause in the audio.\n",
      "Replay the audio recording twice.\n",
      "Carefully turn up the volume of the audio.\n"
     ]
    }
   ],
   "source": [
    "# print first 100 instructions\n",
    "for i in range(100):\n",
    "    print(original_instruction[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Detected kernel version 3.10.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n",
      "/home/b0990106x/.local/lib/python3.10/site-packages/huggingface_hub/file_download.py:1150: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "/home/b0990106x/.local/lib/python3.10/site-packages/s3prl/upstream/wavlm/expert.py:37: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(ckpt)\n",
      "/home/b0990106x/miniconda3/envs/dpo/lib/python3.10/site-packages/torch/nn/utils/weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n",
      "/home/b0990106x/miniconda3/envs/dpo/lib/python3.10/site-packages/torch/nn/functional.py:5849: UserWarning: Support for mismatched key_padding_mask and attn_mask is deprecated. Use same type for both instead.\n",
      "  warnings.warn(\n",
      "/work/b0990106x/trl/CLAPS/inference.py:33: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(checkpoint_path, map_location=device)\n",
      "/home/b0990106x/miniconda3/envs/dpo/lib/python3.10/site-packages/torch/cuda/__init__.py:716: UserWarning: Can't initialize NVML\n",
      "  warnings.warn(\"Can't initialize NVML\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded checkpoint from /work/b0990106x/trl/CLAPS/pretrained/7d/cp_claps_blstm_m_50k_v3/cp_0045000/pytorch_model.bin\n"
     ]
    }
   ],
   "source": [
    "# Configuration\n",
    "sr = 24000\n",
    "text_enc_name = \"google/flan-t5-large\"\n",
    "text_enc_dim = 1024\n",
    "text_blstm_dim = 256\n",
    "speech_enc_name = \"wavlm\"\n",
    "speech_enc_dim = 768\n",
    "speech_blstm_dim = 256\n",
    "rep_dim = 512\n",
    "sub_dim = 0\n",
    "n_sub = 1\n",
    "ckpt_pth = f'{base_path}/CLAPS/pretrained/7d/cp_claps_blstm_m_50k_v3/cp_0045000'\n",
    "project_dir = \"cp_claps\"\n",
    "\n",
    "# Argument Namespace\n",
    "a = argparse.Namespace(\n",
    "    sr=sr,\n",
    "    text_enc_name=text_enc_name,\n",
    "    text_enc_dim=text_enc_dim,\n",
    "    text_blstm_dim=text_blstm_dim,\n",
    "    speech_enc_name=speech_enc_name,\n",
    "    speech_enc_dim=speech_enc_dim,\n",
    "    speech_blstm_dim=speech_blstm_dim,\n",
    "    rep_dim=rep_dim,\n",
    "    sub_dim=sub_dim,\n",
    "    n_sub=n_sub,\n",
    "    ckpt_pth=ckpt_pth,\n",
    "    project_dir=project_dir\n",
    ")\n",
    "\n",
    "# Load CLAP model\n",
    "clap_model, accelerator = load_model(a)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "medium cuda float16\n"
     ]
    }
   ],
   "source": [
    "# asr_model_size = \"tiny\"\n",
    "asr_model_size = \"medium\"\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    asr_model = WhisperModel(asr_model_size, device=\"cuda\", compute_type=\"float16\")\n",
    "    print(asr_model_size, \"cuda\", \"float16\")\n",
    "else:\n",
    "    asr_model = WhisperModel(asr_model_size, device=\"cpu\", compute_type=\"int8\")\n",
    "    print(asr_model_size, \"cpu\", \"int8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "asr model size: medium\n",
      "num_iterations: 1\n",
      "data_size_per_iteration: 1\n",
      "sample_size: 40\n",
      "beta: 0.1\n",
      "learning_rate: 5e-07\n",
      "num_train_epochs: 3\n",
      "ar_checkpoint: lca0503/speech-chatgpt-base-ar-v2-epoch10-wotrans\n",
      "nar_checkpoint: lca0503/speech-chatgpt-base-nar-v2-epoch4-wotrans\n",
      "args_predict: namespace(output_path='/work/b0990106x/trl/output/1123-1620/example.wav', seed=42, device='cuda')\n",
      "model_output_dir: /work/b0990106x/trl/model_output/1123-1620\n",
      "agent_output_dir: /work/b0990106x/trl/output/1123-1620\n",
      "base_path: /work/b0990106x/trl\n",
      "device: cuda\n",
      "eval_train_data_len: 1000\n",
      "eval_test_data_len: 1\n",
      "eval_train_indices: [132]\n",
      "eval_test_indices: [693]\n",
      "eval_train: True\n",
      "eval_test: True\n",
      "num_eval: 10\n",
      "training idx 132: Speak in an angry tone.\n",
      "evaluation idx 693: Speak in an angry tone.\n",
      "evaluation idx 132: Speak in an angry tone.\n"
     ]
    }
   ],
   "source": [
    "# Print configurations\n",
    "print(f\"asr model size: {asr_model_size}\")\n",
    "print(f\"num_iterations: {num_iterations}\")\n",
    "print(f\"data_size_per_iteration: {data_size_per_iteration}\")\n",
    "print(f\"sample_size: {sample_size}\")\n",
    "print(f\"beta: {beta}\")\n",
    "print(f\"learning_rate: {learning_rate}\")\n",
    "print(f\"num_train_epochs: {num_train_epochs}\")\n",
    "print(f\"ar_checkpoint: {ar_checkpoint}\")\n",
    "print(f\"nar_checkpoint: {nar_checkpoint}\")\n",
    "print(f\"args_predict: {args_predict}\")\n",
    "print(f\"model_output_dir: {model_output_dir}\")\n",
    "print(f\"agent_output_dir: {agent_output_dir}\")\n",
    "print(f\"base_path: {base_path}\")\n",
    "print(f\"device: {device}\")\n",
    "print(f\"eval_train_data_len: {eval_train_data_len}\")\n",
    "print(f\"eval_test_data_len: {eval_test_data_len}\")\n",
    "print(f\"eval_train_indices: {eval_train_indices}\")\n",
    "print(f\"eval_test_indices: {eval_test_indices}\")\n",
    "print(f\"eval_train: {eval_train}\")\n",
    "print(f\"eval_test: {eval_test}\")\n",
    "print(f\"num_eval: {num_eval}\")\n",
    "\n",
    "# Print training data\n",
    "for i in train_selected_indices:\n",
    "    print(f'training idx {i}: {original_instruction[i]}')\n",
    "\n",
    "# Print evaluation data\n",
    "if eval_test:\n",
    "    for i in eval_test_indices:\n",
    "        print(f'evaluation idx {i}: {original_instruction[i]}')\n",
    "\n",
    "if eval_train:\n",
    "    for i in eval_train_indices:\n",
    "        print(f'evaluation idx {i}: {original_instruction[i]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/b0990106x/miniconda3/envs/dpo/lib/python3.10/site-packages/transformers/modeling_utils.py:460: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  return torch.load(checkpoint_file, map_location=\"cpu\")\n",
      "/work/b0990106x/trl/trl/models/modeling_base.py:328: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  state_dict = loading_func(filename if not use_safe else safe_filename, **load_kwargs)\n"
     ]
    }
   ],
   "source": [
    "model = AutoModelForSeq2SeqLMWithValueHead.from_pretrained(model_checkpoint, return_dict=True)\n",
    "ar_model = BartForConditionalGeneration.from_pretrained(ar_checkpoint)\n",
    "ar_tokenizer = AutoTokenizer.from_pretrained(ar_checkpoint)\n",
    "# ar_tokenizer.pad_token = ar_tokenizer.eos_token\n",
    "nar_model = NARBartForConditionalGeneration.from_pretrained(nar_checkpoint)\n",
    "nar_tokenizer = AutoTokenizer.from_pretrained(nar_checkpoint)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logging Start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logging to: /work/b0990106x/trl/model_output/1123-1620/log_training.log\n"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "\n",
    "log_path = f'{model_output_dir}/log_training.log'\n",
    "print(f\"Logging to: {log_path}\")\n",
    "\n",
    "# Set up logging\n",
    "logging.basicConfig(\n",
    "    filename=log_path,\n",
    "    filemode='a',\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s',\n",
    "    level=logging.INFO\n",
    ")\n",
    "\n",
    "logging.info(\n",
    "    f\"Parameters:\\n\"\n",
    "    f\"Prepare Data: sample_size: {sample_size}\\n\"\n",
    "    f\"Training: num_iterations: {num_iterations}\\n\"\n",
    "    f\"Training: data_size_per_iteration: {data_size_per_iteration}\\n\"\n",
    "    f\"Training: train_selected_indices: {train_selected_indices}\\n\"\n",
    "    f\"Training: beta: {beta}\\n\"\n",
    "    f\"Training: learning_rate: {learning_rate}\\n\"\n",
    "    f\"Training: num_train_epochs: {num_train_epochs}\\n\"\n",
    "    f\"Training: max_length: {max_length}\\n\"\n",
    "    f\"Training: max_prompt_length: {max_prompt_length}\\n\"\n",
    "    f\"Training: max_target_length: {max_target_length}\\n\"\n",
    "    f\"Training: per_device_train_batch_size: {per_device_train_batch_size}\\n\"\n",
    "    f\"Training: gradient_accumulation_steps: {gradient_accumulation_steps}\\n\"\n",
    "    f\"Training: seed: {seed}\\n\"\n",
    "    f\"Training: ar_checkpoint: {ar_checkpoint}\\n\"\n",
    "    f\"Training: nar_checkpoint: {nar_checkpoint}\\n\"\n",
    "    f\"Training: args_predict: {args_predict}\\n\"\n",
    "    f\"Training: model_output_dir: {model_output_dir}\\n\"\n",
    "    f\"Training: agent_output_dir: {agent_output_dir}\\n\"\n",
    "    f\"Training: base_path: {base_path}\\n\"\n",
    "    f\"Training: device: {device}\\n\"\n",
    "    f\"Evaluation: eval_train_data_len: {eval_train_data_len}\\n\"\n",
    "    f\"Evaluation: eval_test_data_len: {eval_test_data_len}\\n\"\n",
    "    f\"Evaluation: eval_train_indices: {eval_train_indices}\\n\"\n",
    "    f\"Evaluation: eval_test_indices: {eval_test_indices}\\n\"\n",
    "    f\"Evaluation: eval_train: {eval_train}\\n\"\n",
    "    f\"Evaluation: eval_test: {eval_test}\\n\"\n",
    "    f\"Evaluation: num_eval: {num_eval}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initial Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "modified_text: why not, reward: 1.00, file_path: /work/b0990106x/trl/output/1123-1620/example_generate_eval_-1_data_132_item_0.wav\n",
      "modified_text: why not, reward: 1.00, file_path: /work/b0990106x/trl/output/1123-1620/example_generate_eval_-1_data_132_item_1.wav\n",
      "modified_text: why not, reward: 1.00, file_path: /work/b0990106x/trl/output/1123-1620/example_generate_eval_-1_data_132_item_2.wav\n",
      "modified_text: wanda, reward: 0.00, file_path: /work/b0990106x/trl/output/1123-1620/example_generate_eval_-1_data_132_item_3.wav\n",
      "modified_text: why not, reward: 1.00, file_path: /work/b0990106x/trl/output/1123-1620/example_generate_eval_-1_data_132_item_4.wav\n",
      "modified_text: why not, reward: 1.00, file_path: /work/b0990106x/trl/output/1123-1620/example_generate_eval_-1_data_132_item_5.wav\n",
      "modified_text: why not, reward: 1.00, file_path: /work/b0990106x/trl/output/1123-1620/example_generate_eval_-1_data_132_item_6.wav\n",
      "modified_text: why not, reward: 1.00, file_path: /work/b0990106x/trl/output/1123-1620/example_generate_eval_-1_data_132_item_7.wav\n",
      "modified_text: white mog, reward: 0.00, file_path: /work/b0990106x/trl/output/1123-1620/example_generate_eval_-1_data_132_item_8.wav\n",
      "modified_text: why not, reward: 1.00, file_path: /work/b0990106x/trl/output/1123-1620/example_generate_eval_-1_data_132_item_9.wav\n",
      "average asr reward: 0.80\n",
      "modified_text: yes, reward: 1.00, file_path: /work/b0990106x/trl/output/1123-1620/example_generate_eval_-1_data_693_item_0.wav\n",
      "modified_text: yes, reward: 1.00, file_path: /work/b0990106x/trl/output/1123-1620/example_generate_eval_-1_data_693_item_1.wav\n",
      "modified_text: gits, reward: 0.00, file_path: /work/b0990106x/trl/output/1123-1620/example_generate_eval_-1_data_693_item_2.wav\n",
      "modified_text: yes, reward: 1.00, file_path: /work/b0990106x/trl/output/1123-1620/example_generate_eval_-1_data_693_item_3.wav\n",
      "modified_text: yes, reward: 1.00, file_path: /work/b0990106x/trl/output/1123-1620/example_generate_eval_-1_data_693_item_4.wav\n",
      "modified_text: yes, reward: 1.00, file_path: /work/b0990106x/trl/output/1123-1620/example_generate_eval_-1_data_693_item_5.wav\n",
      "modified_text: yes, reward: 1.00, file_path: /work/b0990106x/trl/output/1123-1620/example_generate_eval_-1_data_693_item_6.wav\n",
      "modified_text: yes, reward: 1.00, file_path: /work/b0990106x/trl/output/1123-1620/example_generate_eval_-1_data_693_item_7.wav\n",
      "modified_text: yes, reward: 1.00, file_path: /work/b0990106x/trl/output/1123-1620/example_generate_eval_-1_data_693_item_8.wav\n",
      "modified_text: yes, reward: 1.00, file_path: /work/b0990106x/trl/output/1123-1620/example_generate_eval_-1_data_693_item_9.wav\n",
      "average asr reward: 0.90\n"
     ]
    }
   ],
   "source": [
    "# Start time\n",
    "total_start_time = time.time()\n",
    "\n",
    "def evaluate_model(eval_type, eval_indices, eval_data_len):\n",
    "    metrics, rewards = eval_dpo_claps_asr_batch(\n",
    "        nar_model=nar_model,\n",
    "        ar_tokenizer=ar_tokenizer,\n",
    "        nar_tokenizer=nar_tokenizer,\n",
    "        trained_model=model,\n",
    "        args_predict=args_predict,\n",
    "        all_src_encodec=original_src_encodec,\n",
    "        all_instruction=original_instruction,\n",
    "        all_ground_truth=original_transcription,\n",
    "        iteration=-1,\n",
    "        num_evaluations=num_eval,\n",
    "        eval_data_len=eval_data_len,\n",
    "        selected_indices=eval_indices,\n",
    "        device=device,\n",
    "        clap_model=clap_model,\n",
    "        asr_model=asr_model,\n",
    "        accelerator=accelerator,\n",
    "    )\n",
    "    logging.info(f\"Original Model {eval_type} Set Evaluation:\")\n",
    "    logging.info(f\"Original model metrics on {eval_type} set: {metrics}\")\n",
    "    logging.info(f\"Original model rewards on {eval_type} set: {rewards}\")\n",
    "\n",
    "    reward_list = []\n",
    "    for reward_group in rewards:\n",
    "        filtered_rewards = [r for r in reward_group if r is not None]\n",
    "        reward_list.append(None if not filtered_rewards else np.mean(filtered_rewards))\n",
    "    \n",
    "    logging.info(f\"Original model reward list on {eval_type} set: {reward_list}\")\n",
    "    filtered_reward_list = [r for r in reward_list if r is not None]\n",
    "    avg_reward = None if not filtered_reward_list else np.mean(filtered_reward_list)\n",
    "    logging.info(f\"Original model average rewards on {eval_type} set: {avg_reward}\")\n",
    "\n",
    "if eval_train:\n",
    "    evaluate_model(\"Train\", eval_train_indices, eval_train_data_len)\n",
    "\n",
    "if eval_test:\n",
    "    evaluate_model(\"Test\", eval_test_indices, eval_test_data_len)\n",
    "\n",
    "# Prepare data for training\n",
    "if train_selected_indices:\n",
    "    batch_src_encodec = [original_src_encodec[i] for i in train_selected_indices]\n",
    "    batch_instruction = [original_instruction[i] for i in train_selected_indices]\n",
    "    batch_transcription = [original_transcription[i] for i in train_selected_indices]\n",
    "    logging.info(f\"Processing data from selected indices: {train_selected_indices}\")\n",
    "else:\n",
    "    start_idx, end_idx = 0, data_size_per_iteration\n",
    "    batch_src_encodec = original_src_encodec[start_idx:end_idx]\n",
    "    batch_instruction = original_instruction[start_idx:end_idx]\n",
    "    batch_transcription = original_transcription[start_idx:end_idx]\n",
    "    logging.info(f\"Processing data from index {start_idx} to {end_idx}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "\n",
    "import os\n",
    "os.environ[\"WANDB__SERVICE\"] = \"disabled\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Start training iterations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "modified_text: why not, reward: 1.00, file_path: /work/b0990106x/trl/output/1123-1620/example_generate_data_0_item_0.wav\n",
      "modified_text: why not, reward: 1.00, file_path: /work/b0990106x/trl/output/1123-1620/example_generate_data_0_item_1.wav\n",
      "modified_text: why not, reward: 1.00, file_path: /work/b0990106x/trl/output/1123-1620/example_generate_data_0_item_2.wav\n",
      "modified_text: why not, reward: 1.00, file_path: /work/b0990106x/trl/output/1123-1620/example_generate_data_0_item_3.wav\n",
      "modified_text: wake up, reward: 0.00, file_path: /work/b0990106x/trl/output/1123-1620/example_generate_data_0_item_4.wav\n",
      "modified_text: why not, reward: 1.00, file_path: /work/b0990106x/trl/output/1123-1620/example_generate_data_0_item_5.wav\n",
      "modified_text: wine pop, reward: 0.00, file_path: /work/b0990106x/trl/output/1123-1620/example_generate_data_0_item_6.wav\n",
      "modified_text: winebuck, reward: 0.00, file_path: /work/b0990106x/trl/output/1123-1620/example_generate_data_0_item_7.wav\n",
      "modified_text: why not, reward: 1.00, file_path: /work/b0990106x/trl/output/1123-1620/example_generate_data_0_item_8.wav\n",
      "modified_text: why not, reward: 1.00, file_path: /work/b0990106x/trl/output/1123-1620/example_generate_data_0_item_9.wav\n",
      "modified_text: why not, reward: 1.00, file_path: /work/b0990106x/trl/output/1123-1620/example_generate_data_0_item_10.wav\n",
      "modified_text: why not, reward: 1.00, file_path: /work/b0990106x/trl/output/1123-1620/example_generate_data_0_item_11.wav\n",
      "modified_text: why not, reward: 1.00, file_path: /work/b0990106x/trl/output/1123-1620/example_generate_data_0_item_12.wav\n",
      "modified_text: why not, reward: 1.00, file_path: /work/b0990106x/trl/output/1123-1620/example_generate_data_0_item_13.wav\n",
      "modified_text: why not, reward: 1.00, file_path: /work/b0990106x/trl/output/1123-1620/example_generate_data_0_item_14.wav\n",
      "modified_text: why not, reward: 1.00, file_path: /work/b0990106x/trl/output/1123-1620/example_generate_data_0_item_15.wav\n",
      "modified_text: why not, reward: 1.00, file_path: /work/b0990106x/trl/output/1123-1620/example_generate_data_0_item_16.wav\n",
      "modified_text: why not, reward: 1.00, file_path: /work/b0990106x/trl/output/1123-1620/example_generate_data_0_item_17.wav\n",
      "modified_text: why not, reward: 1.00, file_path: /work/b0990106x/trl/output/1123-1620/example_generate_data_0_item_18.wav\n",
      "modified_text: why not, reward: 1.00, file_path: /work/b0990106x/trl/output/1123-1620/example_generate_data_0_item_19.wav\n",
      "modified_text: why not, reward: 1.00, file_path: /work/b0990106x/trl/output/1123-1620/example_generate_data_0_item_20.wav\n",
      "modified_text: why not, reward: 1.00, file_path: /work/b0990106x/trl/output/1123-1620/example_generate_data_0_item_21.wav\n",
      "modified_text: why what, reward: 0.50, file_path: /work/b0990106x/trl/output/1123-1620/example_generate_data_0_item_22.wav\n",
      "modified_text: why not, reward: 1.00, file_path: /work/b0990106x/trl/output/1123-1620/example_generate_data_0_item_23.wav\n",
      "modified_text: why not, reward: 1.00, file_path: /work/b0990106x/trl/output/1123-1620/example_generate_data_0_item_24.wav\n",
      "modified_text: why not, reward: 1.00, file_path: /work/b0990106x/trl/output/1123-1620/example_generate_data_0_item_25.wav\n",
      "modified_text: why not, reward: 1.00, file_path: /work/b0990106x/trl/output/1123-1620/example_generate_data_0_item_26.wav\n",
      "modified_text: why not, reward: 1.00, file_path: /work/b0990106x/trl/output/1123-1620/example_generate_data_0_item_27.wav\n",
      "modified_text: why not, reward: 1.00, file_path: /work/b0990106x/trl/output/1123-1620/example_generate_data_0_item_28.wav\n",
      "modified_text: why not, reward: 1.00, file_path: /work/b0990106x/trl/output/1123-1620/example_generate_data_0_item_29.wav\n",
      "modified_text: why not, reward: 1.00, file_path: /work/b0990106x/trl/output/1123-1620/example_generate_data_0_item_30.wav\n",
      "modified_text: why not, reward: 1.00, file_path: /work/b0990106x/trl/output/1123-1620/example_generate_data_0_item_31.wav\n",
      "modified_text: why not, reward: 1.00, file_path: /work/b0990106x/trl/output/1123-1620/example_generate_data_0_item_32.wav\n",
      "modified_text: why not, reward: 1.00, file_path: /work/b0990106x/trl/output/1123-1620/example_generate_data_0_item_33.wav\n",
      "modified_text: why not, reward: 1.00, file_path: /work/b0990106x/trl/output/1123-1620/example_generate_data_0_item_34.wav\n",
      "modified_text: why not, reward: 1.00, file_path: /work/b0990106x/trl/output/1123-1620/example_generate_data_0_item_35.wav\n",
      "modified_text: why not, reward: 1.00, file_path: /work/b0990106x/trl/output/1123-1620/example_generate_data_0_item_36.wav\n",
      "modified_text: why not, reward: 1.00, file_path: /work/b0990106x/trl/output/1123-1620/example_generate_data_0_item_37.wav\n",
      "modified_text: why not, reward: 1.00, file_path: /work/b0990106x/trl/output/1123-1620/example_generate_data_0_item_38.wav\n",
      "modified_text: why not, reward: 1.00, file_path: /work/b0990106x/trl/output/1123-1620/example_generate_data_0_item_39.wav\n",
      "Average reward for data index 0: 0.167562974919565\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 14/14 [00:00<00:00, 781.71 examples/s]\n",
      "Map: 100%|██████████| 2/2 [00:00<00:00, 376.69 examples/s]\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend. Please refer to https://wandb.me/wandb-core for more information.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mb09901066\u001b[0m (\u001b[33mb09901066_alan\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.18.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/work/b0990106x/trl/wandb/run-20241123_162152-fvjfmfa6</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/b09901066_alan/huggingface/runs/fvjfmfa6' target=\"_blank\">volcanic-snowflake-145</a></strong> to <a href='https://wandb.ai/b09901066_alan/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/b09901066_alan/huggingface' target=\"_blank\">https://wandb.ai/b09901066_alan/huggingface</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/b09901066_alan/huggingface/runs/fvjfmfa6' target=\"_blank\">https://wandb.ai/b09901066_alan/huggingface/runs/fvjfmfa6</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='6' max='6' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [6/6 00:00, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "modified_text: why not, reward: 1.00, file_path: /work/b0990106x/trl/output/1123-1620/example_generate_eval_0_data_132_item_0.wav\n",
      "modified_text: why not, reward: 1.00, file_path: /work/b0990106x/trl/output/1123-1620/example_generate_eval_0_data_132_item_1.wav\n",
      "modified_text: why not, reward: 1.00, file_path: /work/b0990106x/trl/output/1123-1620/example_generate_eval_0_data_132_item_2.wav\n",
      "modified_text: why not, reward: 1.00, file_path: /work/b0990106x/trl/output/1123-1620/example_generate_eval_0_data_132_item_3.wav\n",
      "modified_text: why not, reward: 1.00, file_path: /work/b0990106x/trl/output/1123-1620/example_generate_eval_0_data_132_item_4.wav\n",
      "modified_text: my bod, reward: 0.00, file_path: /work/b0990106x/trl/output/1123-1620/example_generate_eval_0_data_132_item_5.wav\n",
      "modified_text: why not, reward: 1.00, file_path: /work/b0990106x/trl/output/1123-1620/example_generate_eval_0_data_132_item_6.wav\n",
      "modified_text: why not, reward: 1.00, file_path: /work/b0990106x/trl/output/1123-1620/example_generate_eval_0_data_132_item_7.wav\n",
      "modified_text: why not, reward: 1.00, file_path: /work/b0990106x/trl/output/1123-1620/example_generate_eval_0_data_132_item_8.wav\n",
      "modified_text: why not, reward: 1.00, file_path: /work/b0990106x/trl/output/1123-1620/example_generate_eval_0_data_132_item_9.wav\n",
      "average asr reward: 0.90\n",
      "modified_text: yes, reward: 1.00, file_path: /work/b0990106x/trl/output/1123-1620/example_generate_eval_0_data_693_item_0.wav\n",
      "modified_text: yes, reward: 1.00, file_path: /work/b0990106x/trl/output/1123-1620/example_generate_eval_0_data_693_item_1.wav\n",
      "modified_text: yes, reward: 1.00, file_path: /work/b0990106x/trl/output/1123-1620/example_generate_eval_0_data_693_item_2.wav\n",
      "modified_text: get s, reward: 0.00, file_path: /work/b0990106x/trl/output/1123-1620/example_generate_eval_0_data_693_item_3.wav\n",
      "modified_text: yes, reward: 1.00, file_path: /work/b0990106x/trl/output/1123-1620/example_generate_eval_0_data_693_item_4.wav\n",
      "modified_text: yes, reward: 1.00, file_path: /work/b0990106x/trl/output/1123-1620/example_generate_eval_0_data_693_item_5.wav\n",
      "modified_text: yes, reward: 1.00, file_path: /work/b0990106x/trl/output/1123-1620/example_generate_eval_0_data_693_item_6.wav\n",
      "modified_text: yes, reward: 1.00, file_path: /work/b0990106x/trl/output/1123-1620/example_generate_eval_0_data_693_item_7.wav\n",
      "modified_text: yes, reward: 1.00, file_path: /work/b0990106x/trl/output/1123-1620/example_generate_eval_0_data_693_item_8.wav\n",
      "modified_text: yes, reward: 1.00, file_path: /work/b0990106x/trl/output/1123-1620/example_generate_eval_0_data_693_item_9.wav\n",
      "average asr reward: 0.90\n"
     ]
    }
   ],
   "source": [
    "disable_tqdm = not os.isatty(1)\n",
    "\n",
    "def evaluate_iteration(eval_type, iteration, eval_indices, eval_data_len):\n",
    "    metrics, rewards = eval_dpo_claps_asr_batch(\n",
    "        nar_model=nar_model,\n",
    "        ar_tokenizer=ar_tokenizer,\n",
    "        nar_tokenizer=nar_tokenizer,\n",
    "        trained_model=model,\n",
    "        args_predict=args_predict,\n",
    "        all_src_encodec=original_src_encodec,\n",
    "        all_instruction=original_instruction,\n",
    "        all_ground_truth=original_transcription,\n",
    "        iteration=iteration,\n",
    "        num_evaluations=num_eval,\n",
    "        eval_data_len=eval_data_len,\n",
    "        selected_indices=eval_indices,\n",
    "        device=device,\n",
    "        clap_model=clap_model,\n",
    "        asr_model=asr_model,\n",
    "        accelerator=accelerator\n",
    "    )\n",
    "    logging.info(f\"Trained Model Iteration {iteration} {eval_type} Set Evaluation:\")\n",
    "    logging.info(f\"EVAL: Cosine_Sim metrics {eval_type} Set for iteration {iteration}: {metrics}\")\n",
    "    logging.info(f\"EVAL: Cosine_Sim score {eval_type} Set for iteration {iteration}: {rewards}\")\n",
    "\n",
    "    reward_list = [np.mean([r for r in reward_group if r is not None]) if reward_group else None for reward_group in rewards]\n",
    "    logging.info(f\"EVAL: Trained model Cosine_Sim score list on {eval_type} set: {reward_list}\")\n",
    "    filtered_reward_list = [r for r in reward_list if r is not None]\n",
    "    avg_reward = np.mean(filtered_reward_list) if filtered_reward_list else None\n",
    "    logging.info(f\"EVAL: Trained model average Cosine_Sim score on {eval_type} set: {avg_reward}\")\n",
    "\n",
    "for iteration in tqdm(range(num_iterations), desc=\"Training Iterations\", disable=disable_tqdm):\n",
    "    logging.info(f\"-----------Starting iteration {iteration}-----------\")\n",
    "\n",
    "    resume = False\n",
    "\n",
    "    model_checkpoint, chosen_rewards, rejected_rewards = train_iteration(\n",
    "        model=model,\n",
    "        model_checkpoint=model_checkpoint,\n",
    "        iteration=iteration,\n",
    "        data_size=data_size_per_iteration,\n",
    "        sample_size=sample_size,\n",
    "        ar_model=ar_model,\n",
    "        ar_tokenizer=ar_tokenizer,\n",
    "        nar_model=nar_model,\n",
    "        nar_tokenizer=nar_tokenizer,\n",
    "        all_src_encodec=batch_src_encodec,\n",
    "        all_instruction=batch_instruction,\n",
    "        all_transcription=batch_transcription,\n",
    "        args_predict=args_predict,\n",
    "        agent_output_dir=agent_output_dir,\n",
    "        model_output_dir_base=model_output_dir,\n",
    "        temperature=1.0,\n",
    "        beta=beta,\n",
    "        base_path=base_path,\n",
    "        resume_from_checkpoint=resume,\n",
    "        learning_rate=learning_rate,\n",
    "        num_train_epochs=num_train_epochs,\n",
    "        max_length=max_length,\n",
    "        max_prompt_length=max_prompt_length,\n",
    "        max_target_length=max_target_length,\n",
    "        per_device_train_batch_size=per_device_train_batch_size,\n",
    "        gradient_accumulation_steps=gradient_accumulation_steps,\n",
    "        seed=seed,\n",
    "        clap_model=clap_model,\n",
    "        asr_model=asr_model,\n",
    "        accelerator=accelerator\n",
    "    )\n",
    "\n",
    "    logging.info(f\"Chosen rewards for iteration {iteration}: {chosen_rewards}\")\n",
    "    logging.info(f\"Rejected rewards for iteration {iteration}: {rejected_rewards}\")\n",
    "\n",
    "    if (iteration + 1) % eval_frequency == 0:\n",
    "        if eval_train:\n",
    "            evaluate_iteration(\"Train\", iteration, eval_train_indices, eval_train_data_len)\n",
    "        if eval_test:\n",
    "            evaluate_iteration(\"Test\", iteration, eval_test_indices, eval_test_data_len)\n",
    "\n",
    "    logging.info(f\"-----------Finished iteration {iteration}-----------\")\n",
    "\n",
    "total_end_time = time.time()\n",
    "total_time_taken = total_end_time - total_start_time\n",
    "logging.info(f\"Total time taken for the entire process: {total_time_taken:.2f} seconds\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dpo",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
