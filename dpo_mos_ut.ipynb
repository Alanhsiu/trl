{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"/work/b0990106x/trl/vc\")\n",
    "import importlib\n",
    "import vc\n",
    "importlib.reload(vc)\n",
    "import torch\n",
    "from vc.trainer_encodec_vc_inference import pack_inputs_v2, get_ar_prediction_audio_batch\n",
    "from types import SimpleNamespace\n",
    "from transformers import BartForConditionalGeneration, AutoTokenizer\n",
    "from datasets import Dataset\n",
    "from trl import DPOTrainer, DPOConfig, AutoModelForSeq2SeqLMWithValueHead, create_reference_model\n",
    "from vc.encodec_model.nar_bart_model import NARBartForConditionalGeneration\n",
    "from datetime import datetime\n",
    "import os\n",
    "import numpy as np\n",
    "from dpo_eval import get_reward_mos, eval_dpo_mos\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "from typing import List, Tuple\n",
    "import random\n",
    "import soundfile as sf\n",
    "import math\n",
    "import tempfile\n",
    "from pathlib import Path\n",
    "import utmosv2\n",
    "import shutil\n",
    "\n",
    "\n",
    "sys.path.append('/work/b0990106x/trl/CLAPS')\n",
    "\n",
    "import wandb\n",
    "wandb.init(project=\"dpo\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Utility Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seed(seed):\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(seed)\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "def generate_output_batch(\n",
    "        model,\n",
    "        nar_model, \n",
    "        ar_tokenizer, \n",
    "        nar_tokenizer, \n",
    "        utmos_model,\n",
    "        src_encodec: list, \n",
    "        instruction: list, \n",
    "        args_predict: SimpleNamespace, \n",
    "        episode_counter: int = 0, \n",
    "        temperature: float = 1.0\n",
    ") -> tuple[float, str]:\n",
    "    \n",
    "    # Generate predictions using the AR model\n",
    "    start_time = time.time()\n",
    "    audio_list, decode_ar_list = get_ar_prediction_audio_batch(\n",
    "        args_predict, model, nar_model, ar_tokenizer, nar_tokenizer, src_encodec, instruction, episode_counter, temperature=temperature\n",
    "    )\n",
    "    print(f\"Time taken for generating predictions: {time.time() - start_time} seconds\")\n",
    "    \n",
    "    reward_list = []\n",
    "    valid_audio_paths = []\n",
    "    \n",
    "    # start_time = time.time()\n",
    "    # with tempfile.TemporaryDirectory() as temp_dir:\n",
    "    #     temp_dir_path = Path(temp_dir)\n",
    "        \n",
    "    #     start_eval_time = time.time()\n",
    "    #     for i, audio in enumerate(audio_list): \n",
    "    #         if audio is not None:\n",
    "    #             output_path_ckpt = temp_dir_path / f\"generate_{episode_counter}_item_{i}.wav\"\n",
    "    #             sf.write(output_path_ckpt, np.ravel(audio), samplerate=24000)\n",
    "    #             valid_audio_paths.append(output_path_ckpt)\n",
    "    #         else: \n",
    "    #             reward_list.append(0)\n",
    "    #     print(f\"Time taken for writing audio files: {time.time() - start_eval_time} seconds\")\n",
    "        \n",
    "    #     start_predict_time = time.time()\n",
    "    #     print(f\"Length of valid audio paths: {len(valid_audio_paths)}\")\n",
    "    #     if valid_audio_paths:\n",
    "    #         mos_predictions = utmos_model.predict(\n",
    "    #             input_dir=temp_dir_path, \n",
    "    #             verbose=False,\n",
    "    #             batch_size=32,\n",
    "    #             num_workers=32\n",
    "    #         )\n",
    "            \n",
    "    #         for mos in mos_predictions:\n",
    "    #             predicted_mos = mos.get(\"predicted_mos\", 0) \n",
    "    #             reward = predicted_mos / 5\n",
    "    #             reward_list.append(reward)\n",
    "    #     print(f\"Time taken for predicting MOS: {time.time() - start_predict_time} seconds\")\n",
    "    # print(f\"Time taken for evaluating predictions: {time.time() - start_time} seconds\")\n",
    "    \n",
    "    start_time = time.time()\n",
    "\n",
    "    temp_dir_path = Path(\"/dev/shm/temp_audio_files\")\n",
    "    temp_dir_path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    start_eval_time = time.time()\n",
    "    for i, audio in enumerate(audio_list): \n",
    "        if audio is not None:\n",
    "            output_path_ckpt = temp_dir_path / f\"generate_{episode_counter}_item_{i}.wav\"\n",
    "            sf.write(output_path_ckpt, np.ravel(audio), samplerate=24000)\n",
    "            valid_audio_paths.append(output_path_ckpt)\n",
    "        else: \n",
    "            reward_list.append(0)\n",
    "    print(f\"Time taken for writing audio files: {time.time() - start_eval_time} seconds\")\n",
    "\n",
    "    start_predict_time = time.time()\n",
    "    print(f\"Length of valid audio paths: {len(valid_audio_paths)}\")\n",
    "    if valid_audio_paths:\n",
    "        mos_predictions = utmos_model.predict(\n",
    "            input_dir=temp_dir_path, \n",
    "            verbose=False,\n",
    "            batch_size=32,\n",
    "            num_workers=32\n",
    "        )\n",
    "\n",
    "        for mos in mos_predictions:\n",
    "            predicted_mos = mos.get(\"predicted_mos\", 0) \n",
    "            reward = predicted_mos / 5\n",
    "            reward_list.append(reward)\n",
    "    print(f\"Time taken for predicting MOS: {time.time() - start_predict_time} seconds\")\n",
    "\n",
    "    print(f\"Time taken for evaluating predictions: {time.time() - start_time} seconds\")\n",
    "\n",
    "    shutil.rmtree(temp_dir_path)\n",
    "        \n",
    "    tokenized_decode_ar_list = []\n",
    "    for decode_ar in decode_ar_list:\n",
    "        list_decode_ar = decode_ar.flatten().tolist()   \n",
    "        filtered_decode_ar_list = list_decode_ar[2:-1]\n",
    "        decode_ar_tokens = ar_tokenizer.convert_ids_to_tokens(filtered_decode_ar_list)\n",
    "        tokenized_decode_ar = ar_tokenizer.convert_tokens_to_string(decode_ar_tokens)\n",
    "        tokenized_decode_ar_list.append(tokenized_decode_ar)\n",
    "        \n",
    "    return reward_list, tokenized_decode_ar_list\n",
    "\n",
    "def extract_data_from_json(file_path: str) -> Tuple[List[list], List[str], List[list]]:\n",
    "    with open(file_path, 'r') as f:\n",
    "        data = json.load(f)\n",
    "\n",
    "    all_src_encodec = [item[\"src_encodec\"] for item in data]\n",
    "    all_instruction = [item[\"instruction\"] for item in data]\n",
    "\n",
    "    return all_src_encodec, all_instruction\n",
    "\n",
    "def train_model(\n",
    "        model,\n",
    "        model_ref,\n",
    "        ar_tokenizer,\n",
    "        train_dataset: Dataset,\n",
    "        val_dataset: Dataset,\n",
    "        model_output_dir: str,\n",
    "        beta: float,\n",
    "        resume_from_checkpoint: bool,\n",
    "        model_checkpoint: str,\n",
    "        learning_rate: float = 5e-07,\n",
    "        num_train_epochs: int = 200,\n",
    "        max_length: int = 1024*9,\n",
    "        max_prompt_length: int = 1024*9,\n",
    "        max_target_length: int = 1024*9,\n",
    "        per_device_train_batch_size: int = 1,\n",
    "        gradient_accumulation_steps: int = 1,\n",
    "        seed: int = 42\n",
    ") -> None:\n",
    "\n",
    "    training_args = DPOConfig(\n",
    "        beta = beta,\n",
    "        output_dir = model_output_dir,\n",
    "        resume_from_checkpoint = model_checkpoint if resume_from_checkpoint else None,\n",
    "        seed = seed,\n",
    "        per_device_train_batch_size = per_device_train_batch_size,\n",
    "        num_train_epochs = num_train_epochs,\n",
    "        gradient_accumulation_steps = gradient_accumulation_steps,\n",
    "        learning_rate = learning_rate,\n",
    "        max_length = max_length,\n",
    "        max_prompt_length = max_prompt_length,\n",
    "        max_target_length = max_target_length,\n",
    "        evaluation_strategy=\"steps\",\n",
    "        save_steps = 5000,\n",
    "        logging_dir = f\"{model_output_dir}/logs\"\n",
    "    )\n",
    "    \n",
    "    trainer = DPOTrainer(\n",
    "        model=model,\n",
    "        ref_model=model_ref,\n",
    "        args=training_args,\n",
    "        tokenizer=ar_tokenizer,\n",
    "        train_dataset=train_dataset,\n",
    "        eval_dataset=val_dataset,\n",
    "    )\n",
    "    # Train the model\n",
    "    trainer.train()\n",
    "\n",
    "    # Save the model\n",
    "    model.config.to_json_file(f\"{model_output_dir}/config.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "def process_data_batch(sample_size: int, \n",
    "                       model,\n",
    "                        nar_model, \n",
    "                        ar_tokenizer, \n",
    "                        nar_tokenizer, \n",
    "                        utmos_model,\n",
    "                        selected_src_encodec: List[list], \n",
    "                        selected_instruction: List[str],\n",
    "                        args_predict: SimpleNamespace, \n",
    "                        temperature: float = 1.0, \n",
    "                        iteration: int = 0\n",
    ") -> Tuple[List[str], List[str], List[str], List[float], List[float], List[float]]:\n",
    "    # If sample size is 1, we cannot choose the best and worst outputs\n",
    "    if sample_size < 2:\n",
    "        raise ValueError(\"Parameter 'sample_size' must be greater than 1.\")\n",
    "\n",
    "    chosen, rejected, prompts, chosen_rewards, rejected_rewards, average_rewards = [], [], [], [], [], []\n",
    "\n",
    "    for i in tqdm(range(len(selected_src_encodec)), desc=\"Processing Data\", disable=True):\n",
    "        rewards, tokenized_outputs = [], []\n",
    "        size_of_packed_input = (\n",
    "            len(selected_src_encodec[i][0]) +\n",
    "            len(ar_tokenizer(selected_instruction[i])[\"input_ids\"][1:-1]) +\n",
    "            3\n",
    "        )\n",
    "        if 4 < size_of_packed_input <= 1024:\n",
    "            selected_src_encodec_list = [selected_src_encodec[i]]*sample_size\n",
    "            selected_instruction_list = [selected_instruction[i]]*sample_size\n",
    "            rewards, tokenized_outputs = generate_output_batch(\n",
    "                model=model,\n",
    "                nar_model=nar_model, \n",
    "                ar_tokenizer=ar_tokenizer, \n",
    "                nar_tokenizer=nar_tokenizer,\n",
    "                utmos_model=utmos_model,\n",
    "                src_encodec = selected_src_encodec_list,\n",
    "                instruction=selected_instruction_list, \n",
    "                args_predict=args_predict,\n",
    "                episode_counter=f\"data_{i}\",\n",
    "                temperature=temperature\n",
    "            )\n",
    "\n",
    "        valid_rewards = [r for r in rewards if r is not None]\n",
    "        valid_outputs = [tokenized_outputs[j] for j in range(len(rewards)) if rewards[j] is not None]\n",
    "\n",
    "        if len(valid_rewards) >= 2:\n",
    "            # choose first 20% of the data and last 20% of the data \n",
    "            twenty_percent_num = math.ceil(len(valid_rewards)/2 * 0.2)\n",
    "            max_reward_indexs = np.argsort(valid_rewards)[-twenty_percent_num:]\n",
    "            min_reward_indexs = np.argsort(valid_rewards)[:twenty_percent_num]\n",
    "            average_reward = np.mean(valid_rewards)\n",
    "            chosen_outputs = [valid_outputs[j] for j in max_reward_indexs]\n",
    "            rejected_outputs = [valid_outputs[j] for j in min_reward_indexs]\n",
    "\n",
    "            obs_input = pack_inputs_v2(ar_tokenizer, selected_src_encodec[i], selected_instruction[i])\n",
    "            tokenize_input = ar_tokenizer.convert_ids_to_tokens(obs_input)\n",
    "            tokenize_input_str = ar_tokenizer.convert_tokens_to_string(tokenize_input)\n",
    "            prompts.extend([tokenize_input_str] * len(chosen_outputs))\n",
    "            average_rewards.append(average_reward)\n",
    "            \n",
    "            chosen.extend(chosen_outputs)\n",
    "            chosen_rewards.extend([valid_rewards[j] for j in max_reward_indexs])\n",
    "            rejected.extend(rejected_outputs)\n",
    "            rejected_rewards.extend([valid_rewards[j] for j in min_reward_indexs])\n",
    "        else:\n",
    "            print(f\"Not enough valid rewards for data index {i}\")\n",
    "\n",
    "    # If there is only one data, we need to double the data because we need it for training set and validation set\n",
    "    if len(selected_src_encodec) == 1:\n",
    "        chosen *= 2\n",
    "        rejected *= 2\n",
    "        prompts *= 2\n",
    "        chosen_rewards *= 2\n",
    "        rejected_rewards *= 2\n",
    "        average_rewards *= 2    \n",
    "    \n",
    "    return chosen, rejected, prompts, chosen_rewards, rejected_rewards, average_rewards\n",
    "\n",
    "def generate_data(model,\n",
    "                  ar_tokenizer, \n",
    "                  nar_model, \n",
    "                  nar_tokenizer, \n",
    "                  utmos_model,\n",
    "                  selected_src_encodec: List[list], \n",
    "                  selected_instruction: List[str],\n",
    "                  args_predict: SimpleNamespace, \n",
    "                  sample_size: int, \n",
    "                  iteration: int, \n",
    "                  agent_output_dir: str, \n",
    "                  temperature: float = 1.0\n",
    ") -> Tuple[dict, List[float], List[float]]:\n",
    "    \n",
    "    chosen, rejected, prompts, chosen_rewards, rejected_rewards, average_rewards = process_data_batch(\n",
    "        sample_size=sample_size,\n",
    "        model=model,\n",
    "        nar_model=nar_model,\n",
    "        ar_tokenizer=ar_tokenizer,\n",
    "        nar_tokenizer=nar_tokenizer,\n",
    "        utmos_model=utmos_model,\n",
    "        selected_src_encodec=selected_src_encodec,\n",
    "        selected_instruction=selected_instruction,\n",
    "        args_predict=args_predict,\n",
    "        temperature=temperature,\n",
    "        iteration = iteration\n",
    "    )\n",
    "\n",
    "    data = {\n",
    "        \"prompt\": prompts,\n",
    "        \"chosen\": chosen,\n",
    "        \"rejected\": rejected,\n",
    "        \"chosen_rewards\": chosen_rewards,\n",
    "        \"rejected_rewards\": rejected_rewards,\n",
    "        \"average_rewards\": average_rewards\n",
    "    }\n",
    "\n",
    "    with open(f\"{agent_output_dir}/data_iter_{iteration}.json\", \"w\") as outfile:\n",
    "        json.dump(data, outfile, indent=4)\n",
    "\n",
    "    data_for_dataset = {key: data[key] for key in [\"prompt\", \"chosen\", \"rejected\"]}\n",
    "\n",
    "    return data_for_dataset, chosen_rewards, rejected_rewards\n",
    "\n",
    "def train_iteration(model, \n",
    "                    model_checkpoint,\n",
    "                    iteration, \n",
    "                    data_size, \n",
    "                    sample_size, \n",
    "                    ar_tokenizer,\n",
    "                    nar_model, \n",
    "                    nar_tokenizer,\n",
    "                    utmos_model,\n",
    "                    all_src_encodec, \n",
    "                    all_instruction, \n",
    "                    args_predict, \n",
    "                    agent_output_dir,\n",
    "                    model_output_dir_base, \n",
    "                    beta = 0.1, \n",
    "                    temperature = 1.0,\n",
    "                    resume_from_checkpoint = False,\n",
    "                    learning_rate = 5e-07,\n",
    "                    num_train_epochs = 100,\n",
    "                    max_length = 1024*9,\n",
    "                    max_prompt_length = 1024*9,\n",
    "                    max_target_length = 1024*9,\n",
    "                    per_device_train_batch_size = 1,\n",
    "                    gradient_accumulation_steps = 1,\n",
    "                    seed = 42,\n",
    "):\n",
    "\n",
    "    selected_src_encodec = all_src_encodec[:data_size]\n",
    "    selected_instruction = all_instruction[:data_size]\n",
    "    \n",
    "    # calculate time for generating data\n",
    "    start_time = time.time()\n",
    "    data_for_dataset, chosen_rewards, rejected_rewards = generate_data(model=model, \n",
    "                                                                    ar_tokenizer=ar_tokenizer,\n",
    "                                                                    nar_model=nar_model,\n",
    "                                                                    nar_tokenizer=nar_tokenizer,\n",
    "                                                                    utmos_model=utmos_model,\n",
    "                                                                    selected_src_encodec=selected_src_encodec,\n",
    "                                                                    selected_instruction=selected_instruction,\n",
    "                                                                    args_predict=args_predict,\n",
    "                                                                    sample_size=sample_size,\n",
    "                                                                    iteration=iteration,\n",
    "                                                                    agent_output_dir=agent_output_dir,\n",
    "                                                                    temperature=temperature)\n",
    "    print(f\"generate data time: {time.time() - start_time}\")\n",
    "\n",
    "    dataset = Dataset.from_dict(data_for_dataset)\n",
    "    dataset_dict = dataset.train_test_split(test_size=0.1, shuffle=True, seed=seed)\n",
    "    train_dataset = dataset_dict[\"train\"]\n",
    "    val_dataset = dataset_dict[\"test\"]\n",
    "    \n",
    "    # print train_dataset and val_dataset\n",
    "    if iteration < 1:\n",
    "        print(\"train_dataset\", train_dataset.to_dict())\n",
    "        print(\"val_dataset\", val_dataset.to_dict())\n",
    "\n",
    "    model_output_dir = f\"{model_output_dir_base}/iter_{iteration}\"\n",
    "    os.makedirs(model_output_dir, exist_ok=True)\n",
    "\n",
    "    model_ref = create_reference_model(model)\n",
    "        \n",
    "    # calculate time for training model\n",
    "    start_time = time.time()\n",
    "    train_model(model=model,\n",
    "                model_ref=model_ref,\n",
    "                ar_tokenizer=ar_tokenizer,\n",
    "                train_dataset=train_dataset,\n",
    "                val_dataset=val_dataset,\n",
    "                model_output_dir=model_output_dir,\n",
    "                beta=beta,\n",
    "                resume_from_checkpoint=resume_from_checkpoint,\n",
    "                model_checkpoint=model_checkpoint,\n",
    "                learning_rate = learning_rate,\n",
    "                num_train_epochs = num_train_epochs,\n",
    "                max_length = max_length,\n",
    "                max_prompt_length = max_prompt_length,\n",
    "                max_target_length = max_target_length,\n",
    "                per_device_train_batch_size = per_device_train_batch_size,\n",
    "                gradient_accumulation_steps = gradient_accumulation_steps,\n",
    "                seed = seed)\n",
    "    print(f\"training model time: {time.time() - start_time}\")\n",
    "\n",
    "    return f\"{model_output_dir}/dpo_model\", chosen_rewards, rejected_rewards"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "# Load all data\n",
    "selected_src_encodec, selected_instruction = extract_data_from_json('dpo_data/src_encodec.json')\n",
    "\n",
    "# Define paths and device\n",
    "base_path = \"/work/b0990106x/trl\"\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# Define timestamp\n",
    "now = datetime.now()\n",
    "ts = now.strftime(\"%m%d-%H%M\")\n",
    "print(\"timestamp:\", ts)\n",
    "\n",
    "# Define paths\n",
    "model_output_dir = os.path.join(base_path, \"model_output\", ts) # Location where the model are saved\n",
    "agent_output_dir = os.path.join(base_path, \"output\", ts) # Path of saving the generated audio for reward model to evaluate\n",
    "os.makedirs(model_output_dir, exist_ok=True)\n",
    "os.makedirs(agent_output_dir, exist_ok=True)\n",
    "\n",
    "seed = 42 # Training: seed\n",
    "\n",
    "# Define arguments \n",
    "args_predict = SimpleNamespace(output_path=f\"{base_path}/output/{ts}/example.wav\", seed=seed, device=device)\n",
    "ar_checkpoint = \"lca0503/speech-chatgpt-base-ar-v2-epoch10-wotrans\"\n",
    "nar_checkpoint = \"lca0503/speech-chatgpt-base-nar-v2-epoch4-wotrans\"\n",
    "\n",
    "# Models and Iterations\n",
    "model_checkpoint = ar_checkpoint # Prepare: set the initial model checkpoint\n",
    "sample_size = 80 # Prepare Dataset: generate how many outputs to select max and min for chosen and rejected (original: 10)\n",
    "num_iterations = 1000  # Training: train how many iterations (original: 100)\n",
    "train_selected_indices = [8] \n",
    "# train_selected_indices = [9]\n",
    "# train_selected_indices = random.sample(range(len(selected_src_encodec)), 5) # Training: train on selected data indicies from all_src_encodec\n",
    " # Training: train on selected data indicies from all_src_encodec\n",
    "data_size_per_iteration = len(train_selected_indices) # Training: each iteration will train how many data\n",
    "\n",
    "# Define Training Configuration\n",
    "beta = 0.3 # Training: beta value for DPO (original: 0.1)\n",
    "learning_rate = 5e-07 # Training: learning rate (original: 5e-07)\n",
    "num_train_epochs = 3 # Training: number of training epochs (original: 3)\n",
    "max_length = 1024*9 # Training: max length of the model\n",
    "max_prompt_length = 1024*9 # Training: max length of the prompt\n",
    "max_target_length = 1024*9 # Training: max length of the target\n",
    "per_device_train_batch_size = 8 # Training: batch size (original: 1)\n",
    "gradient_accumulation_steps = 1 # Training: gradient accumulation steps\n",
    "\n",
    "# Evaluation Configuration\n",
    "eval_train = True # Evaluation: evaluate on training data or not\n",
    "eval_test = False # Evaluation: evaluate on testing data or not\n",
    "eval_train_indices = train_selected_indices # Evaluation: evaluate on training data indicies from all_src_encodec\n",
    "eval_test_indices = random.sample(range(len(selected_src_encodec)), 5) # Evaluation: evaluate on testing data indicies from all_src_encodec\n",
    "eval_train_data_len = len(eval_train_indices) # Evaluation: evaluate how many training data\n",
    "eval_test_data_len = len(eval_test_indices) # Evaluation: evaluate how many testing data\n",
    "num_eval = 10 # Evaluation: evaluate how many times per data (original: 10)\n",
    "eval_frequency = 1 # Evaluation: evaluate every how many iterations\n",
    "# Define temperature\n",
    "# eval_selected_indices = random.sample(range(len(all_src_encodec)), eval_data_len) # Evaluation: select 10 data for evaluation\n",
    "print(f\"length of all_src_encodec: {len(selected_src_encodec)}\") # ~ 9000 data\n",
    "print(f\"length of all_instruction: {len(selected_instruction)}\") # ~ 9000 data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "print(f\"num_iterations: {num_iterations}\")\n",
    "print(f\"data_size_per_iteration: {data_size_per_iteration}\")\n",
    "print(f\"sample_size: {sample_size}\")\n",
    "print(f\"beta: {beta}\")\n",
    "print(f\"learning_rate: {learning_rate}\")\n",
    "print(f\"num_train_epochs: {num_train_epochs}\")\n",
    "print(f\"ar_checkpoint: {ar_checkpoint}\")\n",
    "print(f\"nar_checkpoint: {nar_checkpoint}\")\n",
    "print(f\"args_predict: {args_predict}\")\n",
    "print(f\"model_output_dir: {model_output_dir}\")\n",
    "print(f\"agent_output_dir: {agent_output_dir}\")\n",
    "print(f\"base_path: {base_path}\")\n",
    "print(f\"device: {device}\")\n",
    "print(f\"eval_train_data_len: {eval_train_data_len}\")\n",
    "print(f\"eval_test_data_len: {eval_test_data_len}\")\n",
    "print(f\"eval_train_indices: {eval_train_indices}\")\n",
    "print(f\"eval_test_indices: {eval_test_indices}\")\n",
    "print(f\"eval_train: {eval_train}\")\n",
    "print(f\"eval_test: {eval_test}\")\n",
    "print(f\"num_eval: {num_eval}\")\n",
    "\n",
    "# print training data\n",
    "for i in train_selected_indices:\n",
    "    print('training idx', i,':', selected_instruction[i])\n",
    "    \n",
    "# print evaluation data\n",
    "if eval_test:\n",
    "    for i in eval_test_indices:\n",
    "        print('evaluation idx', i,':', selected_instruction[i])\n",
    "\n",
    "if eval_train:\n",
    "    for i in eval_train_indices:\n",
    "        print('evaluation idx', i,':', selected_instruction[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "model = AutoModelForSeq2SeqLMWithValueHead.from_pretrained(model_checkpoint, return_dict=True)\n",
    "ar_model = BartForConditionalGeneration.from_pretrained(ar_checkpoint)\n",
    "ar_tokenizer = AutoTokenizer.from_pretrained(ar_checkpoint)\n",
    "# ar_tokenizer.pad_token = ar_tokenizer.eos_token\n",
    "nar_model = NARBartForConditionalGeneration.from_pretrained(nar_checkpoint)\n",
    "nar_tokenizer = AutoTokenizer.from_pretrained(nar_checkpoint)\n",
    "\n",
    "utmos_checkpoint_path = \"UTMOSv2/models/fusion_stage3/fold0_s42_best_model.pth\"\n",
    "utmos_model = utmosv2.create_model(pretrained=True, checkpoint_path=utmos_checkpoint_path)\n",
    "\n",
    "end_time = time.time()\n",
    "print(f\"Time taken to load models: {end_time - start_time} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logging Start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "\n",
    "log_path = f'{model_output_dir}/log_training.log'\n",
    "print(f\"Logging to: {log_path}\")\n",
    "\n",
    "# Set up logging\n",
    "logging.basicConfig(\n",
    "    filename=log_path, \n",
    "    filemode='a', \n",
    "    format='%(asctime)s - %(levelname)s - %(message)s', \n",
    "    level=logging.INFO\n",
    ")\n",
    "\n",
    "logging.info(\n",
    "    f\"Parameters:\\n\"\n",
    "    f\"Prepare Data: sample_size: {sample_size}\\n\"\n",
    "    f\"Training: num_iterations: {num_iterations}\\n\"\n",
    "    f\"Training: data_size_per_iteration: {data_size_per_iteration}\\n\"\n",
    "    f\"Training: train_selected_indices: {train_selected_indices}\\n\"\n",
    "    f\"Training: beta: {beta}\\n\"\n",
    "    f\"Training: learning_rate: {learning_rate}\\n\"\n",
    "    f\"Training: num_train_epochs: {num_train_epochs}\\n\"\n",
    "    f\"Training: max_length: {max_length}\\n\"\n",
    "    f\"Training: max_prompt_length: {max_prompt_length}\\n\"\n",
    "    f\"Training: max_target_length: {max_target_length}\\n\"\n",
    "    f\"Training: per_device_train_batch_size: {per_device_train_batch_size}\\n\"\n",
    "    f\"Training: gradient_accumulation_steps: {gradient_accumulation_steps}\\n\"\n",
    "    f\"Training: seed: {seed}\\n\"\n",
    "    f\"Training: ar_checkpoint: {ar_checkpoint}\\n\"\n",
    "    f\"Training: nar_checkpoint: {nar_checkpoint}\\n\"\n",
    "    f\"Training: args_predict: {args_predict}\\n\"\n",
    "    f\"Training: model_output_dir: {model_output_dir}\\n\"\n",
    "    f\"Training: agent_output_dir: {agent_output_dir}\\n\"\n",
    "    f\"Training: base_path: {base_path}\\n\"\n",
    "    f\"Training: device: {device}\\n\"\n",
    "    f\"Evaluation: eval_train_data_len: {eval_train_data_len}\\n\"\n",
    "    f\"Evaluation: eval_test_data_len: {eval_test_data_len}\\n\"\n",
    "    f\"Evaluation: eval_train_indices: {eval_train_indices}\\n\"\n",
    "    f\"Evaluation: eval_test_indices: {eval_test_indices}\\n\"\n",
    "    f\"Evaluation: eval_train: {eval_train}\\n\"\n",
    "    f\"Evaluation: eval_test: {eval_test}\\n\"\n",
    "    f\"Evaluation: num_eval: {num_eval}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initial Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "\n",
    "import os\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "\n",
    "from transformers import logging\n",
    "logging.set_verbosity_error()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "import logging\n",
    "import numpy as np\n",
    "\n",
    "total_start_time = time.time()\n",
    "\n",
    "def evaluate_model(eval_type, eval_data_len, eval_indices):\n",
    "    \n",
    "    # Evaluate the model\n",
    "    original_model_metrics_mos, original_model_rewards_mos = eval_dpo_mos(\n",
    "        nar_model=nar_model,\n",
    "        ar_tokenizer=ar_tokenizer,\n",
    "        nar_tokenizer=nar_tokenizer,\n",
    "        trained_model=model,\n",
    "        utmos_model=utmos_model,\n",
    "        args_predict=args_predict,\n",
    "        all_src_encodec=selected_src_encodec,\n",
    "        all_instruction=selected_instruction,\n",
    "        iteration=-1,\n",
    "        num_evaluations=num_eval,\n",
    "        eval_data_len=eval_data_len,\n",
    "        selected_indices=eval_indices,\n",
    "        device=device,\n",
    "    )\n",
    "    \n",
    "    # Log the evaluation metrics\n",
    "    logging.info(f\"Original model metrics on {eval_type} set: {original_model_metrics_mos}\")\n",
    "    logging.info(f\"Original model rewards on {eval_type} set: {original_model_rewards_mos}\")\n",
    "\n",
    "    # Calculate and log MOS scores\n",
    "    reward_list_mos = [np.mean([r for r in rewards if r is not None]) if any(r is not None for r in rewards) else None\n",
    "                       for rewards in original_model_rewards_mos]\n",
    "    logging.info(f\"Original model MOS score list on {eval_type} set: {reward_list_mos}\")\n",
    "\n",
    "    # Filter and calculate average MOS\n",
    "    filter_reward_list_mos = [r for r in reward_list_mos if r is not None]\n",
    "    if filter_reward_list_mos:\n",
    "        average_mos = np.mean(filter_reward_list_mos)\n",
    "        logging.info(f\"Original model average MOS on {eval_type} set: {average_mos}\")\n",
    "    else:\n",
    "        logging.info(f\"Original model average MOS on {eval_type} set: None\")\n",
    "\n",
    "    # Calculate and log weighted reward\n",
    "    weighted_reward = np.mean(filter_reward_list_mos) / 5 if filter_reward_list_mos else None\n",
    "    logging.info(f\"Original model weighted average rewards on {eval_type} set: {weighted_reward}\")\n",
    "\n",
    "if eval_train:\n",
    "    evaluate_model(eval_type=\"training\", eval_data_len=eval_train_data_len, eval_indices=eval_train_indices)\n",
    "\n",
    "if eval_test:\n",
    "    evaluate_model(eval_type=\"testing\", eval_data_len=eval_test_data_len, eval_indices=eval_test_indices)\n",
    "    \n",
    "# If train_selected_indices is not empty, we will use the selected indices for training\n",
    "if train_selected_indices:\n",
    "    batch_src_encodec = [selected_src_encodec[i] for i in train_selected_indices]\n",
    "    batch_instruction = [selected_instruction[i] for i in train_selected_indices]\n",
    "    logging.info(f\"Processing data from selected indices: {train_selected_indices}\")\n",
    "else:\n",
    "    start_idx = 0\n",
    "    end_idx = data_size_per_iteration\n",
    "    batch_src_encodec = selected_src_encodec[start_idx:end_idx] \n",
    "    batch_instruction = selected_instruction[start_idx:end_idx]\n",
    "    logging.info(f\"Processing data from index {start_idx} to {end_idx}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Start training iterations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_iteration_model(eval_type, iteration, eval_data_len, eval_indices):\n",
    "    \n",
    "    trained_model_metrics, trained_model_rewards = eval_dpo_mos(\n",
    "        nar_model=nar_model,\n",
    "        ar_tokenizer=ar_tokenizer,\n",
    "        nar_tokenizer=nar_tokenizer,\n",
    "        trained_model=model,\n",
    "        utmos_model=utmos_model,\n",
    "        args_predict=args_predict,\n",
    "        all_src_encodec=selected_src_encodec,\n",
    "        all_instruction=selected_instruction,\n",
    "        iteration=iteration,\n",
    "        num_evaluations=num_eval,\n",
    "        eval_data_len=eval_data_len,\n",
    "        selected_indices=eval_indices,\n",
    "        device=device,\n",
    "    )\n",
    "    logging.info(f\"EVAL: MOS metrics {eval_type.capitalize()} Set for iteration {iteration}: {trained_model_metrics}\")\n",
    "    logging.info(f\"EVAL: MOS score {eval_type.capitalize()} Set for iteration {iteration}: {trained_model_rewards}\")\n",
    "\n",
    "    reward_list = [np.mean([r for r in rewards if r is not None]) if any(r is not None for r in rewards) else None\n",
    "                   for rewards in trained_model_rewards]\n",
    "    logging.info(f\"EVAL: Trained model reward list on {eval_type} set: {reward_list}\")\n",
    "    filter_reward_list = [r for r in reward_list if r is not None]\n",
    "    if filter_reward_list:\n",
    "        average_reward = np.mean(filter_reward_list)\n",
    "        logging.info(f\"EVAL: Trained model average rewards on {eval_type} set for iteration {iteration}: {average_reward}\")\n",
    "    else:\n",
    "        logging.info(f\"EVAL: Trained model average rewards on {eval_type} set for iteration {iteration}: None\")\n",
    "\n",
    "    weighted_reward = np.mean(filter_reward_list) / 5 if filter_reward_list else None\n",
    "    logging.info(f\"EVAL: Trained model weighted average rewards on {eval_type} set for iteration {iteration}: {weighted_reward}\")\n",
    "\n",
    "# Training loop\n",
    "for iteration in tqdm(range(num_iterations), desc=\"Training Iterations\", disable=True):\n",
    "    logging.info(f\"-----------Starting iteration {iteration}-----------\")\n",
    "\n",
    "    # resume = iteration > 0 # resume from the previous checkpoint when iteration > 0\n",
    "    resume = False\n",
    "\n",
    "    # Train the model for the current iteration\n",
    "    model_checkpoint, chosen_rewards, rejected_rewards = train_iteration(\n",
    "        model,\n",
    "        model_checkpoint,\n",
    "        iteration=iteration,\n",
    "        data_size=data_size_per_iteration,\n",
    "        sample_size=sample_size,\n",
    "        ar_tokenizer=ar_tokenizer,\n",
    "        nar_model=nar_model,\n",
    "        nar_tokenizer=nar_tokenizer,\n",
    "        utmos_model=utmos_model,\n",
    "        all_src_encodec=batch_src_encodec,\n",
    "        all_instruction=batch_instruction,\n",
    "        args_predict=args_predict,\n",
    "        agent_output_dir=agent_output_dir,\n",
    "        model_output_dir_base=model_output_dir,\n",
    "        temperature=1.0,\n",
    "        beta=beta,\n",
    "        resume_from_checkpoint=resume,\n",
    "        learning_rate=learning_rate,\n",
    "        num_train_epochs=num_train_epochs,\n",
    "        max_length=max_length,\n",
    "        max_prompt_length=max_prompt_length,\n",
    "        max_target_length=max_target_length,\n",
    "        per_device_train_batch_size=per_device_train_batch_size,\n",
    "        gradient_accumulation_steps=gradient_accumulation_steps,\n",
    "        seed=seed\n",
    "    )\n",
    "\n",
    "    logging.info(f\"Chosen rewards for iteration {iteration}: {chosen_rewards}\")\n",
    "    logging.info(f\"Rejected rewards for iteration {iteration}: {rejected_rewards}\")\n",
    "    logging.info(f\"Finished training iteration {iteration}\")\n",
    "\n",
    "    # Evaluate model every eval_frequency iterations\n",
    "    if (iteration + 1) % eval_frequency == 0:\n",
    "        if eval_train:\n",
    "            evaluate_iteration_model(eval_type=\"training\", iteration=iteration, eval_data_len=eval_train_data_len, eval_indices=eval_train_indices)\n",
    "        if eval_test:\n",
    "            evaluate_iteration_model(eval_type=\"testing\", iteration=iteration, eval_data_len=eval_test_data_len, eval_indices=eval_test_indices)\n",
    "\n",
    "    logging.info(f\"-----------Finished iteration {iteration}-----------\")\n",
    "\n",
    "total_end_time = time.time()\n",
    "\n",
    "# Calculate total time taken\n",
    "total_time_taken = total_end_time - total_start_time\n",
    "logging.info(f\"Total time taken for the entire process: {total_time_taken:.2f} seconds\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dpo",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
